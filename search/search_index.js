var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Welcome","text":""},{"location":"index.html#beneficiary-fhir-data-server-bfd","title":"Beneficiary FHIR Data Server (BFD)","text":"<p>The BFD Server is an internal backend system used at CMS to represent Medicare beneficiaries' demographic, enrollment, and claims data in FHIR format.</p>"},{"location":"index.html#dasg-mission","title":"DASG Mission","text":"<p>Drive innovation in data sharing so that beneficiaries and their healthcare partners have the data they need to make informed decisions about their healthcare.</p>"},{"location":"index.html#bfd-mission","title":"BFD Mission","text":"<p>Enable the CMS Enterprise to drive innovation in data sharing so that beneficiaries and their healthcare partners have the data they need to make informed decisions about their healthcare.</p>"},{"location":"index.html#bfd-vision","title":"BFD Vision","text":"<p>Provide a comprehensive, performant, and trustworthy platform to transform the way that the CMS enterprise shares and uses data.</p>"},{"location":"index.html#license","title":"License","text":"<p>This project is in the worldwide public domain. As stated in LICENSE.</p>"},{"location":"LICENSE.html","title":"License","text":"<p>As a work of the United States Government, this project is in the public domain within the United States.</p> <p>Additionally, we waive copyright and related rights in the work worldwide through the CC0 1.0 Universal public domain dedication.</p>","tags":["license","legal"]},{"location":"LICENSE.html#cc0-10-universal-summary","title":"CC0 1.0 Universal Summary","text":"<p>This is a human-readable summary of the Legal Code (read the full text).</p>","tags":["license","legal"]},{"location":"LICENSE.html#no-copyright","title":"No Copyright","text":"<p>The person who associated a work with this deed has dedicated the work to the public domain by waiving all of his or her rights to the work worldwide under copyright law, including all related and neighboring rights, to the extent allowed by law.</p> <p>You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission.</p>","tags":["license","legal"]},{"location":"LICENSE.html#other-information","title":"Other Information","text":"<p>In no way are the patent or trademark rights of any person affected by CC0, nor are the rights that other persons may have in the work or in how the work is used, such as publicity or privacy rights.</p> <p>Unless expressly stated otherwise, the person who associated a work with this deed makes no warranties about the work, and disclaims liability for all uses of the work, to the fullest extent permitted by applicable law. When using or citing the work, you should not imply endorsement by the author or the affirmer.</p>","tags":["license","legal"]},{"location":"api/request-audit-headers.html","title":"BFD Audit Headers","text":"<p>This document details the HTTP headers that should be included when calling BFD,   to ensure that proper audit information is available to the BFD team. Future versions of BFD may enforce/require some or all of these headers.</p> <p>These fields allow engineers &amp; operators to read the BFD logs   and see exactly why data was requested. Some of this information is redundant with information already captured elsewhere,   but experience has shown it to be very helpful when investigating issues.</p>","tags":["api"]},{"location":"api/request-audit-headers.html#synchronous-requests","title":"Synchronous Requests","text":"<p>For synchronous (i.e. non-bulk) requests, BFD users SHALL include meaningful values for as many as possible of the following HTTP headers.</p> <ul> <li><code>BlueButton-OriginalQueryId</code>: a unique ID (e.g. UUID) generated by the frontend system, per HTTP request to that frontend system.<ul> <li>This ID SHALL be included in all log events for the frontend system.</li> <li>This allows requests to be traced across systems.</li> </ul> </li> <li><code>BlueButton-OriginalQueryCounter</code>: start at <code>1</code> and increment for every request to BFD with the same <code>BlueButton-OriginalQueryId</code> value.</li> <li><code>BlueButton-OriginalQueryTimestamp</code>: an ISO-8601 UTC timestamp representing (roughly) when the original request reached the frontend system.</li> <li><code>BlueButton-DeveloperId</code>: the unique ID in the frontend system for the developer/vendor of the third-party application.</li> <li><code>BlueButton-Developer</code>: the human-readable name in the frontend system for the developer/vendor of the third-party application.</li> <li><code>BlueButton-ApplicationId</code>: the unique ID in the frontend system for the application/client that will receieve the data.</li> <li><code>BlueButton-Application</code>: the human-readable name in the frontend system for the application/client that will receive the data.</li> <li><code>BlueButton-UserId</code>: the unique ID in the frontend system for the user (e.g. beneficiary) that data is being requested on behalf of.</li> <li><code>BlueButton-User</code>: the human-readable login ID (e.g. email address) in the frontend system for the user (e.g. beneficiary) that the data is being requested on behalf of.</li> <li><code>BlueButton-BeneficiaryId</code>: the unique beneificary ID (e.g. <code>Patient.id</code>) from BFD for the user the beneficiary whose data is being requested.</li> <li><code>x-forwarded-for</code>: a standard header for identifying the originating IP address of a client connecting to a web server through an HTTP proxy or a load balancer.</li> </ul>","tags":["api"]},{"location":"api/request-audit-headers.html#asynchronous-requests","title":"Asynchronous Requests","text":"<p>For asynchronous (i.e. bulk) requests, BFD users SHALL include meaningful values for as many as possible of the following HTTP headers.</p> <ul> <li><code>BlueButton-OriginalQueryId</code>: (see above; same thing)<ul> <li>For asynchronous jobs, users SHALL use the <code>BlueButton-OriginalQueryId</code> for the HTTP request that created/submitted/whatever the job.</li> </ul> </li> <li><code>BULK-CLIENTID</code>: a unique identifier of the bulk client for whom this request is for (i.e. UUID, ACO ID, NPI, Part D Contract)</li> <li><code>BULK-CLIENTNAME</code>: a human readable name for the bulk client for whom this request is for to aid in tracing and debugging.</li> <li><code>BULK-JOBID</code>: a unique identifier for the job (Job ID, UUID, etc.)</li> </ul>","tags":["api"]},{"location":"api/request-options.html","title":"Request options","text":"<ul> <li>api hide:</li> <li>tags</li> </ul>"},{"location":"api/request-options.html#bfd-request-options","title":"BFD Request Options","text":"<p>This document details the request options that can be used when calling BFD. Future versions of BFD may apply some of these options automatically,   based on other request details (e.g. authentication/authorization).</p>"},{"location":"api/request-options.html#optional-data","title":"Optional Data","text":"<p>Some data fields are optional; they're only included when the request is configured to do so.</p> <ul> <li>HTTP Header: <code>IncludeIdentifiers</code><ul> <li>Operations: all <code>/Patient</code> requests</li> <li>Default value: <code>false</code></li> <li>Supported values: <code>false</code>, <code>true</code>, <code>hicn</code>, <code>mbi</code></li> <li>Description:   Do not set this header more than once; an arbitrary value will be selected if that happens.   When set to <code>mbi</code>, BFD will include all of the known MBIs for the requested beneficiary (unhashed).   When set to <code>hicn</code>, BFD will include all of the known HICNs for the requested beneficiary (unhashed).   When set to <code>true</code>, BFD will include all of the known MBIs and HICNs for the requested beneficiary (unhashed).   When set to <code>false</code>, BFD will not include any (unhashed) MBIs or HICNs for the requested beneficiary.</li> </ul> </li> <li>HTTP Header: <code>IncludeAddressFields</code><ul> <li>Operations: all <code>/Patient</code> requests</li> <li>Default value: <code>false</code></li> <li>Supported values: <code>false</code>, <code>true</code></li> <li>Description:   When set to <code>true</code>, BFD will include all of the detailed address data for the requested beneficiary.   When set to <code>false</code>, BFD will not include detailed address data for the requested beneficiary.   Please note that, even when <code>false</code> county and ZIP/postal codes will still be included for the specified beneficiary.</li> </ul> </li> </ul>"},{"location":"changelog/index.html","title":"Changelog","text":""},{"location":"contributing/index.html","title":"Contributing to the BFD","text":"<p>We want to ensure a welcoming environment for all of our projects! Our staff follows the 18F Code of Conduct and all contributors should do the same.</p>  <p>Note</p> <p>Although this is a public repo, contributing to the BFD is for CMS-approved contributors only, not outside contributors.</p>","tags":["Contributing"]},{"location":"contributing/index.html#background","title":"Background","text":"<p>BFD exists to enable the CMS Enterprise to drive innovation in data sharing so that beneficiaries and their healthcare partners have the data they need to make informed decisions about their healthcare.</p> <p>We provide a comprehensive, performant, and trustworthy platform to transform the way that the CMS enterprise shares and uses data by providing FHIR-formatted data, including beneficiary demographic, enrollment, and claims data.</p> <p>Review the README for additional information on BFD.</p>","tags":["Contributing"]},{"location":"contributing/index.html#contributing-changes","title":"Contributing changes","text":"<p>Contributions to the BFD are welcome from any party inside CMS. Small changes like \"good first issues\" can be submitted for consideration without any additional process. This LGTM resource provides candidates for \"good first issues\".</p> <p>Any substantive change must go though an RFC process before work on the change itself can start.</p> <p>Any code changes should be properly commented and accompanied by appropriate unit and integration test as per DASG Engineering Standards.</p>","tags":["Contributing"]},{"location":"contributing/index.html#faq","title":"FAQ","text":"<p>Q: What kind of changes don't require an RFC?</p> <p>A: In general bug fixes and small changes that do not affect behavior or meaning. If you're unsure please quickly ask in the #bfd channel or in the Scrum of Scrums meeting. If your PR involves a substantial change, it will be rejected and you will be asked to go through the RFC process.</p> <p>Q: How do I know what \u201cfirst issues\u201d are up for grabs?</p> <p>A: First issues are tracked on the BFD\u2019s Jira board with the \"Good_First_Issue\" label: https://jira.cms.gov/issues/?jql=project%20%3D%20BFD%20AND%20status%20%3D%20Open%20AND%20labels%20%3D%20Good_First_Issue</p>","tags":["Contributing"]},{"location":"contributing/index.html#proposing-substantive-changes","title":"Proposing substantive changes","text":"<p>Substantive changes need to go through a design process involving the core team. Opening an RFC provides a path for this inclusion and process. Start an RFC by copying the file <code>rfcs/0000-template.md</code> to <code>rfcs/0000-&lt;my idea&gt;.md</code> and fill in the details. Open a PR using the RFC template submit a pull request. The RFC will remain open for a 2 week period, at the end of which a go/no-go meeting will be held. If approved by at least two core team members and there are no outstanding reservations, the RFC will be accepted. Once accepted the author of the RFC and their team can scope the work within their regular process. Link or reference the RFC in the related JIRA ticket. The core team will respond with design feedback, and the author should be prepared to revise it in response.</p>","tags":["Contributing"]},{"location":"contributing/index.html#faq_1","title":"FAQ","text":"<p>Q: What qualifies as a substantive change?</p> <p>A: There is no strict definition, however examples of substantive changes are:</p> <ol> <li>Any change to or addition of an API endpoint (either URL or response) that is not a bug fix.</li> <li>Changes that affect the ingestion of data into the BFD (the ETL process).</li> <li>Changes that significantly alter the structure of the codebase.</li> </ol> <p>Q: What if I'm not sure if I need an RFC?</p> <p>A: Reach out to the #bfd channel or ask in the Scrum of Scrums meeting and see what the BFD team thinks.</p> <p>Q: How should I prepare for an RFC?</p> <p>A: Bring the idea to the #bfd channel or the Scrum of Scrums meeting and talk it over with the core team.</p> <p>Q: What if my RFC is not accepted?</p> <p>A: It will be closed, but can be reopened if it is updated to address the items that prevented it's acceptance.</p> <p>Q: What if my team doesn\u2019t have the resources to implement our accepted RFC?</p> <p>A: Anyone can coordinate with you and the core team to take over the work.</p>","tags":["Contributing"]},{"location":"contributing/index.html#getting-started","title":"Getting started","text":"<p>Going to work on this project? Great! There are currently two documented methods for getting a local environment up and running to get you setup for development.</p> <p>Getting started on BFD</p>","tags":["Contributing"]},{"location":"contributing/index.html#opening-a-pr","title":"Opening A PR","text":"<p>To contribute work back to the BFD your branch must be pushed to the <code>CMSgov/beneficiary-fhir-data</code> repository. <pre><code>git push origin &lt;your username&gt;/&lt;your feature name&gt;\n# To make sure \"origin\" points to CMSgov/beneficiary-fhir-data run\n# git remote -v\n# if a different remote points to CMSgov/beneficiary-fhir-data\n# replace \"origin\" with that remotes name in the origional command\n</code></pre> In order to obtain permission to do this contact a github administrator or reach out on #bfd. Once pushed, open a pull request and use this PR template found in .github</p> <p>Please fill out each section of the body of the PR or quickly justify why the section does not apply to this particular change. Reviewers will automatically be suggested or requested based on the projects CODEOWNERS file, feel free to add other reviewers if desired. Once all automated checks pass and two reviewers have approved your pull request, a code owner from the core team will do a final review and make the decision to merge it in or not.</p> <p>If you have any questions feel free to reach out on the #bfd channel in CMS slack!</p>","tags":["Contributing"]},{"location":"contributing/index.html#faq_2","title":"FAQ","text":"<p>Q: What if the core team rejects my PR?</p> <p>A: The BFD core team will commit to never rejecting a PR without providing a path forward. The developer who put up the PR should review any feedback, and discuss with their product manager the scope of the work that is now outstanding.</p>","tags":["Contributing"]},{"location":"contributing/style-guide/index.html","title":"Style Guide","text":"<p>This document details the specific code and documentation styles recommended and enforced within the BFD codebase.</p>"},{"location":"contributing/style-guide/index.html#enforcement","title":"Enforcement","text":"<p>All style rules will be enforced via checkstyle during the build (as possible). To ease development, you may wish to install a checkstyle checker plugin to your IDE, and point it at the checkstyle.xml located in the apps directory. This will allow instant checking of checkstyle rules within the IDE, so you don't need to wait until build time.</p> <p>Enforcement will only be automated within projects that have been cleaned up to meet the documentation standards; until a project has been cleaned up it will be excluded from build enforcement. Old code that is an excluded project does not need to be updated if the class is touched, unless the new code adds new contextual information that would be useful to capture.</p> <p>Newly written classes/fields/methods in all projects within BFD should follow these conventions, regardless of automatic enforcement.</p> <p>Test classes and any related test code (such as test utils) are included in code that is expected to follow these style guidelines. Generated code is excluded, as it can be tricky (and of questionable value) to properly style/document generated code.</p>"},{"location":"contributing/style-guide/index.html#javadoc-style","title":"Javadoc Style","text":"<p>The goal of the BFD Javadoc style is to allow for easier maintainability and understanding of classes, methods, interfaces, constructors and fields. Documentation should be as concise as possible while still encapsulating any contextual or historical information that will help future maintainers make informed code changes. Basically, ensure someone with no context of the code can come in and quickly understand and safely make adjustments without needing a ton of research or system knowledge.</p> <p>Meeting this goal will require your careful consideration about when, where, and what to document; stay vigilant!</p> <p>Javadocs within BFD will follow these conventions:</p> <ul> <li>All methods and constructors of all scopes must have a javadoc</li> <li>Must have a description that explains what the method does, and any useful contextual/historical information that may help maintainability</li> <li>All method parameters should be documented with the <code>@param</code> tag, and a description of what the parameter is<ul> <li>If <code>null</code> is explicitly disallowed for a parameter, it should be documented</li> </ul> </li> <li>Method returns should be documented with the <code>@returns</code> tag and a description of what is returned (and how if multiple returns under various conditions)<ul> <li>If the method can return <code>null</code> it should be noted and under what condition this may happen</li> </ul> </li> <li>Method overrides or interface implementations should have a javadoc that is simply <code>{@inheritDoc}</code> unless the overridden method's functionality diverges enough to need additional documentation<ul> <li>This includes <code>toString</code>, <code>hashCode</code>, etc.</li> </ul> </li> <li>All classes and interfaces of all scopes must have a javadoc</li> <li>All class-level fields of all scopes must have a javadoc<ul> <li>Information about the field should be captured in the field javadoc, which can be referenced by the getter/setter to reduce duplication</li> <li>Reference other classes when it makes sense to reduce duplication (for instance, a class level field that represents a CcwCodebookVariable can just link to the CcwCodebookVariable in the documentation)</li> </ul> </li> </ul>"},{"location":"insights/index.html","title":"Index","text":""},{"location":"insights/index.html#generally-useful","title":"Generally Useful","text":"<ul> <li>Data Organization: Concepts and naming conventions for S3 buckets, streams, databases and other</li> <li>Terraform Organization: Concepts and naming conventions for Terraform scripts</li> <li>Security Model: Security model</li> <li>Dataflow: Example dataflow</li> <li>Lessons Learned: Notes for future developers</li> <li>Core Tables: Design notes on the core tables</li> </ul>"},{"location":"insights/index.html#ab2d-docs","title":"AB2D Docs","text":"<ul> <li>Firehose: Notes on how to setup a AB2D firehose</li> </ul>"},{"location":"insights/ab2d_firehose.html","title":"AB2D Firehose Setup","text":"","tags":["ab2d","kinesis","firehose","bfd-insights"]},{"location":"insights/ab2d_firehose.html#bucket","title":"Bucket","text":"<p>The bucket for AB2D data is <code>bfd-insights-ab2d-BFDACCTNUM</code></p> <p>The CMK for the bucket is <code>arn:aws:kms:us-east-1:BFDACCTNUM:key/KEYID</code></p>","tags":["ab2d","kinesis","firehose","bfd-insights"]},{"location":"insights/ab2d_firehose.html#kinesis-firehose-setup","title":"Kinesis Firehose Setup","text":"<p>Here's the terraform template for setting up a firehose.</p> <pre><code>resource \"aws_kinesis_firehose_delivery_stream\" \"main\" {\n  name                  = local.full_name\n  tags                  = var.tags\n  destination           = \"extended_s3\"\n\n  # Encrypt while processing\n  server_side_encryption {\n    enabled = true\n  }\n\n  extended_s3_configuration {\n    role_arn            = aws_iam_role.firehose.arn\n    bucket_arn          = local.bucket_arn\n    kms_key_arn         = data.aws_kms_key.bucket_cmk.arn # Encrypt on delivery\n    buffer_size         = var.buffer_size\n    buffer_interval     = var.buffer_interval\n    compression_format  = \"GZIP\"\n    prefix              = \"databases/${var.database}/${var.stream}/dt=!{timestamp:yyyy-MM-dd}/\"\n    error_output_prefix = \"databases/${var.database}/${var.stream}_errors/!{firehose:error-output-type}/!{timestamp:yyyy-MM-dd}/\"\n  }\n}\n</code></pre> <p>Key aspects include: - Partition by delivery time date - Use a stream name that is the table name - Put all tables under the <code>ab2d</code> database - Use the \"GZIP\" compression format. Saves $. - Don't mix error files with the stream data</p>","tags":["ab2d","kinesis","firehose","bfd-insights"]},{"location":"insights/ab2d_firehose.html#policy","title":"Policy","text":"<p>Here's the template for the firehose IAM policy. The sections on getting S3 and KMS access is critical.</p> <pre><code>resource \"aws_iam_policy\" \"firehose\" {\n  name        = local.full_name\n  path        = \"/bfd-insights/\"\n  description = \"Allow firehose delivery to ${var.bucket}\"\n  policy      = &lt;&lt;-EOF\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"glue:GetTable\",\n                \"glue:GetTableVersion\",\n                \"glue:GetTableVersions\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:AbortMultipartUpload\",\n                \"s3:GetBucketLocation\",\n                \"s3:GetObject\",\n                \"s3:ListBucket\",\n                \"s3:ListBucketMultipartUploads\",\n                \"s3:PutObject\"\n            ],\n            \"Resource\": [\n                \"${local.bucket_arn}\",\n                \"${local.bucket_arn}/*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"kms:Encrypt\",\n                \"kms:Decrypt\",\n                \"kms:ReEncrypt*\",\n                \"kms:GenerateDataKey*\",\n                \"kms:DescribeKey\"\n            ],\n            \"Resource\": [\n                \"${data.aws_kms_key.bucket_cmk.arn}\"\n            ]\n        },\n        {\n            \"Sid\": \"\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:us-east-1:${local.account_id}:log-group:/aws/kinesisfirehose/${local.full_name}:log-stream:*\"\n            ]\n        },\n        {\n            \"Sid\": \"\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"kinesis:DescribeStream\",\n                \"kinesis:GetShardIterator\",\n                \"kinesis:GetRecords\",\n                \"kinesis:ListShards\"\n            ],\n            \"Resource\": \"arn:aws:kinesis:us-east-1:${local.account_id}:stream/${local.full_name}\"\n        }\n    ]\n}\nEOF\n}\n</code></pre>","tags":["ab2d","kinesis","firehose","bfd-insights"]},{"location":"insights/bb2_notes.html","title":"BB2 Project information","text":"","tags":["bb2","insights"]},{"location":"insights/bb2_notes.html#names","title":"Names","text":"Category Name     Project <code>bb2</code>   Bucket <code>bfd-insights-bb2-BFD_ACCT_NUM</code>   CMK <code>arn:aws:kms:us-east-1:BFD_ACCT_NUM:key/KEYID</code>   Database <code>bb2</code>   tag:project <code>bb2</code>","tags":["bb2","insights"]},{"location":"insights/bb2_notes.html#raw-log-schema","title":"Raw Log Schema","text":"<p>The raw logs from the bb2 server will be NDJSON files that are GZIPed. The raw logs are partitioned and ordered by delivery time.</p> <pre><code>[\n{name=\"instance_id\",  type=\"string\",    comment=\"AWS instance id recording the event\"},\n{name=\"component\",    type=\"string\",    comment=\"Always bb2.web\"},\n{name=\"vpc\",          type=\"string\",    comment=\"dev, prod, impl, etc.\"},\n{name=\"log_name\",     type=\"string\",    comment=\"BB2 log name\"},\n{name=\"message\",      type=\"string\",    comment=\"JSON object\"},\n]\n</code></pre>","tags":["bb2","insights"]},{"location":"insights/bb2_notes.html#event-normalization","title":"Event Normalization","text":"<p>Per the developing standard for DASG Audit events, every normalized events needs:</p>    Area Requirement BB2 Value     Time event period/timestamp in UTC Z Timestamp in message   Source Recording software Component, Instance id, VPC   Agent  Application from message   Entity  From message   Span Correlation ids From message","tags":["bb2","insights"]},{"location":"insights/core_tables.html","title":"Introduction","text":"<p>In the initial Insights design, there are three tiers of tables: - Raw - Raw data imported from specific projects. Semi-structured - Core - Structured DASG-tables that consolidate all - Marts - Per project summaries of the core and raw tables</p> <p>In a grandiose sense, these tiers are examples of the data lake, the data warehouse, and data mart architectures. This note discusses early thoughts on the design of the core tables.</p>","tags":["insights"]},{"location":"insights/core_tables.html#purpose","title":"Purpose","text":"<p>DASG is a family of projects which provide CMS data to a variety of data consumers at a high level. The core tables provide a DASG-wide view of the activities of all DASG projects. The core tables have to combine and generalize the records from each project.</p> <p>Some common use-cases drive the design of the core tables: - The ability to tie a release of data to all the software that impacted the release of data. - The ability to present DASG with standard metrics for each project - The ability to give to a beneficiary a report about accesses to their CMS data by DASG</p>","tags":["insights"]},{"location":"insights/core_tables.html#common-concepts","title":"Common Concepts","text":"<p>For this discussion, here are few terms: - Entity - The information that is referenced by an event. A typical entity is an FHIR resource for a specific beneficiary - Agent - The external software that receives data from DASG - Source - The DASG software that generates an event - Organization - The external entity that runs the agents that receive CMS data. An Agent is a piece of software, while Organizations are people and legal entities.</p>","tags":["insights"]},{"location":"insights/core_tables.html#prior-work","title":"Prior Work","text":"<p>Two standards inspire the design of the core tables: - FHRI Audit Events - Provide a very general way to express audit events. FHIR Audit Events build on previous work done on auditing. - OpenTracing - The OpenTracing standard builds on the work of Zipkin and other distributed tracing standards.</p> <p>In essence, the core table design is a combination of these two standards molded to be specific to the DASG use-case.</p>","tags":["insights"]},{"location":"insights/core_tables.html#events-table","title":"Events Table","text":"<p>The events table has been designed to combine both FHIR Audit Event and the OpenTracing standards.</p>    category name Type Comments Required FHIR OpenTracing     span id UUID event id yes      other_ids List(UUID) aka ids no      child_of UUID  no  childOf reference    follows_from List(UUID)  no  followsFrom reference    period Timestamp UTC no period span    recorded Timestamp UTC yes     operation type String Details below yes type, subtype operation_type    mutation String CRUD + E yes action    result error Boolean  yes  error    status_code Number  yes outcome standard tag    status_description String  yes outcomeDescription standard tag   agent agent String type yes, if agent is present      organization String        client_id UUID ID not value yes, if agent is present      security_token_id String  yes, if agent is present      ip_addr String       source component String Component that records the event. yes  component    vpc String env or other       instance_id String AWS instance id yes     diagnostics mdc map(String,String) standard open tracing tags no     entity type String List of EOB,Coverage,Patient,Roster yes      entity_id UUID  no      bene_id bene_id  no      mbi_hash String  no      hicn_hash String  no      bene_list String Reference to a list when no      security_label String PHI,PII yes security_label     name String Use only when a identifier is not available no enity.name     description String Use only when a identifier is not available no entity.description","tags":["insights"]},{"location":"insights/core_tables.html#other-tables","title":"Other Tables","text":"","tags":["insights"]},{"location":"insights/core_tables.html#beneficiary-table","title":"Beneficiary Table","text":"<p>There will most likely be several bene tables: a cross-walk table that correlates different beneficiary identifiers; a lookup table with demographics enrich reports; a table to calculate DPC's BAC.</p>","tags":["insights"]},{"location":"insights/core_tables.html#agents-table","title":"Agents Table","text":"<p>A list of all software that receives DASG data.</p>","tags":["insights"]},{"location":"insights/core_tables.html#sources-table","title":"Sources Table","text":"<p>A list of all the software that runs in DASG systems.</p>","tags":["insights"]},{"location":"insights/core_tables.html#organizations-table","title":"Organizations Table","text":"<p>A common table with entries for all organizations.</p>","tags":["insights"]},{"location":"insights/data_organization.html","title":"Organization","text":"<p>To help people new to the BFD-Insights project, we should use the same names and organization in S3 buckets, Athena databases, and Glue workflows.  This document lists the names used and the conventions they follow. Please keep it up to date. </p>"},{"location":"insights/data_organization.html#concepts","title":"Concepts","text":""},{"location":"insights/data_organization.html#lake","title":"Lake","text":"<p>A lake is a set of databases and their query engines, plus the ELT jobs and workflows to load and transform the data within. As such, a lake is more of a concept than an actual object. Note: the production data lake <code>prod-lake</code> has data from both the <code>prod</code> and <code>prod-sbx</code> environments. </p> <p>Visulization tools like QuickSight conceptually sit outside the data lake. </p>"},{"location":"insights/data_organization.html#projects","title":"Projects","text":"<p>Projects represent a particular DASG project. The abbreviated name of the projects are used. Examples include <code>ab2d</code>, <code>bb2</code>, <code>bfd</code>, <code>bcda</code>, and <code>dpc</code>. <code>dasg</code> is sometime to used as a project name to indicate concerns that cross-project boundaries. </p>"},{"location":"insights/data_organization.html#moderate-and-high-sensitivity","title":"Moderate and High Sensitivity","text":"<p>At the first level, a lake's data is divided according to the sensitivity of the data. There are different buckets for each sensitivity level. </p> <ul> <li>high: Contains information which are highly sensitive. PII data has high sensitivity. </li> <li>moderate: Contains data which are moderately sensitive. DASG logs have moderate sensitivity. Logs are scrubbed not to contain high sensitivity data. </li> </ul>"},{"location":"insights/data_organization.html#groups","title":"Groups","text":"<p>There are 4 groups of users defined: </p> <ul> <li>Admins are .gov employees who setup security policies.</li> <li>Analysts are Data Engineers who have access to all except security configs.</li> <li>Authors create QuickSight dashboards.</li> <li>Readers are leadership and product stakeholders with access to read QuickSight dashboards. </li> </ul>"},{"location":"insights/data_organization.html#resources-naming-conventions","title":"Resources Naming Conventions","text":""},{"location":"insights/data_organization.html#all-resources","title":"All resources","text":"<ul> <li>Resources start with <code>bfd-insights</code> to distinguish them from other account resources</li> <li>Where tagging is supported, included tags are: <code>business</code>,  <code>product</code> <code>sensitivity</code>, and <code>project</code>.  </li> </ul>"},{"location":"insights/data_organization.html#buckets","title":"Buckets","text":"<p>There are two forms of bucket names: a per project name or a sensitivity name. Since s3 buckets need to be globally unique names, they also include the account-id to ensure they unique. </p> <p>Examples: <code>bfd-insights-moderate-577373831711</code>, <code>bfd-insights-ab2d-577373831711</code></p>"},{"location":"insights/data_organization.html#top-level-folders","title":"Top-level Folders","text":"<p>At the top level of a bucket, these folders are setup:</p> <ul> <li>users (optional) folder for specific users to store data and query results</li> <li>databases folder for databases</li> <li>adhoc (optional) folder to hold miscellaneous </li> </ul>"},{"location":"insights/data_organization.html#databases-and-tables","title":"Databases and Tables","text":"<p>Databases names follow this convention: <code>&lt;project&gt;&lt;_suffix&gt;</code>. The project is required, but any suffix can follow including no suffix.</p> <p>Table names should be simple and reflect the contents of the table.</p> <p>All database and table names should be lower-cased and only inlcude letters, numbers and _. They should not include <code>bfd</code> or <code>insights</code> as a prefix. </p>"},{"location":"insights/data_organization.html#data-folders","title":"Data Folders","text":"<p>Folders that hold data follow the following convention:  <pre><code>/databases/&lt;database&gt;/&lt;table&gt;/&lt;partitions&gt;\n</code></pre> Where - database is the abbreviated name of the project plus any other suffixes if there are multiple databases in a project - table describes the content of the table - partition are folders used by table partitions. These must follow the Hive convention of <code>&lt;partition_name&gt;=&lt;value&gt;</code></p> <p>All names should be lower cased and only inlcude letters, numbers and _. </p>"},{"location":"insights/data_organization.html#users-folders","title":"Users Folders","text":"<p>All user folders follow this convention. </p> <pre><code>/users/&lt;user-name&gt;\n/users/&lt;user-name&gt;/query_results\n</code></pre>"},{"location":"insights/data_organization.html#components","title":"Components","text":"<p>The component names are used to provide a human readable name to independently running software. Each EC2 image or ECS container should have a component name, for example. There is a cross-project component table, so each component name needs to made unique by adding the project name. </p> <p><pre><code>&lt;project_name&gt;.&lt;component&gt;\n</code></pre> Examples: <code>bb2.web</code>, <code>dpc.api</code>, and <code>bcda.worker</code></p>"},{"location":"insights/lessons_learned.html","title":"Lessons Learned","text":"<p>Notes to our future selves about the mistakes and pains that we went through.</p>"},{"location":"insights/lessons_learned.html#s3-encryption","title":"S3 Encryption","text":"<p>CMS has a recommendation/policy or best practice of using AWS-KMS with AWS managed key. Buckets are setup with encryption as the default, but engineers need to configure services to use this. Each bucket has a unique key. This policy means that every S3 <code>GetObject</code> and <code>PutObject</code> operation needs permissions for the KMS <code>Encrypt</code>, <code>GenerateDataKey*</code>, and <code>Decrypt</code> operations. If an S3 operation comes back as access-denied, you should check both the S3 and KMS permissions for the principal.  </p>"},{"location":"insights/lessons_learned.html#cross-account-access","title":"Cross-Account Access","text":"<p>Cross-account access varies for each AWS resource or service. Most have little support for it. Kinesis Firehose, an essential component in the design, does not support it, for example. S3 and KMS, however, do. So, S3 buckets are the entry-points into the system. </p> <p>A subtle note that came up while writing Terraform scripts. The resource policy controls Cross-account access in an S3 bucket. Unlike IAM policies, there can be only one S3 policy document per bucket. So, as we broke the project in multiple sub-projects, it became clear that we needed to have one bucket per project.</p>"},{"location":"insights/lessons_learned.html#quicksight-and-permissions","title":"QuickSight and Permissions","text":"<p>In many ways, QuickSight is a separate product from AWS. Its security permissions are controlled by its control panel, not IAM, for example. Underneath the QuickSight permission system is an IAM role: <code>aws-quicksight-service-role-v0</code>. The QS control panel edits this role. If you modify this role directly or if you change the resources that QS needs to access, QS may start to have unexplained permission problems. For example, QS couldn't list workgroups. The fix is to rebuild the QS service role. The following link explains this recovery. </p> <p>https://aws.amazon.com/premiumsupport/knowledge-center/quicksight-permission-errors/</p> <p>The current deployment script keeps QS happy, but future scripts may need to take over managing the QS service role explicitly. </p> <p>Another problem, QS doesn't have support CMK, which is CMS policy to use. The current workaround is: - Use KMS grants to give the QS role access to the keys. These grants must be redone every time you rebuild the QS role. There is a script todo this.  - Use Athena workgroups that override client settings to force query results to use SSE-KMS. </p> <p>More useful links: - https://docs.aws.amazon.com/quicksight/latest/user/troubleshoot-athena-insufficient-permissions.html</p> <ul> <li>https://aws.amazon.com/premiumsupport/knowledge-center/quicksight-deny-policy-allow-bucket/</li> </ul>"},{"location":"insights/lessons_learned.html#make-a-plan-for-cost-estimation-and-monitoring","title":"Make a plan for cost estimation and monitoring","text":"<ul> <li>Perform cost estimates early in development on a small sample of the data to assess viability on the complete dataset. </li> <li>Continue performing cost estimations while scaling up the data volume to ensure that costs are scaling as expected. </li> <li>Continue performing cost monitoring even after going live to ensure that the cost profile is not changing unexpectedly. </li> <li>Monitor spending across all AWS services. Some services may have significant usage that is not obvious (KMS for instance).</li> <li>Maintain a spreadsheet while developing pipelines to understand the impacts to cost of fixes or enhancements.</li> <li>Don't run long or intensive Glue jobs on the weekend.</li> </ul>"},{"location":"insights/lessons_learned.html#configure-aws-budgets-and-monitor-proactively-with-aws-cost-explorer","title":"Configure AWS Budgets and monitor proactively with AWS Cost Explorer","text":"<ul> <li>Create individual alarms for services that are used by the pipeline (Glue, Athena, S3, KMS, etc).</li> <li>Create an overall alarm for all services.</li> <li>Be mindful of the once a year billing spike when services are pre-purchased.</li> <li>Ensure that all resources are tagged in a way that makes cost monitoring easy.</li> </ul>"},{"location":"insights/lessons_learned.html#learn-the-cost-differences-and-tradeoffs-between-different-aws-services-and-capabilities","title":"Learn the cost differences and tradeoffs between different AWS services and capabilities","text":"<ul> <li>Athena is billed by the TB scanned ($5/TB as of this writing) regardless of the query complexity.</li> <li>Glue is billed by the DPU hour ($0.44/DPU hour as of this writing) regardless of the size of data processed.</li> <li>Both Athena and Glue will incur costs for S3 and KMS.</li> <li>Athena has a query timeout of 30 minutes and cannot open more than 100 partitions.</li> <li>This is both an annoying limitation and a fail-safe that avoids runaway spending.</li> </ul> <p>Given these constraints, it is possible for some use cases to load large volumes of data with Athena (using <code>insert into select</code>) at a much cheaper AWS cost than with Glue. However, the 30 minute timeout and 100 partition maximum may result in spending engineer time (another aspect of cost that should be considered) to break up the job manually. For a practical example, see this Runbook.</p>"},{"location":"insights/lessons_learned.html#know-your-aws-features-and-their-cost-implications","title":"Know your AWS features and their cost implications","text":"<ul> <li>Use S3 Bucket keys for insights buckets that accumulate many small objects to save on KMS costs.</li> <li>Firehose unavoidably produces many small objects which quickly become costly to decrypt for reporting jobs.</li> <li>Bucket keys reduce this overhead significantly.</li> <li>Consider using Parquet format for wide tables.</li> <li>Parquet will reduce the amount of data accessed by Athena for many queries since only the columns in the query will      be scanned. This saves on Athena cost and makes the queries faster.</li> <li>Parquet is more costly during ingestion if using Glue jobs and can be more complex to configure and work with.</li> <li>Cloudwatch provides an export feature that can be used to load historical data but it has some quirks.</li> <li>Export target bucket cannot be configured with KMS.</li> <li>The export format does not match the export format from Cloudwatch subscriptions.</li> <li>This Grok filter can be used to extract the message as a string <code>%{TIMESTAMP_ISO8601:timestamp:string} %{GREEDYDATA:message:string}</code></li> <li>Athena provides good support for JSON extraction which can be used on a Glue table using the Grok filter.</li> </ul>"},{"location":"insights/onboarding.html","title":"Onboarding","text":""},{"location":"insights/onboarding.html#new-devops","title":"New Dev/Ops","text":"<p>This was the list of onboarding tasks done for Andy Daykin</p> <ol> <li>Create ticket to get a IAM User account with CLI access.  </li> <li>Add to Analyst group</li> <li>Add to GitHub staff members list</li> <li>Add to QuickSight </li> </ol>"},{"location":"insights/security.html","title":"Security Model","text":""},{"location":"insights/terraform.html","title":"Terraform Scripts","text":"<p>BFD-Insights scripts to create and modify all AWS resources. Although some provisioning is done within QuickSight, the Terraform scripts implement the vast majority of the provisioning in the BFD-Insights project.    </p>"},{"location":"insights/terraform.html#top-level-folders","title":"Top Level Folders","text":"<ul> <li>modules:  Common modules between deployment </li> <li>prod-lake: Create the buckets, workgroups, and other resources. Must be deployed before projects. </li> <li>group: Create users and their membership into groups. Depends on prod-lakes. </li> <li>test-lake: The pipelines and database for testing ETL scripts. </li> <li>projects: Individual projects which create workflows. Depends on prod-lakes. </li> </ul>"},{"location":"insights/terraform.html#modules","title":"Modules","text":"<p>Various code resource modules. These modules should be generic to projects. Modules should form an abstraction. </p>"},{"location":"insights/terraform.html#projects","title":"Projects","text":"<p>Project folders may contain: - main.tf required main entry point - variables.tf optional inputs - outputs.tf optional outputs - modules optional sub-modules for the project </p>"},{"location":"insights/typical_workflow.html","title":"Example Workflow","text":"<p>This is example conceptual data flow for a DASG dashboard which pulls from multiple projects. It uses the common event flow. </p>"},{"location":"rfcs/index.html","title":"RFCs","text":"<p>Each \"Request for Comment (RFC)\" is a proposal for a substantial change to our systems and/or processes. The RFC process is intended to provide some formalization around such proposals,   allowing the entire team a chance to review them and to have a voice in such decisions before they're made. It's also intended to democratize the process: anyone can create an RFC.</p> <p>Pragmatically: such decisions need review and input, and GitHub pull requests are a great system for enabling that.</p>"},{"location":"rfcs/index.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>RFCs</li> <li>Table of Contents</li> <li>When Is an RFC Needed?</li> <li>Before Creating an RFC</li> <li>The RFC Process<ul> <li>Motion for Final Comment</li> <li>Postponement</li> <li>Implementation</li> </ul> </li> <li>Inspiration</li> </ul>"},{"location":"rfcs/index.html#when-is-an-rfc-needed","title":"When Is an RFC Needed?","text":"<p>What counts as \"substantial\"? Things like the following:</p> <ul> <li>A major new feature.</li> <li>Deprecation or removal of existing features.</li> <li>Most changes that would require a Security Impact Aseessment (SIA).</li> <li>Major changes to our processes, particularly any that might break team members' existing workflows.</li> </ul> <p>When (reasonably) in doubt, create an RFC first, as the lack of one might hold up merging your PRs.</p>"},{"location":"rfcs/index.html#before-creating-an-rfc","title":"Before Creating an RFC","text":"<p>Things to do before drafting your RFC:</p> <ol> <li>Review the security guidance on Confluence: Open Source and Public Code in OEDA.<ol> <li>Part of the motivation for this public RFC process is that CMS views being open &amp; transparent as an important in and of themselves.</li> <li>Nonetheless, those goals need to balanced against the risks they present: both from attackers and from a public relations perspective.</li> <li>When in doubt, please run your draft RFC by your CMS Information System Security Officer (ISSO) prior to sharing it publicly.</li> </ol> </li> <li>Have a conversation with your company or program Information System Security Officer (ISSO) about your proposal.<ol> <li>Which parts of the proposal might be For Official Use Only (FOUO)? Those parts will need to be excluded, or instead put into a separate, encrypted addenda.</li> <li>Which parts of the proposal might be System Configuration Management Information (SCMI)?    Those parts will need to be excluded, or instead put into a separate, encrypted addenda.</li> </ol> </li> <li>Have a conversation with your company and program tech leads.<ol> <li>Would they be likely to support your proposal?</li> <li>If not, what objections would you need to overcome to get them to vote in favor of it?</li> </ol> </li> <li>Have a conversation with other relevant program and/or community members.<ol> <li>Would they be likely to support your proposal?</li> <li>If not, what objections would you need to overcome to get them to vote in favor of it?</li> </ol> </li> </ol> <p>There'a no harm in throwing up an RFC as a \"trial balloon\" but a much faster way to get feedback on an idea first... is to talk about it with others. Accordingly, you ought to have these conversations (e.g. in Slack) first, before publishing your RFC.</p>"},{"location":"rfcs/index.html#the-rfc-process","title":"The RFC Process","text":"<p>Create and draft your RFC, as follows:</p> <ol> <li>Copy <code>rfcs/0000-template.md</code> to <code>rfcs/0000-my-feature.md</code> (where \"my-feature\" is descriptive. don't assign an RFC number yet).</li> <li>Fill in the RFC.    Put care into the details and follow the template as much as is reasonable.    Try hard to honestly evaluate the benefits and drawbacks; disingenuous RFCs are likely to be poorly received.</li> <li>Submit a GitHub pull request with your new RFC. Be prepared to receive and incorporate feedback!</li> <li>Build consensus and incorporate feedback.    Not every RFC will be approved/merged!    If you want your beautiful baby RFC to leave the nest, you'll need to win support for it.<ul> <li>Do not squash or merge commits; we want all of the context, conversation, and history to stick around for the benefit of future team members.</li> </ul> </li> <li>Expect surprise.    It's pretty unlikely that the first version of your RFC was right on the mark;      the final result may be very different (and better!).</li> </ol>"},{"location":"rfcs/index.html#motion-for-final-comment","title":"Motion for Final Comment","text":"<p>Once your RFC has been sufficiently reviewed and edited via the Pull Request process,   you will likely want a final decision on it: Does the team wish to accept this proposal? To reach that decision, you or anyone can propose a \"motion for final comment\",   along with a disposition: merge, close, postpone.</p> <ul> <li>All relevant engineers receive a vote.   If this is not obvious, the project's technical lead will decide.</li> <li>How is a decision reached?   Not consensus; that's not always possible.   Instead, a majority + 1 vote is required, with no vetos from relevant technical leads.</li> <li>Voting will not last longer than 5 business days. Abstenions count as \"nays\".</li> </ul> <p>If the RFC receives enough \"aye\" votes, the PR will be noted as approved and merged. Be sure to assign it an official RFC number just prior to that merge.</p>"},{"location":"rfcs/index.html#postponement","title":"Postponement","text":"<p>Postponement is a way for the team to say, \"this is probably a good idea, but not for at least a couple of PIs.\" Postponed RFCs do not get merged, just closed. If/when the time is right, someone can reopen them or re-propose a similar RFC.</p>"},{"location":"rfcs/index.html#implementation","title":"Implementation","text":"<p>Your RFC has been approved and merged: yay, we're done! Wait a second... How's it get implemented?</p> <p>At this point, the RFC goes into the planning process:   epics, stories, etc. are created and prioritized. It is normal and expected (but not at all required!) that folks other than proposers will end up implementing RFCs.</p>"},{"location":"rfcs/index.html#inspiration","title":"Inspiration","text":"<p>This RFC process is largely derived from the following:</p> <ul> <li>Rust RFCs</li> <li>Swift Programming Language Evolution<ul> <li>Swift Evolution Process</li> <li>Swift Evolution Proposal Template</li> </ul> </li> </ul>"},{"location":"rfcs/deferred/0008-partial-backfill-ccw.html","title":"RFC Proposal","text":"<ul> <li>RFC Proposal ID: <code>0008-partial-backfill-ccw</code></li> <li>Start Date: 2020-01-04</li> <li>RFC PR: CMSgov/beneficiary-fhir-data#427</li> <li>JIRA Ticket(s):<ul> <li>BFD-589: Epic: Partial Backfill of CCW Records</li> </ul> </li> </ul> <p>BFD recently started receiving new claim fields from the CCW for new and updated claims,   but has not yet \"backfilled\" those fields into claims that were inserted prior to the change. The CCW team will, hopefully early in 2021,   be ready to send BFD the data required for that backfill. However, that backfill data will all be in new RIF layouts,   which will only contain the record primary keys and the fields to be backfilled. BFD does not support such layouts and will need to. Adding that fucntionality will be a moderate-to-major architectural change for BFD.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0008-partial-backfill-ccw.html#status","title":"Status","text":"<ul> <li>Status: Deferred</li> <li>Implementation JIRA Ticket(s): NONE</li> </ul>","tags":["RFC's"]},{"location":"rfcs/deferred/0008-partial-backfill-ccw.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>RFC Proposal</li> <li>Status</li> <li>Table of Contents</li> <li>Motivation</li> <li>Proposed Solution<ul> <li>Proposed Solution: Detailed Design</li> <li>Design Option A: Hacky Approach</li> <li>Design Option B: Overhaul the Existing Code</li> <li>Design Option C: Create a New Pipeline</li> <li>Proposed Solution: Unresolved Questions</li> <li>Proposed Solution: Drawbacks</li> <li>Proposed Solution: Notable Alternatives</li> </ul> </li> <li>Prior Art</li> <li>Future Possibilities</li> <li>Addendums</li> </ul>","tags":["RFC's"]},{"location":"rfcs/deferred/0008-partial-backfill-ccw.html#motivation","title":"Motivation","text":"<p>BFD peers and users need and expect this data to be there, even for older claims. To a large extent, we haven't really completed the work of adding the new fields until this backfill is performed.</p> <p>It's worth explaining why this will require moderate-mojor architectural changes to BFD,   as that is not obvious unless you're already familiar with BFD's architecture and code. The short of it is this: BFD relies heavily on the fixed set of nine (or so) RIF layouts that the CCW sends.</p> <p>Those layouts are fed into a source code generator that automatically produces:</p> <ul> <li>RIF/CSV parsing code to read those layouts.</li> <li>The database schema that those records will be inserted/update into.</li> <li>Java objects to model those DB tables (i.e. JPA/Hibernate entity classes).</li> <li>And, by extension, much of the code that performs the inserts and updates into the DB.</li> </ul> <p>This was a great design up until now! All of that automatic code generation was actually added as an enhancement   to remove a very large class of bugs that BFD kept running into. And it succeeded: the BFD team has not had to spend its time chasing down subtle copy-paste and data mapping bugs   that had been leading to pernicious (and expensive to resolve!) data corruption.</p> <p>The main shortcoming of the approach, though, is its current reliance on the fact that   the RIF layout matches the Java classes/objects that the CSV records are read into,   which in turn match the JPA entities that the CSV records become,   which in turn matches the DB schema that the records are inserted/updated into. Adding an extra layer in there, to allow for RIF layouts that don't match the JPA entities,   is not a small thing.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0008-partial-backfill-ccw.html#proposed-solution","title":"Proposed Solution","text":"<p>When the CCW sends \"merge\" data to BFD,   that data is loaded by the ETL systems like any other expected data,   without requiring any special operator intervention.</p> <p>Such data can be identified by inspecting the RIF manifest files in S3. For example, here's a sample manifest for such data   (note the \"<code>_MERGE</code>\" suffix on the <code>type</code> attribute):</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?&gt;\n&lt;dataSetManifest xmlns=\"http://cms.hhs.gov/bluebutton/api/schema/ccw-rif/v9\"\n  timestamp=\"1994-11-05T13:15:30Z\" sequenceId=\"42\"&gt;\n\n  &lt;entry name=\"sample-a-bcarrier.txt\" type=\"CARRIER_MERGE\" /&gt;\n\n&lt;/dataSetManifest&gt;\n</code></pre>","tags":["RFC's"]},{"location":"rfcs/deferred/0008-partial-backfill-ccw.html#proposed-solution-detailed-design","title":"Proposed Solution: Detailed Design","text":"<p>The detailed design is TBD;   I want to think this one through \"in code\", first, via a prototype or three.</p> <p>One thing I'm explicitly not doing is shying away from using this challenge   as a place to explore how we might also meet other current and upcoming architectural needs. This is an intentional decision: we need to start making progress on some of these needs/challenges   and this effort is as good an excuse as any. This additional exploration work will need to be balanced against the more urgent   need to actually ship the partial backfill solution, though.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0008-partial-backfill-ccw.html#design-option-a-hacky-approach","title":"Design Option A: Hacky Approach","text":"<p>I'm just mentioning this one for completeness' sake, as it is not suitable to a recurring need like this will be   (we may need to run a partial backfill quarterly, going forwards). That said, one approach is to just build a custom version of the BFD Pipeline where:</p> <ul> <li>most fields are <code>Optional</code></li> <li>most of the tests are removed (as they will fail to compile given the <code>Optional</code> change)</li> <li>the BFD Server is not built</li> <li><code>RifLoader</code> was configured to only run <code>_MERGE</code> data sets</li> </ul> <p>The BFD team would also have to carefully orchestrate things to ensure   that this custom build was produced and deployed at the correct time,   and then reverted/undeployed back to the normal BFD Pipeline after the backfill was complete.</p> <p>Why is this approach a bad idea? Well, as mentioned, it's very manual and would have to be repeated from scratch every quarter. Being so manual, it's extremely error-prone, which is bad by itself,   but given how painful a DB restore would be, is just far too risky.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0008-partial-backfill-ccw.html#design-option-b-overhaul-the-existing-code","title":"Design Option B: Overhaul the Existing Code","text":"<p>The second-most minimal approach would be to update the existing code to add a couple new layers to the ETL:</p> <ol> <li>Create separate structs/classes to represent parsed RIF records.</li> <li>Add a RIF-to-JPA entity mapping layer in that supports both partial and full loads.</li> </ol> <p>That's a brief description for some incredibly complex work. It'd require extensiive changes and extensions to our automatic source code generation,   which is by far the trickiest bit of code we have.</p> <p>There are a number of risks with this approach:</p> <ul> <li>It's a lot of tedious and tricky work.<ul> <li>This is true of all our options, but still worth mentioning.</li> </ul> </li> <li>It risks some performance degradation,     as moving data through those extra layers will require a lot of additional memory copies, GC pressure, etc.</li> <li>Keeping the changes current/merged with the main branch in Git will be very difficult.</li> </ul> <p>That said, it's not a bad approach if the backfill is the only thing we want to try and address.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0008-partial-backfill-ccw.html#design-option-c-create-a-new-pipeline","title":"Design Option C: Create a New Pipeline","text":"<p>This is the most far-reaching option: create a new BFD Pipeline application that   not only supports the partial backfill need   but also is architecturally in line with other needs we have coming for BFD's ETL. So far, those needs are:</p> <p>A. Support <code>_MERGE</code> data sets from the CCW to partially backfill new fields into older claims. B. Better orchestrate DB schema upgrades, to reduce the large risk and personnel stress that they currently incur. C. Provide more metadata and automatic generation of code and resources,      which can be consumed downstream to improve our internal and external developer documentation. D. Support data sources beyond just the CCW. E. Support our potential future performance and size scaling needs,      as the amounts and types of data being managed by BFD continues to increase. F. Automatically orchestrate with the current BFD Pipeline application      to ensure that only one ETL process is running at a time,      in order to avoid creating data races.</p> <p>There are a number of risks with this approach:</p> <ul> <li>It's a lot of complex architectural and implementation work.<ul> <li>I mean, it sounds like a lot of fun to me, but that's kinda' my thing.</li> <li>The risks of errors remina high, but those risks can be fully mitigated with an adequate focus on testing.</li> </ul> </li> <li>It took a while to tune the current ETL system to achieve its current level of performance.   Unless the lessons from that effort are carried forward into this work,     this approach risks performance degradation.</li> <li>This will require a solid &amp; consistent investment of time and effort from one or more senior engineers.<ul> <li>This risk can be mitigated by keeping the initial prototypes small and flexible,     to prove out and test the approach before too much investment is made into a full implementation.</li> </ul> </li> </ul> <p>Something that will need to explored and decided on with this option is whether   to go with a bespoke ETL application that feeds directly into the database   or to instead build on top of open source orchestration and messaging platforms,   such as Apache Airflow for orchestration   and Apache Kafka for pub/sub messaging. Here's how I'm framing this question right now,   when I reach out and ask other folks for their input:</p>  <p>This year, we need to add some major new functionality to our data/ETL pipeline,   to meet new business needs. It\u2019s enough of a change from before that it\u2019s one of those rare (for me)   \u201cwell, maybe we should just rewrite it,\u201d moments.</p> <p>I\u2019m considering moving towards something more dependent on frameworks, such as Airflow and/or Kafka. What we have right now is reasonably simple bespoke code (Java). But we\u2019re going to need to orchestrate several different types of ETL and ensure (best we can, anyways)   that the end result is consistent &amp; safe and I suspect that Airflow and/or Kafka may help. We\u2019re an AWS-only shop, if that matters.</p> <p>Does anyone here have experience with those two frameworks/tools for use in ETL and have opinions?</p> <p>FWIW, I\u2019m normally pretty anti-framework for ETL;   most frameworks I\u2019ve tried in the past required so much custom code, anyways,   that it was hard to see the point \u2014   especially once you account for all the time you\u2019ll spend debugging the tools.</p>  <p>It's worth noting that AWS does appear to offer managed variants for both Airflow and Kafka.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0008-partial-backfill-ccw.html#proposed-solution-unresolved-questions","title":"Proposed Solution: Unresolved Questions","text":"<p>Collect a list of action items to be resolved or officially deferred before this RFC is submitted for final comment, including:</p> <ul> <li>For Design Option C, what exactly is being proposed? The current explanation is a bit vague.</li> </ul>","tags":["RFC's"]},{"location":"rfcs/deferred/0008-partial-backfill-ccw.html#proposed-solution-drawbacks","title":"Proposed Solution: Drawbacks","text":"<p>TODO</p> <p>Why should we not do this?</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0008-partial-backfill-ccw.html#proposed-solution-notable-alternatives","title":"Proposed Solution: Notable Alternatives","text":"<p>TODO</p> <ul> <li>Why is this design the best in the space of possible designs?</li> <li>What other designs have been considered and what is the rationale for not choosing them?</li> <li>What is the impact of not doing this?</li> </ul>","tags":["RFC's"]},{"location":"rfcs/deferred/0008-partial-backfill-ccw.html#prior-art","title":"Prior Art","text":"<p>TODO: Discuss BFD Insights' approach (AWS Glue, I think?).</p> <p>The following discussion is very relevant to this RFC:   Hacker News: How to Become a Data Engineer in 2021.</p> <p>Here are my notes from the original article:</p> <ul> <li>Older tools such as Informatica, Pentaho, and Talend are characterized as legacy approaches.</li> <li>I don't really buy the assertin that data engineers need to be practiced with     a DB's underlying DB structures and algorithms, e.g. B-trees.   That said, I would agree that they should be at least passingly familiar with them.<ul> <li>I think the most important insight from this is really related to index caching:     modern DBs will try to keep \"hot\" index pages cached in memory     and may exhibit pathological behavior when they can't.   Which portions of the trees are likely to be \"hot\"?   The upper levels of the tree, as bounded by the system's page size.</li> <li>This insight has been particularly important when interacting with PostgreSQL's query planner.   If the DB determines that it can't keep \"enough\" index pages in memory, it will refuse to use the index.   In addition to being frustrating due to the poor visibility developers have into this behavior,     it also cautions against viewing table partiioning as a silver bullet:     there's no reason to assume that simply having more, smaller index trees will perform any better.   And on the flip side, it points towards DB sharding as potentially being necessary in the future.   If our indices ever outgrow what we can fit into memory on large RDS instances,     sharding seems like a likely (albeit expensive) solution.   To be clear, I'm still of the opinion that we're a long way away from needing to shard,     but it's worth keeping in mind for the future,     in addition to evaluating alternative DB platforms.</li> </ul> </li> <li>Both the article and discussion repeatedly make the point that SQL is an essential technology.   This rings true: it is still clearly the best tool for many data problems.</li> <li>The article calls out Python's poor performance as a concern.   I share this concern, but think it's nevertheless worth exploring Airflow and other Python-based options.</li> <li>When reading the article's \"Big Data Tools\" section it's worth keeping in mind     what problems we are trying to solve for BFD.</li> <li>Problems we don't have:<ul> <li>We can invent an event streaming problem for ourselves but we don't intrinsically need to apply that technique.</li> <li>We don't have much in the way of data processing to do.</li> <li>We don't need an analytics platform.</li> </ul> </li> <li>Problems we do have:<ul> <li>We're doing a massive amount of very simple ETL under modest time constraints.</li> <li>Actually, it's mostly \"EL\" not \"ETL\": we don't want to apply many data transformations at load time.   If we had to reload/reprocess all records every time we changed our mapping we would be in a very bad place.</li> </ul> </li> <li>It's also worth keeping in mind the scale of our systems:   We have terabytes of data but not petabytes.   Billions of records but not trillions.   We're not really a big data system, as such.   Instead, BFD is just a data-lake-sized online database, heavily optimized to support a limited number of query types.</li> </ul> <p>Here are my notes from the article's discussion on HN:</p> <ul> <li>Lots of mentions of Snowflake, though that doesn't seem germane to the problems we're looking at here.   (Worth considering later, though.)</li> <li>dbt sounds interesting, but again: we don't want to do much transformation prior to load.<ul> <li>If we ever wanted to dual-purpose the DB as an analytics platform, I think we should look at dbt.</li> </ul> </li> <li>Fivetran sounds interesting, but appears to not offer a hosted option,     and is thus a non-starter, unless/until they get FedRAMP'd.</li> <li>It references this,     Emerging Architectures for Modern Data Infrastructure,     which is interesting in general, but also has the a useful new (to me) acronym:     \"ELT\" for \"extract, then load, then transform\" and calls it out as being less brittle than traditional ETL.   Nice term for capturing what we do in BFD.</li> <li>A comment mentioned \"Data Vault\", which turned out to be an interesting read:     Data vault modeling.   I'm not sold on the suggested storage structure, but the underlying philosophy makes sense.</li> <li>These two comments ring true: https://news.ycombinator.com/item?id=25733701     and https://news.ycombinator.com/item?id=25732147.   Developers uncomfortable with SQL should be encouraged and supported to \"push through\" that.</li> <li>AWS Step Functions appear to be the preferred approach     when going serverless.</li> <li>As a complete sidenote, I wandered across this very useful article while reading this discussion and related items:     We\u2019re All Using Airflow Wrong and How to Fix It.   It makes the case that Airflow's built-in operators are buggy and hard to debug     and argues for instead using just the Kubernetes operator to run custom code for every task.   It's a compelling argument, especially since we could just as easily substitute in Docker, instead.</li> </ul>","tags":["RFC's"]},{"location":"rfcs/deferred/0008-partial-backfill-ccw.html#future-possibilities","title":"Future Possibilities","text":"<p>TODO</p> <p>Think about what the natural extension and evolution of your proposal would be and how it would affect the language and project as a whole in a holistic way. Try to use this section as a tool to more fully consider all possible interactions with the project and language in your proposal. Also consider how the this all fits into the roadmap for the project and of the relevant sub-team.</p> <p>This is also a good place to \"dump ideas\", if they are out of scope for the RFC you are writing but otherwise related.</p> <p>If you have tried and cannot think of any future possibilities, you may simply state that you cannot think of anything.</p> <p>Note that having something written down in the future-possibilities section is not a reason to accept the current or a future RFC;   such notes should be in the section on motivation or rationale in this or subsequent RFCs. The section merely provides additional information.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0008-partial-backfill-ccw.html#addendums","title":"Addendums","text":"<p>TODO</p> <p>The following addendums are required reading before voting on this proposal:</p> <ul> <li>(none at this time)</li> </ul> <p>Please note that some of these addendums may be encrypted. If you are unable to decrypt the files, you are not authorized to vote on this proposal.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0012-rda-claims-jsonb-DEFERRED.html","title":"RFC Proposal","text":"<ul> <li>RFC Proposal ID: <code>0012-rda-claims-jsonb</code></li> <li>Start Date: 2021-10-18</li> <li>RFC PR: CMSgov/beneficiary-fhir-data#794</li> <li>JIRA Ticket(s):<ul> <li>DCGEO-219</li> </ul> </li> </ul> <p>The current schema uses a normalized relational structure that requires seven tables (4 for FISS and 3 for MCS) plus accompanying foreign keys and indexes. More tables and indexes will be needed in the near future as the data returned by the RDA API expands. By switching to a single table per type of claim with a JSONB column to store all of a claim's data in a single column we can simplify the schema and improve performance. Benchmarking on a local postresql database saw ingestion rates 3-5 times faster with this structure.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0012-rda-claims-jsonb-DEFERRED.html#status","title":"Status","text":"<ul> <li>Status: Deferred</li> </ul> <p>This RFC has been deferred for the following reasons:</p> <ul> <li>The performance improvement on a production Aurora cluster was less significant than that on a local postgresql database.</li> <li>A normalized database schema has some advantages for data exploration that could be helpful during the early phases of RDA production data ingestion.</li> <li>There was some interest in adopting JSONB for adjudicated claims at the same time.  This would require more experimentation and impact assessment before moving forward.</li> </ul> <p>Note: The PR containing a proof of concept implementation has been closed but should still be available in github.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0012-rda-claims-jsonb-DEFERRED.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>RFC Proposal</li> <li>Status</li> <li>Table of Contents</li> <li>Motivation</li> <li>Proposed Solution<ul> <li>Proposed Solution: Detailed Design</li> <li>Proposed Solution: Unresolved Questions</li> <li>Proposed Solution: Drawbacks</li> <li>Proposed Solution: Notable Alternatives</li> </ul> </li> <li>Prior Art</li> <li>Future Possibilities</li> <li>Addendums</li> </ul>","tags":["RFC's"]},{"location":"rfcs/deferred/0012-rda-claims-jsonb-DEFERRED.html#motivation","title":"Motivation","text":"<p>Normalized relational databases provide great flexibility for querying relational data using joins and/or sub-queries. However the BFD database exists solely to serve the BFD API server and the queries that it needs for its operations. These queries generally consist of a simple query on one or two columns from the claim table followed by reading all of the associated detail data into a DTO object for each claim to allow delivery to clients. Consolidating the records from the normalized relational structure into an object graph requires JPA to perform extra database queries and extra work in memory to assemble the records from detail tables.</p> <p>Postgresql (and Amazon Aurora) supports storing object graphs as JSON directly in a single column of a record. Using this feature will allow the use of only one table per claim type. The records in this table would contain the minimum number of columns required to support BFD API queries or data analytics plus one additional column to hold the entire claim as JSON. With this structure JPA would be able to find and retrieve an entire claim using only one query. Also the work performed in memory to convert the claim JSON into an object graph would be simpler since the heirarchical structure of the graph directly matches that of the JSON.</p> <p>Benchmarking a prototype of this concept against a local postgresql database revealed that claims could be ingested 5.7x faster for FISS claims and 3.5x faster for MCS claims. The larger throughput improvement for FISS claims corresponds to the greater complexity of the FISS schema (4 tables) vs MCS (3 tables) in the normalized relational schema.</p> <p>In addition to acheiving higher throughput during claim ingestion, the JSONB based schema results in a simpler database structure. That simpler schema (1 table each for FISS and MCS claims) would require far less maintenance over time. Table changes would only be required when a new type of query is added to BFD API. Addition of new fields and sub-objects to the claim data returned by the RDA API would not require any schema migration since that data would simply change the JSON written to the JSONB column. Contrast this to adding a new field containing multiple sub-objects (e.g. payers) to MCS claims. With a normalized relational schema this would require adding a new table to hold the individual payer records. Along with that extra table the database would also have to maintain additional indexes and foreign/primary key constraints.</p> <p>Because postgresql supports directly querying fields within the JSONB column the new schema would still allow ad-hoc queries to be used for data exporation or analysis.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0012-rda-claims-jsonb-DEFERRED.html#proposed-solution","title":"Proposed Solution","text":"<p>This section discusses changes specifically for FISS claims.  The changes for MCS claims are directly analagous to the ones for FISS.</p> <p>The new schema simplifies the normalized one for by reducing the table count from four:</p> <pre><code>+------------------------------------------------------+\n|                                                      |\n|                      FISS CLAIM                      |\n|                                                      |\n+-------+------------------+-------------------+-------+\n        |                  |                   |\n        |                  |                   |\n        |                  |                   |\n+-------v-------+  +-------v-------+   +-------v-------+\n|               |  |               |   |               |\n|   DIAG CODE   |  |     PAYER     |   |   PROC CODE   |\n|               |  |               |   |               |\n+---------------+  +---------------+   +---------------+\n</code></pre> <p>To one:</p> <pre><code>+------------------------------------------------------+\n|                                                      |\n|                      FISS CLAIM                      |\n|                                                      |\n+------------------------------------------------------+\n</code></pre> <p>The single table for FISS claims has a small number of columns: - <code>dcn</code>: primary key - <code>mbi</code>: to support MBI query - <code>mbiHash</code>: to support hashed MBI query - <code>stmtCovToDate</code>: to support date query - <code>sequenceNumber</code>: to track version of the claim - <code>lastUpdated</code>: to track last time the record was updated - <code>claim</code>: entire claim object as JSON</p> <p>Each of the queryable non-JSON columns has an associated index. The <code>claim</code> column is not indexed.</p> <p>When the API receives a request for a claim with a particular MBI a simple query is performed on that column. The <code>claim</code> value of the matching record is returned as part of the query. In memory this JSON is converted into an object graph using an open source library. This conversion happens seamlessly using a JPA feature, the <code>Convert</code> field annotation.</p> <p>Code changes are minimal across the BFD code base since the claim data objects are simply converted from JPA entities to Plain Old Java Objects by removing the JPA annotations. Two new entity classes are added for the two remaining database tables. These have a field for each of the queryable columns plus a field to hold the claim POJO. Clients perform queries using the new entity class but then use the returned POJO exactly as they previously used the old JPA entities.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0012-rda-claims-jsonb-DEFERRED.html#proposed-solution-detailed-design","title":"Proposed Solution: Detailed Design","text":"<p>This section discusses changes specifically for FISS claims.  The changes for MCS claims are directly analagous to the ones for FISS.</p> <p>The <code>bfd-model-rda</code> objects are modified as follows:</p> <ul> <li>All of the JPA annotations are removed from the existing JPA entity classes (PreAdjFissClaim, PreAdjFissPayer, etc).</li> <li>All of the static inner classes for composite keys are removed from the existing entity classes as well.</li> <li>Redundant fields are removed from the detail objects (e.g. <code>dcn</code> is no longer needed in <code>PreAdjFissPayer</code>).</li> <li>One new JPA entity class is added for each claim type (<code>PreAdjFissClaimJson</code> and <code>PreAdjMcsClaimJson</code>).  Details below.</li> <li>An abstract base class implementing the JPA <code>AttributeConverter</code> interface is added.  Details below.</li> <li>A concrete implementation of this base class is added for each claim type (<code>PreAdjFissClaimConverter</code> and <code>PreAdjMcsClaimConverter</code>).</li> </ul> <p>The database schema is simplified to a single table for each claim type:</p> <p><pre><code>create table \"pre_adj\".\"FissClaimsJson\" (\n    \"dcn\"            varchar(23)   not null,\n    \"mbi\"            varchar(13),\n    \"mbiHash\"        varchar(64),\n    \"stmtCovToDate\"  date,\n    \"sequenceNumber\" bigint        not null,\n    \"lastUpdated\"    timestamp with time zone,\n    \"claim\"          ${type.jsonb} not null,\n    constraint \"FissClaimsJson_pkey\" primary key (\"dcn\")\n);\n</code></pre> The schema also contains one index for each of the queryable columns.</p> <p>Note the template macro for the <code>claim</code> column type. This macro is used since the type of the column will be different in postgresql vs HSQLDB: - In postgresql the <code>JSONB</code> type is used to allow use of JSON path querying for ad-hoc queries. - In HSQLDB <code>longvarchar</code> is used since <code>JSONB</code> is not supported for that database. With this macro in place integration and unit tests can work with either postgresql or HSQLDB with no code changes.</p> <p>The JPA entity for the <code>FissClaimsJson</code> object is correspondingly simple:</p> <pre><code>@Entity\n@Getter\n@Setter\n@EqualsAndHashCode(onlyExplicitlyIncluded = true)\n@Builder\n@AllArgsConstructor\n@NoArgsConstructor\n@FieldNameConstants\n@Table(name = \"`FissClaimsJson`\", schema = \"`pre_adj`\")\npublic class PreAdjFissClaimJson {\n  public PreAdjFissClaimJson(PreAdjFissClaim claim) {\n    this(\n        claim.getDcn(),\n        claim.getMbi(),\n        claim.getMbiHash(),\n        claim.getStmtCovToDate(),\n        claim.getLastUpdated(),\n        claim.getSequenceNumber(),\n        claim);\n  }\n\n  @Id\n  @Column(name = \"`dcn`\", length = 23, nullable = false)\n  @EqualsAndHashCode.Include\n  private String dcn;\n\n  @Column(name = \"`mbi`\", length = 13)\n  private String mbi;\n\n  @Column(name = \"`mbiHash`\", length = 64)\n  private String mbiHash;\n\n  @Column(name = \"`stmtCovToDate`\")\n  private LocalDate stmtCovToDate;\n\n  @Column(name = \"`lastUpdated`\", nullable = false)\n  private Instant lastUpdated;\n\n  @Column(name = \"`sequenceNumber`\", nullable = false)\n  private Long sequenceNumber;\n\n  @Column(name = \"`claim`\", nullable = false, columnDefinition = \"jsonb\")\n  @Convert(converter = PreAdjFissClaimConverter.class)\n  private PreAdjFissClaim claim;\n}\n</code></pre> <p>Although the underlying column type is JSONB, the <code>claim</code> field in the entity is the root POJO for a claim. JPA supports automatic conversion between any database type and a Java type through the <code>@Convert</code> annotation. This annotation tells JPA what class to use to perform the conversion.</p> <p>The implementation of this converter uses Jackson to convert between an object and JSON. The code to call Jackson is generic so a single base class supports any type of POJO. A concrete subclass is defined for each POJO type and simply calls the base class constructor to set the correct <code>Class&lt;T&gt;</code> for the POJO.</p> <pre><code>public class AbstractJsonConverter&lt;T&gt; implements AttributeConverter&lt;T, String&gt; {\n  /**\n   * {@code ObjectMapper} instances are thread safe so this singleton instance ensures consistent\n   * formatting behavior for all instances.\n   */\n  private static final ObjectMapper objectMapper =\n      new ObjectMapper()\n          .enable(SerializationFeature.INDENT_OUTPUT)\n          .registerModule(new Jdk8Module())\n          .registerModule(new JavaTimeModule())\n          .configure(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS, false)\n          .setSerializationInclusion(JsonInclude.Include.NON_NULL);\n\n  private final Class&lt;T&gt; klass;\n\n  protected AbstractJsonConverter(Class&lt;T&gt; klass) {\n    this.klass = klass;\n  }\n\n  @Override\n  public String convertToDatabaseColumn(T attribute) {\n    return objectToJson(attribute);\n  }\n\n  @Override\n  public T convertToEntityAttribute(String dbData) {\n    return jsonToObject(dbData);\n  }\n\n  @Nullable\n  private String objectToJson(@Nullable Object value) {\n    try {\n      if (value == null) {\n        return null;\n      }\n      return objectMapper.writeValueAsString(value);\n    } catch (final Exception ex) {\n      throw new RuntimeException(\n          String.format(\"Failed to convert %s to JSON: %s\", klass.getSimpleName(), ex.getMessage()),\n          ex);\n    }\n  }\n\n  @Nullable\n  private T jsonToObject(@Nullable String value) {\n    try {\n      if (value == null) {\n        return null;\n      }\n      return objectMapper.readValue(value, klass);\n    } catch (final Exception ex) {\n      throw new RuntimeException(\n          String.format(\"Failed to convert JSON to %s: %s\", klass.getSimpleName(), ex.getMessage()),\n          ex);\n    }\n  }\n}\n</code></pre> <p>Changes outside of the <code>bfd-model-rda</code> module are straight forward. For example, in the <code>bfd-server-war</code> module's <code>pre_adj</code> package the following are changed:</p> <ul> <li><code>ClaimTypeV2</code> and <code>ClaimResponseTypeV2</code> are modified to use the new entity class and retrieve the root POJO from its <code>claim</code> field.</li> <li><code>FissClaimTransformerV2</code> and <code>FissClaimResponseTransformerV2</code> are modified to reference the new entity class and retrieve the root POJO from its <code>claim</code> field.</li> </ul> <p>Other than those changes and a few test changes the rest of the server module is unchanged.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0012-rda-claims-jsonb-DEFERRED.html#proposed-solution-unresolved-questions","title":"Proposed Solution: Unresolved Questions","text":"<p>Collect a list of action items to be resolved or officially deferred before this RFC is submitted for final comment, including:</p> <p>None.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0012-rda-claims-jsonb-DEFERRED.html#proposed-solution-drawbacks","title":"Proposed Solution: Drawbacks","text":"<p>Drawback 1: This is a departure from existing practice in the BFD database schema.</p> <p>As such it creates code and design differences that could trip up new developers. However this is offset by: - The database schema is greatly simplified. - The likelihood of schema migrations being needed for future RDA API changes are greatly reduced. - The implementation is simple to understand and would work as-is with other entities in the future if desired.</p> <p>Drawback 2: Postgresql JSONB columns store the full JSON in binary form.</p> <p>This increases storage reqirements for the schema since the field names in the JSON are repeated in every record. The same benchmark that measured faster ingestion rates also measured increased storage requirements: - Storage per FISS claim increased from 1,460 bytes per claim to 1,958. - Storage per MCS claim increased from 1,036 bytes per claim to 1,822. The size measurement was not extremely precise as the size increased between updates. This variation was likely due to internal postgresql details.</p> <p>Although the overal storage in the database was increased somewhat with JSONB the overall performance would not be adversely affected. Since the JSONB column is not indexed, the size of the index for each column would be the same with JSONB as with the current schema. Overall I/O with the database should be lower with this schema since only one `SELECT`` is needed for each claim rather than one for the main table plus one for each detail table.</p> <p>Drawback 3: Without the relational structure we can't perform queries in the normal way.</p> <p>Standard SQL query syntax for fields within the JSON would not be available however postgresql provides a set of operators and functions for querying information within a <code>JSONB</code> column. This feature is documented here: https://www.postgresql.org/docs/12/functions-json.html</p> <p>The functions and operators are different than what a SQL user may be used to but they allow full access to values contained in the JSON. Postgresql also supports building indexes from properties of objects stored in a JSONB column although we plan to copy often used values into separate columns within the record so that BFD API queries don't have to depend on non-standard query syntax.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0012-rda-claims-jsonb-DEFERRED.html#proposed-solution-notable-alternatives","title":"Proposed Solution: Notable Alternatives","text":"<p>Since this proposal could be boiled down to using a NoSQL approach for FISS and MCS claims why not go all the way? A case could be made to move this data into Amazon's DynamoDB since it is built for this sort of design. While there could be advantages to that approach the proposed design has some practical advantages: - Using postgresql allows us to reap the benefits of a NoSQL approach without the need to add a whole new database technology to BFD. - RDS is already approved for storing PII so the changes can be implemented at any time without having to wait for review and approval of DynamoDB.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0012-rda-claims-jsonb-DEFERRED.html#prior-art","title":"Prior Art","text":"<p>None</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0012-rda-claims-jsonb-DEFERRED.html#future-possibilities","title":"Future Possibilities","text":"<p>Assuming this proposal works as expected the same technique could ultimately be applied to the rest of the BFD database.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0012-rda-claims-jsonb-DEFERRED.html#addendums","title":"Addendums","text":"<p>The following addendums are required reading before voting on this proposal:</p> <ul> <li>(none at this time)</li> </ul> <p>Please note that some of these addendums may be encrypted. If you are unable to decrypt the files, you are not authorized to vote on this proposal.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0016-cursor-paging.html","title":"RFC Proposal","text":"<ul> <li>RFC Proposal ID: <code>0016-cursor-paging.md</code></li> <li>Start Date: 2020-04-01</li> <li>RFC PR: beneficiary-fhir-data/rfcs#0016</li> <li>JIRA Ticket(s):</li> <li>BFD-109</li> </ul>","tags":["RFC's"]},{"location":"rfcs/deferred/0016-cursor-paging.html#status","title":"Status","text":"<ul> <li>Status: Deferred</li> <li>Implementation JIRA Ticket(s): NONE</li> </ul>","tags":["RFC's"]},{"location":"rfcs/deferred/0016-cursor-paging.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>RFC Proposal</li> <li>Status</li> <li>Table of Contents</li> <li>Motivation</li> <li>Proposed Solution<ul> <li>Client Perspective</li> <li>Experimental Results</li> <li>Rollout</li> <li>Proposed Solution: Detailed Design</li> <li>Proposed Solution: Unresolved Questions</li> <li>Proposed Solution: Drawbacks</li> <li>Proposed Solution: Notable Alternatives</li> </ul> </li> <li>Prior Art</li> <li>Future Possibilities</li> <li>Addendums</li> </ul>","tags":["RFC's"]},{"location":"rfcs/deferred/0016-cursor-paging.html#motivation","title":"Motivation","text":"<p>This RFC changes how the BFD handles requests for explanation of benefits (EOB). Internally, it uses a more efficient database query to fetch EOBs. For partners, it provides a way for BFD clients to request resources with lower timeouts and higher throughput.</p> <p>The latency of the BFD's EOB search is directly related to the number of claims a beneficiary has.</p> <p>Since the number of claims per beneficiary varies widely, a BFD client must set 10 to 30-second timeout on their EOB fetches to accommodate a large number of claims in the result set.</p> <p>Paging is the term given to the technique to split a single large result set into many smaller result sets. Since smaller result sets take less processing time, paging should imply lower request latency. The FHIR specification and the BFD API support paging. Unfortunately, the BFD's current paging algorithm is an inefficient implementation of offset paging, where the BFD data server queries for the entire result set and then discards the results outside of the requested page.</p> <p>The latency variability (also known as jitter) of a BFD EOB request leads to several problems: - As mentioned, it requires the BFD client to set long timeout values - SLO and SLA latency metrics are difficult to specify because normal operations may take a long time to complete - Request retries, usually a good practice in distributed systems, can lead to a very wasteful failure-mode where computationally expensive partial results are repeatedly discarded. - Jitter adversely affects the throughput of a system.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0016-cursor-paging.html#proposed-solution","title":"Proposed Solution","text":"<p>The proposal is to replace the current offset paging algorithm with a cursor-based algorithm. Internally, the BFD can implement cursor-based paging with an efficient seek query. From a database perspective, the work done is nearly constant for each request.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0016-cursor-paging.html#client-perspective","title":"Client Perspective","text":"<p>The proposal slightly changes the bundle resource returned by the BFD when a _count parameter is specified. The <code>total</code> field and the <code>last</code> and <code>prev</code> links are no longer present. The contents of the bundle's next link have a <code>cursor</code> parameter instead of a <code>startIndex</code> parameter. The proposal does not alter the EOB resources themselves. We expect clients to need little or no code change for this proposal if they are already using paging in their requests.</p> <p>Here an example request that requests results with paging. <pre><code>$curl 'https://&lt;host&gt;/v1/fhir/ExplanationOfBenefit?\n_format=application%2Fjson%2Bfhir&amp;_count=10&amp;patient=-19990000000001'\n</code></pre></p> <p>Here is a partial result with the current offset paging scheme.</p> <pre><code>{\n  \"resourceType\": \"Bundle\",\n  \"id\": \"6c027b21-67e6-480f-a515-78cb41d362aa\",\n  \"meta\": {\n    \"lastUpdated\": \"2020-03-31T19:37:08.147-04:00\"\n  },\n  \"type\": \"searchset\",\n  \"total\": 52,\n  \"link\": [\n    {\n      \"relation\": \"first\",\n      \"url\": \"https://localhost:7443/v1/fhir/ExplanationOfBenefit?_format=application%2Fjson%2Bfhir&amp;startIndex=0&amp;_count=10&amp;patient=-19990000000001\"\n    },\n    {\n      \"relation\": \"next\",\n      \"url\": \"https://localhost:7443/v1/fhir/ExplanationOfBenefit?_format=application%2Fjson%2Bfhir&amp;startIndex=10&amp;_count=10&amp;patient=-19990000000001\"\n    },\n    {\n      \"relation\": \"last\",\n      \"url\": \"https://localhost:7443/v1/fhir/ExplanationOfBenefit?_format=application%2Fjson%2Bfhir&amp;startIndex=50&amp;_count=10&amp;patient=-19990000000001\"\n    },\n    {\n      \"relation\": \"self\",\n      \"url\": \"https://localhost:7443/v1/fhir/ExplanationOfBenefit?_count=10&amp;_format=application%2Fjson%2Bfhir&amp;patient=-19990000000001\"\n    }\n  ],\n  \"entry\": [\n    {\n      \"resource\": {\n        \"resourceType\": \"ExplanationOfBenefit\",\n        \"id\": \"carrier-10344810963\",\n        \"meta\": {\n          \"lastUpdated\": \"2020-03-31T19:29:46.332-04:00\"\n        },\n        \"extension\": [\n          {\n            \"url\": \"https://bluebutton.cms.gov/resources/variables/prpayamt\",\n            \"valueMoney\": {\n              \"value\": 0.00,\n              \"system\": \"urn:iso:std:iso:4217\",\n              \"code\": \"USD\"\n            }\n</code></pre> <p>Here is the partial result for the same request when cursor paging is enabled.</p> <pre><code>{\n  \"resourceType\": \"Bundle\",\n  \"id\": \"a18ca539-b4bb-4385-956f-1f977acd4351\",\n  \"meta\": {\n    \"lastUpdated\": \"2020-03-31T19:37:08.147-04:00\"\n  },\n  \"type\": \"searchset\",\n  \"link\": [\n    {\n      \"relation\": \"first\",\n      \"url\": \"https://localhost:7443/v1/fhir/ExplanationOfBenefit?_format=application%2Fjson%2Bfhir&amp;_count=10&amp;patient=-19990000000001\"\n    },\n    {\n      \"relation\": \"next\",\n      \"url\": \"https://localhost:7443/v1/fhir/ExplanationOfBenefit?_format=application%2Fjson%2Bfhir&amp;_count_=10&amp;patient=-19990000000001&amp;cursor=carrier_9324614917\"\n    },\n    {\n      \"relation\": \"self\",\n      \"url\": \"https://localhost:7443/v1/fhir/ExplanationOfBenefit?_format=application%2Fjson%2Bfhir&amp;_count_=10&amp;patient=-19990000000001\"\n    }\n  ],\n  \"entry\": [\n    {\n      \"resource\": {\n        \"resourceType\": \"ExplanationOfBenefit\",\n        \"id\": \"carrier-10344810963\",\n        \"meta\": {\n          \"lastUpdated\": \"2020-03-31T19:29:46.332-04:00\"\n        },\n</code></pre>","tags":["RFC's"]},{"location":"rfcs/deferred/0016-cursor-paging.html#experimental-results","title":"Experimental Results","text":"<p>A proof-of-concept implementation of the cursor paging was implemented and deployed to the BFD's TEST environment. The service was lightly loaded, but the test harness made requests in parallel with 4 workers.  The test fetched EOBs for a random 1000 synthetic beneficiaries for 5 minutes. Each request iterated over all the EOBs of a beneficiary. The results showed that cursor-based paging had significantly less latency variability.</p>     Mean P50 P90 P95 P99     No Paging 145ms 126ms 220ms 263ms 394ms   Offset Paging 91ms 86ms 117ms 132ms 155ms   Cursor Paging 24ms 17ms 58ms 63ms 73ms","tags":["RFC's"]},{"location":"rfcs/deferred/0016-cursor-paging.html#rollout","title":"Rollout","text":"<p>If this RFC is adopted, there will be phased rollout of cursor-based paging.</p> <ol> <li>Cursor based paging is experiemental. Both offset paging and cursor paging are supported. Default is offset paging. Cursor based paging is used when an optional <code>cursor</code> parameter is present.</li> <li>Cursor based paging is the default. Offset is still supported but its use is depricated.</li> <li>Cursor based paging is the only type of paging supported. Offset based paging is removed.</li> </ol>","tags":["RFC's"]},{"location":"rfcs/deferred/0016-cursor-paging.html#proposed-solution-detailed-design","title":"Proposed Solution: Detailed Design","text":"","tags":["RFC's"]},{"location":"rfcs/deferred/0016-cursor-paging.html#proposed-solution-unresolved-questions","title":"Proposed Solution: Unresolved Questions","text":"<ul> <li>With regards to the proposed implementation:</li> <li>How would these results hold up under a more realistic workload?</li> <li>Does this entail moving our bulk users to this model? How might it impact their total EOBs/second throughput?</li> <li>What about how this will fail during DB updates, where the DB replicas are experiencing lag and not consistent with each other? Is it any worse than the current behavior?</li> <li>It's mentioned latency will be reduced with the proposed paging implementation. Is throughput also addressed?</li> <li>Please provide more context on how jitter adversely affects the throughput of a system.</li> <li>In the proposed solution from the database perspective, what SQL is being executed? Current SQL queries? DB cursors? Or just seek based paging?</li> </ul>","tags":["RFC's"]},{"location":"rfcs/deferred/0016-cursor-paging.html#proposed-solution-drawbacks","title":"Proposed Solution: Drawbacks","text":"","tags":["RFC's"]},{"location":"rfcs/deferred/0016-cursor-paging.html#proposed-solution-notable-alternatives","title":"Proposed Solution: Notable Alternatives","text":"","tags":["RFC's"]},{"location":"rfcs/deferred/0016-cursor-paging.html#prior-art","title":"Prior Art","text":"<p>The proposal is base on a talk by Markus Winand\u2019s \"Pagination Done the PostgreSQL Way\". The slide deck is easy to follow and recommended for engineers who want to understand more about the seek technique.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0016-cursor-paging.html#future-possibilities","title":"Future Possibilities","text":"<p>The BFD's team goals to improve latency, capacity, and throughput continue. Significant areas for improvement remain. This proposal improves the BFD's database queries but does not address other bottlenecks like JSON serialization. With cursor paging, testing reveals much fewer latency spikes. Nevertheless, some spikes remain and require further investigations.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0016-cursor-paging.html#addendums","title":"Addendums","text":"<ol> <li>\"Pagination Done the PostgreSQL Way\" by Markus Winands.</li> <li>\"Experimental Paging\" Cursor paging POC implementation</li> <li>\"BFD Server Performance, Part 1: Initial Investigations\" by Karl Davis</li> <li>\"BFD Server Performance, Part 2: Bottlenecks\" by Karl Davis</li> </ol>","tags":["RFC's"]},{"location":"rfcs/deferred/0017-filtering.html","title":"RFC Proposal","text":"<ul> <li>RFC Proposal ID: <code>0007-filtering-fields</code></li> <li>Start Date: 2020-08-21</li> <li>RFC PR: rfcs#0007</li> <li>JIRA Ticket(s):<ul> <li>https://jira.cms.gov/browse/AB2D-1863</li> </ul> </li> </ul> <p>This proposal strengthens the guarantee made by the use of the _elements parameter for the fhir/ExplanationOfBenefit endpoint.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0017-filtering.html#status","title":"Status","text":"<ul> <li>Status: Deferred</li> <li>Implementation JIRA Ticket(s): NONE</li> </ul>","tags":["RFC's"]},{"location":"rfcs/deferred/0017-filtering.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>RFC Proposal</li> <li>Status</li> <li>Table of Contents</li> <li>Motivation</li> <li>Proposal<ul> <li>Proposed Solution: Unresolved Questions</li> <li>Proposed Solution: Drawbacks</li> <li>Proposed Solution: Notable Alternatives</li> </ul> </li> <li>Future Possibilities</li> </ul>","tags":["RFC's"]},{"location":"rfcs/deferred/0017-filtering.html#motivation","title":"Motivation","text":"<p>A single BFD EOB retrieval call (UHC) with 2 million beneficiaries took 24 hours to retrieve three months of data. Larger PDPs can have in excess of 6 million beneficiaries.  To have a request take over 3 days is not acceptable.</p> <p>AB2D receives more data than needed from BFD when we make API calls to gather EOBs (Explanation of Benefits).  AB2D currently filters out irrelevant fields.  This might not be terrible for dozens of API calls, but for millions of requests it can impact our performance significantly.  Since we only need a projection of the data (10 out of 69 top-level getters; 6 out of 42 fields in the ItemComponent), it makes sense for the server to send us only the data consumed.  This reduces bandwidth requirements for receiving the data.</p> <p>For example, GraphQL solves this problem by allowing clients to specify field selection (e.g. a database projection) at invocation time.</p> <p>While not as powerful as the GraphQl offering where one can field select as deep in the hierarchy as desired, FHIR does offer a top level projection through the use of the _elements parameter supported both in R3 and R4.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0017-filtering.html#proposal","title":"Proposal","text":"<p>FHIR section 3.1.1.5.9 specifies the behavior of _elements as follows: <code>Servers are not obliged to return just the requested elements. Servers SHOULD always return mandatory elements whether they are requested or not. Servers SHOULD mark the resources with the tag SUBSETTED to ensure that the incomplete resource is not actually used to overwrite a complete resource.</code></p> <p>This proposal changes <code>not obliged</code> to <code>obliged</code> in the FHIR specification for the fhir/ExplanationOfBenefit endpoint, thus providing a guarantee of the performance saving behavior.  The mandatory elements and tag marking behavior are unchanged.  This proposal does not address reducing the number of fields returned in ItemComponent as the FHIR specification makes no provision for selecting fields of contained objects.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0017-filtering.html#proposed-solution-unresolved-questions","title":"Proposed Solution: Unresolved Questions","text":"<ul> <li>How much will this save us?</li> </ul> <p>As part of a larger effort to reduce job times from days to minutes, AB2D is constructing a performance test harness which will at its basic element, perform a single EOB retrieval.  Anecdotally, BFD EOB retrievals have been calibrated in the 500 msec range.  This harness will allow a more deliberate measurement of the actual response times, independent of the current AB2D architecture.  Once the basic harness is ready, before and after retrieval times can be obtained to provide a firm answer to the question.</p> <p>Additional goals for the harness are to assess how much can concurrency increase.  Currently, AB2D limits EOB retrievals to 32 concurrent requests.  The plans are to run cooperative experiments to establish a maximum concurrent limit.</p> <p>The harness will make it much more convenient to characterize current performance and to confirm or disprove any expected benefits of changes.</p> <p>Proposed profiling is invoking BFD with at least 10,000 EOB Requests by Patient ID (ExplanationOfBenefit?patient=) and to measure the aggregate time of completion of all calls.  In other words, sum up the time taken by each call over all calls.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0017-filtering.html#proposed-solution-drawbacks","title":"Proposed Solution: Drawbacks","text":"<p>This solution incurs a nominal amount of extra processing to traverse the EOB object and null out fields that do not need to be returned.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0017-filtering.html#proposed-solution-notable-alternatives","title":"Proposed Solution: Notable Alternatives","text":"<p>GraphQL is a notable alternative where the invocation explicitly specifies the requested fields.  Since GraphQL supports introspection, the client can actively determine the available fields.  A hybrid approach using registered queries requires queries to be specified in advance so that they can be locked down by a production server.</p> <p>The downside is that this would be a significant amount of work for BFD to adopt a different API style while continuing to support the REST calls.</p>","tags":["RFC's"]},{"location":"rfcs/deferred/0017-filtering.html#future-possibilities","title":"Future Possibilities","text":"<p>There could be dynamic projections on the database side as well. Right now there are 194 columns in the beneficiary table, and we certainly do not need all of them, so retrieving only the needed columns would reduce the load on the database.</p> <p>Other groups would likely want to utilize field selection as well for other endpoints, but that is out of scope for this RFC.</p>","tags":["RFC's"]},{"location":"rfcs/howto/index.html","title":"Resources","text":"<p>This directory is intended to provide a home for non-text resources that are referenced by text within the RFC documents, e.g. graphical diagrams, images, etc. It's inspired by a directory of the same name featured in the rust-lang/rfcs repository. At the time of this writing, the resources directory includes just one image in an SVG format. </p>"},{"location":"rfcs/howto/index.html#plantuml","title":"PlantUML","text":"<p>PlantUML is \"is an open-source tool allowing users to create diagrams from a plain text language\" (from the PlantUML entry on Wikipedia). It's capable of producing commong diagrams, both UML and otherwise, in a variety of formats including ASCII diagrams, PNG, SVG.</p>"},{"location":"rfcs/howto/index.html#plantuml-svgs","title":"PlantUML SVGs","text":"<p>It's probably not unique to PlantUML, but no less helpful, the SVG output features an embdedd comment that includes both the MD5 hash of the diagram as well as DSL source code. For this reason, the original PlantUML DSL encoded files can be considered optional.</p>"},{"location":"rfcs/howto/index.html#plantuml-example","title":"PlantUML Example","text":"<p>The following example in plantuml with a comment describing the export to SVG, note that only comments inside the <code>@startuml</code> and <code>@enduml</code> will appear in the resultant SVG/XML: <pre><code>' hello-world.plantuml\n@startuml\n/'\n ' export PLANTUML_JAR=&lt;path-to-your-plantuml-jar&gt;\n ' java -jar $PLANTUML_JAR -svg hello-world.plantuml\n '/\nAlice -&gt; Bob: Authentication Request\nBob --&gt; Alice: Authentication Response\nAlice -&gt; Bob: Another authentication Request\nAlice &lt;-- Bob: Another authentication Response\n@enduml\n</code></pre></p> <p>Rendering as an SVG using the prescribed comment embedded in the diagram's DSL above:</p> <pre><code>export PLANTUML_JAR=&lt;path-to-your-plantuml-jar&gt;\njava -jar $PLANTUML_JAR -svg hello-world.plantuml\n</code></pre> <p>Rendered as an SVG in the raw XML format, note the final comment:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;&lt;svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" contentScriptType=\"application/ecmascript\" contentStyleType=\"text/css\" height=\"221px\" preserveAspectRatio=\"none\" style=\"width:295px;height:221px;background:#FFFFFF;\" version=\"1.1\" viewBox=\"0 0 295 221\" width=\"295px\" zoomAndPan=\"magnify\"&gt;&lt;defs&gt;&lt;filter height=\"300%\" id=\"fuqcworyn19rk\" width=\"300%\" x=\"-1\" y=\"-1\"&gt;&lt;feGaussianBlur result=\"blurOut\" stdDeviation=\"2.0\"/&gt;&lt;feColorMatrix in=\"blurOut\" result=\"blurOut2\" type=\"matrix\" values=\"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 .4 0\"/&gt;&lt;feOffset dx=\"4.0\" dy=\"4.0\" in=\"blurOut2\" result=\"blurOut3\"/&gt;&lt;feBlend in=\"SourceGraphic\" in2=\"blurOut3\" mode=\"normal\"/&gt;&lt;/filter&gt;&lt;/defs&gt;&lt;g&gt;&lt;line style=\"stroke:#A80036;stroke-width:1.0;stroke-dasharray:5.0,5.0;\" x1=\"30\" x2=\"30\" y1=\"40.4883\" y2=\"177.7305\"/&gt;&lt;line style=\"stroke:#A80036;stroke-width:1.0;stroke-dasharray:5.0,5.0;\" x1=\"263.5\" x2=\"263.5\" y1=\"40.4883\" y2=\"177.7305\"/&gt;&lt;rect fill=\"#FEFECE\" filter=\"url(#fuqcworyn19rk)\" height=\"30.4883\" style=\"stroke:#A80036;stroke-width:1.5;\" width=\"47\" x=\"5\" y=\"5\"/&gt;&lt;text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"33\" x=\"12\" y=\"25.5352\"&gt;Alice&lt;/text&gt;&lt;rect fill=\"#FEFECE\" filter=\"url(#fuqcworyn19rk)\" height=\"30.4883\" style=\"stroke:#A80036;stroke-width:1.5;\" width=\"47\" x=\"5\" y=\"176.7305\"/&gt;&lt;text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"33\" x=\"12\" y=\"197.2656\"&gt;Alice&lt;/text&gt;&lt;rect fill=\"#FEFECE\" filter=\"url(#fuqcworyn19rk)\" height=\"30.4883\" style=\"stroke:#A80036;stroke-width:1.5;\" width=\"40\" x=\"241.5\" y=\"5\"/&gt;&lt;text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"26\" x=\"248.5\" y=\"25.5352\"&gt;Bob&lt;/text&gt;&lt;rect fill=\"#FEFECE\" filter=\"url(#fuqcworyn19rk)\" height=\"30.4883\" style=\"stroke:#A80036;stroke-width:1.5;\" width=\"40\" x=\"241.5\" y=\"176.7305\"/&gt;&lt;text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"26\" x=\"248.5\" y=\"197.2656\"&gt;Bob&lt;/text&gt;&lt;polygon fill=\"#A80036\" points=\"251.5,67.7988,261.5,71.7988,251.5,75.7988,255.5,71.7988\" style=\"stroke:#A80036;stroke-width:1.0;\"/&gt;&lt;line style=\"stroke:#A80036;stroke-width:1.0;\" x1=\"30.5\" x2=\"257.5\" y1=\"71.7988\" y2=\"71.7988\"/&gt;&lt;text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"13\" lengthAdjust=\"spacing\" textLength=\"147\" x=\"37.5\" y=\"67.0566\"&gt;Authentication Request&lt;/text&gt;&lt;polygon fill=\"#A80036\" points=\"41.5,97.1094,31.5,101.1094,41.5,105.1094,37.5,101.1094\" style=\"stroke:#A80036;stroke-width:1.0;\"/&gt;&lt;line style=\"stroke:#A80036;stroke-width:1.0;stroke-dasharray:2.0,2.0;\" x1=\"35.5\" x2=\"262.5\" y1=\"101.1094\" y2=\"101.1094\"/&gt;&lt;text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"13\" lengthAdjust=\"spacing\" textLength=\"157\" x=\"47.5\" y=\"96.3672\"&gt;Authentication Response&lt;/text&gt;&lt;polygon fill=\"#A80036\" points=\"251.5,126.4199,261.5,130.4199,251.5,134.4199,255.5,130.4199\" style=\"stroke:#A80036;stroke-width:1.0;\"/&gt;&lt;line style=\"stroke:#A80036;stroke-width:1.0;\" x1=\"30.5\" x2=\"257.5\" y1=\"130.4199\" y2=\"130.4199\"/&gt;&lt;text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"13\" lengthAdjust=\"spacing\" textLength=\"199\" x=\"37.5\" y=\"125.6777\"&gt;Another authentication Request&lt;/text&gt;&lt;polygon fill=\"#A80036\" points=\"41.5,155.7305,31.5,159.7305,41.5,163.7305,37.5,159.7305\" style=\"stroke:#A80036;stroke-width:1.0;\"/&gt;&lt;line style=\"stroke:#A80036;stroke-width:1.0;stroke-dasharray:2.0,2.0;\" x1=\"35.5\" x2=\"262.5\" y1=\"159.7305\" y2=\"159.7305\"/&gt;&lt;text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"13\" lengthAdjust=\"spacing\" textLength=\"209\" x=\"47.5\" y=\"154.9883\"&gt;Another authentication Response&lt;/text&gt;&lt;!--MD5=[7b907e995d48c862f5d0492a126b987a]\n@startuml\n/'\n ' export PLANTUML_JAR=&lt;path-to-your-plantuml-jar&gt;\n ' java -jar $PLANTUML_JAR -svg hello-world.plantuml\n '/\nAlice -&gt; Bob: Authentication Request\nBob - -&gt; Alice: Authentication Response\nAlice -&gt; Bob: Another authentication Request\nAlice &lt;- - Bob: Another authentication Response\n@enduml\n\n@startuml\nAlice -&gt; Bob: Authentication Request\nBob - -&gt; Alice: Authentication Response\nAlice -&gt; Bob: Another authentication Request\nAlice &lt;- - Bob: Another authentication Response\n@enduml\n\nPlantUML version 1.2022.1(Tue Feb 01 12:19:58 CST 2022)\n(GPL source distribution)\nJava Runtime: OpenJDK Runtime Environment\nJVM: OpenJDK 64-Bit Server VM\nDefault Encoding: UTF-8\nLanguage: en\nCountry: US\n--&gt;&lt;/g&gt;&lt;/svg&gt;\n</code></pre>"},{"location":"rfcs/howto/template.html","title":"RFC Proposal","text":"<ul> <li>RFC Proposal ID: <code>0000-my-awesome-feature</code> (fill me in with a unique ident)</li> <li>Start Date: (fill me in with today's date, YYYY-MM-DD)</li> <li>RFC PR: rust-lang/rfcs#0000</li> <li>JIRA Ticket(s):<ul> <li>BLUEBUTTON-0000</li> </ul> </li> </ul> <p>Write a brief summary here: a one paragraph explanation of the feature. Try to structure it like an \"elevator pitch\": it should provide readers with a high-level understanding of the goals and proposed solution.</p> <p>Please note: many of the other sections below will not be needed for some proposals;   don't waste time writing responses that don't deliver real value. For any such not-needed section, simply write in \"N/A\".</p>"},{"location":"rfcs/howto/template.html#status","title":"Status","text":"<ul> <li>Status: (Proposed/Approved/Rejected/Implemented)</li> <li>Implementation JIRA Ticket(s):<ul> <li>BLUEBUTTON-0001</li> </ul> </li> </ul>"},{"location":"rfcs/howto/template.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>RFC Proposal</li> <li>Status</li> <li>Table of Contents</li> <li>Motivation</li> <li>Proposed Solution<ul> <li>Proposed Solution: Detailed Design</li> <li>Proposed Solution: Unresolved Questions</li> <li>Proposed Solution: Drawbacks</li> <li>Proposed Solution: Notable Alternatives</li> </ul> </li> <li>Prior Art</li> <li>Future Possibilities</li> <li>Addendums</li> </ul>"},{"location":"rfcs/howto/template.html#motivation","title":"Motivation","text":"<p>Why are we doing this? What use cases does it support? What is the expected outcome? Why is now the appropriate time to address this?</p>"},{"location":"rfcs/howto/template.html#proposed-solution","title":"Proposed Solution","text":"<p>Explain the proposal as if it was already implemented and shipped, and you were just explaining it to another developer or user. That generally means:</p> <ul> <li>Introducing new named concepts.</li> <li>Identifying and address each of the various audiences who might (or should) care about this proposal.   Explaining the solution using concepts and terms relevant to eaach of them.   Explaining how they should think about the solution; detailing the impact as concretely as possible.   Possible audiences might include:<ul> <li>Internal team: engineers, operators, product management, business owners.</li> <li>External users: engineers, operators, product management, business owners, end users.</li> </ul> </li> <li>Explaining the feature largely in terms of examples.<ul> <li>Screencasts are often a good idea.<ul> <li>On Mac OS X, you can use the built-in Quicktime Player or the built-in Mac OS X Mojave (and up) feature.</li> </ul> </li> <li>Diagrams are often a good idea.<ul> <li>Keep it simple! Use something like http://asciiflow.com/.</li> </ul> </li> </ul> </li> <li>As part of implementing this proposal, will any documentation updates be needed, e.g. changelogs, Confluence pages, etc.?   If so, draft them now! Include the draft as a subsection or addendum.</li> </ul>"},{"location":"rfcs/howto/template.html#proposed-solution-detailed-design","title":"Proposed Solution: Detailed Design","text":"<p>This is the technical portion of the RFC. Explain the design in sufficient detail that:</p> <ul> <li>Its interaction with other features is clear.</li> <li>It is reasonably clear how the feature would be implemented.</li> <li>Corner cases are dissected by example.</li> </ul> <p>The section should return to the examples given in the previous section, and explain more fully how the detailed proposal makes those examples work.</p>"},{"location":"rfcs/howto/template.html#proposed-solution-unresolved-questions","title":"Proposed Solution: Unresolved Questions","text":"<p>Collect a list of action items to be resolved or officially deferred before this RFC is submitted for final comment, including:</p> <ul> <li>What parts of the design do you expect to resolve through the RFC process before this gets merged?</li> <li>What parts of the design do you expect to resolve through the implementation of this feature before stabilization?</li> <li>What related issues do you consider out of scope for this RFC that could be addressed in the future independently of the solution that comes out of this RFC?</li> </ul>"},{"location":"rfcs/howto/template.html#proposed-solution-drawbacks","title":"Proposed Solution: Drawbacks","text":"<p>Why should we not do this?</p>"},{"location":"rfcs/howto/template.html#proposed-solution-notable-alternatives","title":"Proposed Solution: Notable Alternatives","text":"<ul> <li>Why is this design the best in the space of possible designs?</li> <li>What other designs have been considered and what is the rationale for not choosing them?</li> <li>What is the impact of not doing this?</li> </ul>"},{"location":"rfcs/howto/template.html#prior-art","title":"Prior Art","text":"<p>Discuss prior art, both the good and the bad, in relation to this proposal. A few examples of what this can include are:</p> <ul> <li>For feature proposals:   Does this feature exist in other similar-ish APIs and what experience have their community had?</li> <li>For architecture proposals:   Is this architecture used by other CMS or fedgov systems and what experience have they had?</li> <li>For process proposals:   Is this process used by other CMS or fedgov programs and what experience have they had?</li> <li>For other teams:   What lessons can we learn from what other communities have done here?</li> <li>Papers and other references:   Are there any published papers or great posts that discuss this?   If you have some relevant papers to refer to, this can serve as a more detailed theoretical background.</li> </ul> <p>This section is intended to encourage you as an author to think about the lessons from other languages, provide readers of your RFC with a fuller picture. If there is no prior art, that is fine - your ideas are interesting to us whether they are brand new or if it is an adaptation from other languages.</p> <p>Note that while precedent set by other programs is some motivation, it does not on its own motivate an RFC. Please also take into consideration that we (and the government in general) sometimes intentionally diverge from common \"best practices\".</p>"},{"location":"rfcs/howto/template.html#future-possibilities","title":"Future Possibilities","text":"<p>Think about what the natural extension and evolution of your proposal would be and how it would affect the language and project as a whole in a holistic way. Try to use this section as a tool to more fully consider all possible interactions with the project and language in your proposal. Also consider how the this all fits into the roadmap for the project and of the relevant sub-team.</p> <p>This is also a good place to \"dump ideas\", if they are out of scope for the RFC you are writing but otherwise related.</p> <p>If you have tried and cannot think of any future possibilities, you may simply state that you cannot think of anything.</p> <p>Note that having something written down in the future-possibilities section is not a reason to accept the current or a future RFC;   such notes should be in the section on motivation or rationale in this or subsequent RFCs. The section merely provides additional information.</p>"},{"location":"rfcs/howto/template.html#addendums","title":"Addendums","text":"<p>The following addendums are required reading before voting on this proposal:</p> <ul> <li>(none at this time)</li> </ul> <p>Please note that some of these addendums may be encrypted. If you are unable to decrypt the files, you are not authorized to vote on this proposal.</p>"},{"location":"rfcs/implemented/0000-jboss-replacement.html","title":"RFC Proposal","text":"<ul> <li>RFC Proposal ID: <code>0000-bye-bye-jboss</code> (fill me in with a unique ident)</li> <li>Start Date: 2019-09-17</li> <li>RFC PR: CMSgov/beneficiary-fhir-data#39</li> <li>JIRA Ticket(s):<ul> <li>BLUEBUTTON-1112</li> </ul> </li> </ul> <p>This RFC proposes switching the Data Server application from JBoss to Jetty. Why? Primarily the licensing costs of JBoss,   though the number of issues we've had with JBoss being unreliable over the years is also a consideration. Jetty is an open source, well-supported, and widely-used alternative.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0000-jboss-replacement.html#status","title":"Status","text":"<ul> <li>Status: Implemented</li> <li>Implementation JIRA Ticket(s):<ul> <li>BLUEBUTTON-1112</li> </ul> </li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0000-jboss-replacement.html#motivation","title":"Motivation","text":"<p>For business reasons, running JBoss inside our future home, the CMS Cloud Services (CCS) environment,   will not really be feasible. Also, as mentioned above, JBoss hasn't been the world's most reliable piece of technology for us. Accordingly, we need to switch to an alternative application server,   either prior to or as part of the CCS migration.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0000-jboss-replacement.html#proposed-solution","title":"Proposed Solution","text":"<p>Java web applications are often packaged as Web Archive (WAR) files and run inside \"application containers\"   such as JBoss, Wildfly, Jetty, Tomcat, Websphere, etc. These containers provide support for various pieces and versions of the JavaEE specification,   from the \"Web Profile\" to the \"full platform\".</p> <p>The BFD Server only requires the \"Web Profile\" portions of JavaEE,   and was originally designed to run in Jetty. Early on, we moved from Jetty to a combination of JBoss in production and Wildfly elsewhere,   as it was felt that the support provided by JBoss was needed.</p> <p>This RFC proposes undoing that, and moving back to Jetty. This will be a major change, but if done properly   our end users will experience no negative impacts and may not notice the change at all. We'll also need to perform a significant amount of testing,   as switching application servers can result in very unexpected regressions.</p> <p>Long-term, though, this change will likely be a major net-positive:</p> <ul> <li>We'll be able to remove the hilarious amount of fragile Bash scripts     required to manage our current JBoss and Wildfly usage.</li> <li>We'll reduce the differences between our local and production environments,     as we'll now be using the same application server for both.</li> <li>We'll be able to more simply implement planned features,     such as altering authentication and/or authorization.</li> <li>We'll be more easily able to adopt auto-scaling and immutable deploys.</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0000-jboss-replacement.html#proposed-solution-detailed-design","title":"Proposed Solution: Detailed Design","text":"<p>Nothing too magic here: we're gonna' rip out JBoss and Wildfly and replace them with Jetty. To simplify deployment, we'll add a new <code>bfd-server-launcher</code> module to the Data Server project,   which will build a simple Jetty executable capsule. This provides a couple of advantages:</p> <ol> <li>We already use capsules to deploy the Data Pipeline service,      so deploying the Data Server in the same way will improve consistency.</li> <li>Having a custom Jetty executable with a mostly-hardcoded configuration will cut down on future configuration errors.</li> </ol> <p>It's worth noting that all of these changes are already implemented in this RFC's PR:   the development work is done.</p> <p>For testing, we'll employ a combination of:</p> <ul> <li>Our existing integration tests, in our local development environments.</li> <li>Hand-running similar tests in our TEST environment.</li> <li>Running our performance tests against the Jetty-fied Data Server in our TEST environment.</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0000-jboss-replacement.html#proposed-solution-unresolved-questions","title":"Proposed Solution: Unresolved Questions","text":"<p>N/A</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0000-jboss-replacement.html#proposed-solution-drawbacks","title":"Proposed Solution: Drawbacks","text":"<p>The major tradeoff here is that JBoss costs a significant amount of money but provides excellent vendor support. Jetty is open source: we'll need to support it ourselves. Given the number of issues we've had with JBoss over the years despite the support,   that feels like an accpetable tradeoff.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0000-jboss-replacement.html#proposed-solution-notable-alternatives","title":"Proposed Solution: Notable Alternatives","text":"<p>In the Java ecosystem, you can hardly throw a rock without hitting an application server. So why Jetty?</p> <ul> <li>Mostly just, \"why not?\"</li> <li>We've used it before and the engineer likely to implement this change     (Karl) is more familiar and comfortable with it than other application servers, e.g. Tomcat.</li> <li>It does reasonably well on the Tech Empower benchmarks.</li> </ul> <p>If problems are encountered during testing, we will absolutely consider alternatives. Unless/until then, though, we're just going with what we know, to reduce risk and time-to-completion.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0000-jboss-replacement.html#prior-art","title":"Prior Art","text":"<p>N/A</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0000-jboss-replacement.html#future-possibilities","title":"Future Possibilities","text":"<p>N/A</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0000-jboss-replacement.html#addendums","title":"Addendums","text":"<p>The following addendums are required reading before voting on this proposal:</p> <ul> <li>(none at this time)</li> </ul> <p>Please note that some of these addendums may be encrypted. If you are unable to decrypt the files, you are not authorized to vote on this proposal.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0001-beneficiary-fhir-server-rfc-process-v1.html","title":"RFC Proposal","text":"<ul> <li>RFC Proposal ID: <code>0001-beneficiary-fhir-server-rfc-process-v1</code></li> <li>Start Date: 2019-05-15</li> <li>RFC PR: bluebutton-data-model/rfcs#0001</li> <li>JIRA Ticket(s):<ul> <li>BLUEBUTTON-948</li> </ul> </li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0001-beneficiary-fhir-server-rfc-process-v1.html#status","title":"Status","text":"<ul> <li>Status: Implemented</li> <li>Implementation JIRA Ticket(s):<ul> <li>BLUEBUTTON-949</li> </ul> </li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0001-beneficiary-fhir-server-rfc-process-v1.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>RFC Proposal</li> <li>Status</li> <li>Table of Contents</li> <li>Motivation</li> <li>Proposed Solution<ul> <li>Proposed Solution: Detailed Design</li> <li>Proposed Solution: Unresolved Questions</li> <li>Proposed Solution: Drawbacks</li> <li>Proposed Solution: Notable Alternatives</li> </ul> </li> <li>Prior Art</li> <li>Future Possibilities</li> <li>Addendums</li> </ul> <p>The Blue Button Data/backend team should have an informal-as-is-reasonable process for managing and evaluating major proposals.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0001-beneficiary-fhir-server-rfc-process-v1.html#motivation","title":"Motivation","text":"<p>Why are we doing this? I mean... alll the cool kids are doing it:</p> <ul> <li>Blue Button 2.0 Security Impact Assessments</li> <li>Blue Button 2.0 Auth/frontend Architecture Decision Records</li> <li>BCDA Engineering Tech Specs</li> </ul> <p>More seriously, we want to move from the current process where these proposals are \"hidden\" to one where the entire team can participate. This is expected to:</p> <ol> <li>Lead to more thoughtful input on these decisions, and thus better decisions.</li> <li>Formalize the currently-informal process, leading to clearer outcomes.</li> <li>A historical record of our thought process and decisions, making it easier for new team members to get up to speed.</li> </ol>","tags":["RFC's"]},{"location":"rfcs/implemented/0001-beneficiary-fhir-server-rfc-process-v1.html#proposed-solution","title":"Proposed Solution","text":"<p>This very-first RFC is a \"process only\" RFC, in which the team will detail, discuss, and agree on a formal process for making major decisions,   e.g. on architecture, process (like this!), and major features. Specifically, we're reviewing and hopefully approving these documents:</p> <ul> <li>./README.md</li> <li>./0000-template.md</li> <li>./0001-beneficiary-fhir-server-rfc-process-v1.md</li> </ul> <p>Other future RFCs will likely include:</p> <ul> <li>\"Should we adopt a formal (internal) Code of Conduct / Working Agreement?\"</li> <li>\"Should we switch from self-managed PostgreSQL-on-EC2 to a more managed DB offering?\"</li> <li>\"Should we move our Security Impact Assessment (SIA) process to GitHub?\"</li> <li>\"Should we move our development and/or production systems to Docker?\"</li> </ul> <p>Questions to consider when evaluating this particular RFC:</p> <ul> <li>Is this process inclusive enough?<ul> <li>GitHub's pull requests have a great workflow for engineers, but is it user-friendly enough for non-developers?</li> </ul> </li> <li>Knowing our team, will the process work well for the people and personalities involved, or will it somehow turn toxic?</li> <li>How might we change this proposed process to make it more agile?</li> <li>Are we all comfortable having these conversations in public, on GitHub?</li> <li>Does this RFC follow the proposed process well enough to validate the process and associated templates?</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0001-beneficiary-fhir-server-rfc-process-v1.html#proposed-solution-detailed-design","title":"Proposed Solution: Detailed Design","text":"<p>Are these two documents correct and sufficient?:</p> <ul> <li>./README.md<ul> <li>Intended to introduce the RFC process at a high level.</li> </ul> </li> <li>./0000-template.md<ul> <li>Intended to be copy-pasted for each new proposed RFC.</li> </ul> </li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0001-beneficiary-fhir-server-rfc-process-v1.html#proposed-solution-unresolved-questions","title":"Proposed Solution: Unresolved Questions","text":"<p>The following unresolved questions were identified during review of this RFC:</p> <ol> <li>How might we protect that shouldn't-be-public information to keep it private, while still (largely) using this open process?<ul> <li>I suspect that git-crypt and/or Ansible Vault are reasonable choices.</li> <li>I think it's safe to defer this issue until it comes up for the first time; better to make a decision on tooling then.</li> </ul> </li> </ol>","tags":["RFC's"]},{"location":"rfcs/implemented/0001-beneficiary-fhir-server-rfc-process-v1.html#proposed-solution-drawbacks","title":"Proposed Solution: Drawbacks","text":"<p>Why should we not do this?</p> <ul> <li>Is this too much paperwork?<ul> <li>Karl's opinion: no, each RFC should only take an hour or two to draft.</li> </ul> </li> <li>Is our current (informal) process working well enough that this should be postponed?<ul> <li>Karl's opinion: Maybe, but I was going to have to start writing these things down anyways, and I'd rather do the writing in Markdown on GitHub than in Confluence.</li> </ul> </li> <li>Have similar attempts by other communities failed in any spectacular ways that the proposed process doesn't account for?<ul> <li>Karl's opinion: Not that I know of. Worst failure case I've seen is folks using the RFC process to float bad ideas and being sad when people reject or ignore them.</li> </ul> </li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0001-beneficiary-fhir-server-rfc-process-v1.html#proposed-solution-notable-alternatives","title":"Proposed Solution: Notable Alternatives","text":"<p>This is very much intended to be, \"at least a little bit better than what BCDA and the Blue Button Auth/frontend are doing.\" It's not a perfect process, nor does such a perfect process even exist. Over those processes, though, we have the major advantage that GitHub has excellent support for a review workflow, and Confluence (which they use) doesn't.</p> <p>Given the Rust lang team's success with it, it clearly scales up way past what we will need it to. Given our dogfood use of it here, we can be reasonably confident that it works okay for our much smaller needs.</p> <p>If we don't adopt some formal process, we'll likely continue largely with our current informal process,   which largely boils down to, \"do whatever the loudest person (i.e. Karl) thinks. That's not ideal. This is better.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0001-beneficiary-fhir-server-rfc-process-v1.html#prior-art","title":"Prior Art","text":"<p>As previously mentioned, there's lots of prior art here:</p> <ul> <li>Rust RFCs</li> <li>Blue Button 2.0 Security Impact Assessments</li> <li>Blue Button 2.0 Auth/frontend Architecture Decision Records</li> <li>BCDA Engineering Tech Specs</li> </ul> <p>We should measure the success of this RFC's proposal against those by comparing the relevant team/community sizes vs. engagement in the RFC process. What percentage of the Blue Button Data/backend team gets involved in RFCs, on average?</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0001-beneficiary-fhir-server-rfc-process-v1.html#future-possibilities","title":"Future Possibilities","text":"<p>No future changes to this process have (yet) been identified.</p> <p>It is worth noting, though, that this process's main inspiration Rust RFCs   has seen tons of changes to it since the process was first formalized in 2014:   Rust RFCs: README.md: History.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0001-beneficiary-fhir-server-rfc-process-v1.html#addendums","title":"Addendums","text":"<p>The following addendums are required reading before voting on this proposal:</p> <ul> <li>(none at this time)</li> </ul> <p>Please note that some of these addendums may be encrypted. If you are unable to decrypt the files, you are not authorized to vote on this proposal.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0002-monorepo.html","title":"0002 Monorepo","text":"<ul> <li>Proposal ID: <code>0002-monorepo</code></li> <li>Start Date: <code>2019-07-20</code></li> <li>RFC PR: <code>N/A</code></li> <li>JIRA Ticket(s):<ul> <li>BLUEBUTTON-1086: Switch BFS to monorepo</li> </ul> </li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0002-monorepo.html#motivation","title":"Motivation","text":"<p>The Beneficiary FHIR Server systems should move from being split across 11 Git repositories to a single, combined repository: a \"monorepo\". This has a number of benefits: simpler onboarding, better testing of changes, and more efficient day-to-day operations.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0002-monorepo.html#status","title":"Status","text":"<ul> <li>Status: Implemented</li> <li>Implementation JIRA Ticket(s):<ul> <li>BLUEBUTTON-1086</li> </ul> </li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0002-monorepo.html#motivation_1","title":"Motivation","text":"<p>The most immediate motivation for this change is that we're all tired of dealing with the current setup. Having 11 repositories makes everything harder: we keep having to split up what are conceptually single changes into multiple separate pull requests. Aside from the hassle of the extra moving pieces, it makes it effectively impossible to test such multi-PR changesets in our AWS TEST environment. Given our team's current focus on major architectural changes, the sooner we make the switch to a monorepo, the better.</p> <p>Initially, everything was split out to support goals that are no longer relevant:</p> <ul> <li>It was expected that we'd be using a more traditional named-version release strategy,     where being able to tag things individually is important.   However, we have long since moved to a Continuous Deployment strategy,     where the separate repositories are more of a liability than a benefit.</li> <li>The approach was also intended to allow for more code sharing between projects (i.e. between BFS and BB2),     which is awkward with monolithic repositories.</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0002-monorepo.html#proposed-solution","title":"Proposed Solution","text":"<p>It's pretty simple: move our source code (and its history) into a single, combined repository: <code>https://github.com/CMSgov/beneficiary-fhir-server/</code>. That repository will be laid out as follows:</p> <pre><code>beneficiary-fhir-server.git/\n  dev/\n  original-project-1/\n  original-project-2/\n  ...\n  original-project-11/\n  README.md\n  (and other files...)\n</code></pre>","tags":["RFC's"]},{"location":"rfcs/implemented/0002-monorepo.html#proposed-solution-detailed-design","title":"Proposed Solution: Detailed Design","text":"<p>The following repositories will be migrated to the new <code>beneficiary-fhir-server</code> repository:</p> <ul> <li>https://github.com/CMSgov/bluebutton-parent-pom</li> <li>https://github.com/CMSgov/bluebutton-data-model</li> <li>https://github.com/CMSgov/bluebutton-data-pipeline</li> <li>https://github.com/CMSgov/bluebutton-data-server</li> <li>https://github.com/CMSgov/bluebutton-data-server-perf-tests</li> <li>https://github.com/CMSgov/bluebutton-ansible-playbooks-data</li> <li>https://github.com/CMSgov/ansible-role-bluebutton-data-pipeline</li> <li>https://github.com/CMSgov/ansible-role-bluebutton-data-server</li> <li>https://github.com/CMSgov/bluebutton-ansible-playbooks-data-sandbox<ul> <li>Hopefully not needed much longer, so may be removed eventually.</li> </ul> </li> <li>https://github.com/CMSgov/bluebutton-functional-tests<ul> <li>Not yet used, but we should fix that (eventually).</li> </ul> </li> <li>https://github.com/CMSgov/bluebutton-data-ccw-db-extract<ul> <li>Out of date, but we should fix that (eventually).</li> </ul> </li> <li>https://github.com/CMSgov/bluebutton-text-to-fhir<ul> <li>Unmaintained, and will be removed after migration.</li> </ul> </li> <li>https://github.com/CMSgov/bluebutton-csv-codesets<ul> <li>Unmaintained, and will be removed after migration.</li> </ul> </li> </ul> <p>See this proposed migration script: ../dev/monorepo-build.sh.</p> <p>Small side note: by default, <code>git log &lt;somefile&gt;</code> does not show history across filenames,   but <code>git blame</code> does. Adding the <code>--follow</code> flag to <code>git log</code>,   e.g. <code>git log --follow &lt;somefile&gt;</code> solves this.</p> <p>In addition to the migration and refactoring contained in the script,   a number of additional reorganization/refactoring items will be attempted manually   as part of the overall migration process. A working list of ideas for these items is contained in comments at the bottom of the script. In a number of cases, our code is split up across more modules than necessary,   which just makes things harder to do than necessary. These tasks need to be performed manually as there aren't great command line options for many of the steps,   e.g. changing a Maven project's <code>groupId</code>, <code>artifactId</code>, etc. To some extent, these changes will be limited to those that can be completed within a reasonable time   as the project will be in a pseudo code freeze for the duration of the process.</p> <p>One drawback to this approach is that it can only migrate the <code>master</code> branch of each original repository;   non-<code>master</code> branches, such as those for unmerged pull requests, will not be moved automatically. Any pull requests that can be merged prior to the move, should be. Any other branches that are no longer needed should be deleted prior to the move.d</p> <p>A one-time manual recreation of all remaining branches will have to be performed, via the following steps:</p> <ol> <li>Visit each PR in a web browser.</li> <li>Adjust the browser URL by appending \"<code>.diff</code>\" to it.    This will produce a Git patch file, containing all of the PR's changes (collapsed to a single changeset with no comments).</li> <li>Download/save the patch file locally.</li> <li>Apply the patch by running: <code>git apply &lt;patch_file&gt; --directory=&lt;target_subdir_of_monorepo&gt; &amp;&amp; git commit</code>.</li> </ol> <p>Once the migration has been completed, the original repositories should be archived,   via the Settings page for each repository in GitHub.</p> <p>During the migration, developers should be encouraged to spend time updating Confluence, JIRA, etc. A full \"code freeze\" would be excessive, but at the same time,   patches made against the old projects will likely not apply cleanly against the new ones.</p> <p>Afterwards, a brief post about the transition should also be published to the Blue Button site and Google Group.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0002-monorepo.html#proposed-solution-unresolved-questions","title":"Proposed Solution: Unresolved Questions","text":"<p>The following questions need to be resolved before this RFC is submitted for final comment:</p> <ul> <li>(none at this time)</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0002-monorepo.html#proposed-solution-drawbacks","title":"Proposed Solution: Drawbacks","text":"<p>Given our continuous deployment approach, this doesn't really apply,   but it's worth noting that monorepos don't lend themselves to tagging releases of individual subprojects.</p> <p>In addition, Jenkins' multibranch pipeline job type is the default choice these days (for good reason: it's excellent)   but doesn't really support the concept of multiple joba from/for a single repo. To some extent, that's fine: we want to build everything anytime anything changes,   as this will allow us to test and deploy all pieces of our code when we branch. It will be a bit awkward for tasks that make sense to run in Jenkins   but aren't really part of our default build workflow,   e.g. full performance tests or Jenkins-as-cron kind of items. At least one option for managing that is having a Jenkins job parameter   such as \"<code>jobType</code>\" that the Jenkinsfile uses to select which actual job code to go run. Other options exist as well, so this drawback feels manageable.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0002-monorepo.html#proposed-solution-notable-alternatives","title":"Proposed Solution: Notable Alternatives","text":"<p>N/A</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0002-monorepo.html#prior-art","title":"Prior Art","text":"<p>It's worth noting that lots of organizations far larger than us (Facebook, Google, etc.) are all using monorepos.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0002-monorepo.html#future-possibilities","title":"Future Possibilities","text":"<p>N/A</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0002-monorepo.html#addendums","title":"Addendums","text":"<p>The following addendums are required reading before voting on this proposal:</p> <ul> <li>../dev/monorepo-build.sh</li> </ul> <p>Please note that some of these addendums may be encrypted. If you are unable to decrypt the files, you are not authorized to vote on this proposal.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0004-since-parameter.html","title":"0004-since-parameter","text":"<ul> <li>RFC Proposal ID: <code>0000-since-parameter-support</code></li> <li>Start Date: October 1, 2019</li> <li>Version 2.1</li> <li>RFC PR: https://github.com/CMSgov/beneficiary-fhir-data/pull/155</li> <li>JIRA Ticket(s):<ul> <li>BlueButton-1506: Bulk Export Since Support</li> </ul> </li> </ul> <p>This RFC proposal adds features to BFD's API to allow BFD's partners to implement the Bulk Export <code>_since</code> parameter. The proposal discusses these new features as well as the logic that BFD's partners need to implement the <code>_since</code> parameter correctly.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0004-since-parameter.html#status","title":"Status","text":"<ul> <li>Status: Implemented</li> <li>Implementation JIRA Ticket(s):<ul> <li>BLUEBUTTON-1506</li> </ul> </li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0004-since-parameter.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>Status</li> <li>Table of Contents</li> <li>Motivation</li> <li>Proposed Solution</li> <li>BFD API Details</li> <li>Single Beneficiary Implementors Details</li> <li>Bulk Export Implementors Details</li> <li>BFD Implementation Details</li> <li>Roster Change Corner Case</li> <li>Time Corner Cases</li> <li>Database Schema Corner Cases</li> <li>Alternatives Considered</li> <li>Future Possibilities</li> <li>References</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0004-since-parameter.html#motivation","title":"Motivation","text":"<p>Consumers of CMS's beneficiary data APIs, whether they call BlueButton 2.0, ACO API, or DPC, want the most up-to-date information. Ideally, these apps and services would like to call a CMS API as soon as CMS updates its claim information. When they do call, they only want new data from CMS, not the information they already have.</p> <p>Before this RFC, BFD only supported returning all resources for a single beneficiary. Calls were returning more than 5 years of beneficiary data, when only the last weeks of data may be needed. This behavior is highly inefficient, especially for the bulk export jobs that happen weekly. In this case, each call is returning 260 times as much information as is needed on average.</p> <p>The FHIR [1] standard has provisions for an \"update me about new information\" pattern in its APIs. For bulk export operations, this feature is called the <code>_since</code> parameter. For single beneficiary operations, it is called the <code>_lastUpdated</code> parameter.</p> <p>Early feedback from both ACO API and DPC customers have nearly unanimously pointed out the need for <code>_since</code> parameter support [2]. For the ACO API, where an export operation can take many hours and result in 100's GB of data, ACO API customers have stated that they need <code>_since</code> support to move to production.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0004-since-parameter.html#proposed-solution","title":"Proposed Solution","text":"<p>This proposal adds 4 changes to the BFD API that are needed for downstream partners to implement the <code>_since</code> parameter.</p> <ol> <li>The <code>lastUpdated</code> metadata field of EOB, Patient, and Coverage FHIR resources contains the time they were written to the master DB by the ETL process.</li> <li>The search operation of the EOB, Patient, and Coverage resources support a <code>_lastUpdated</code> query parameter. When specified, the search filters resources against the passed in the date range. The capabilities statement includes the <code>_lastUpdated</code> search parameter.</li> <li>The BFD tracks the currency of the updates to BFD. To correctly handle clock skew and data propagation problems, it reports this information back to clients.</li> <li>The BFD server adds optimizations on resource searches with <code>_lastUpdated</code> for the case where the result set is empty. These searches should return results in a similar time to the time taken by a metadata query.</li> </ol>","tags":["RFC's"]},{"location":"rfcs/implemented/0004-since-parameter.html#bfd-api-details","title":"BFD API Details","text":"<p>All the proposed API changes follow the FHIR standard. Keeping compability with the FHIR specification allows BFD clients to use existing FHIR libraries and tools.</p> <p>The first improvement is a new <code>lastUpdated</code> field in the metadata object of a resource. This field contains the timestamp that the ETL process wrote the resource to the BFD DB. Like all FHIR date fields, this timestamp must include the server's timezone [4]. Resources based on records loaded before this RFC do not have a <code>lastUpdated</code> field.</p> <p>The second change is a <code>_lastUpdated</code> query parameter for resource searches per the FHIR specification [5]. FHIR enumerates a large set of comparison operators, but BFD supports a subset of these operators: <code>lt</code>, <code>le</code>, <code>gt</code> and <code>ge</code> operators. Two <code>_lastUpdated</code> parameters can be specified to form the upper and lower bounds of a time interval.</p> <p>The BFD tracks the timestamp of the last write to the BFD database. In the Bulk Export specification, this timestamp is called the <code>transactionTime</code> and the BFD has adopted this name as well. The third change to the API is returning the transaction time as the <code>lastUpdated</code> field of Bundle resources. See the Time Corner Cases section for more discussion on the reasoning behind for this design choice.</p> <p>When the BFD is loading beneficiary records, the transaction time it returns will vary between calls. Because of the issues listed in the Time Corner Cases section, the transaction time may actually decrease between calls, although it will generally be increasing. All resources returned in a bundle will have a <code>lastUpdated</code> timestamp that is less than or equal to the <code>lastUpdated</code> timestamp of the entire <code>Bundle</code> resource.</p> <p>The BFD database records loaded before this RFC implementation will have a null <code>lastUpdated</code> field. The BFD treats these records as if they have a very early <code>lastUpdated</code> value. Searches with a <code>_lastUpdated</code> parameter without a lower bound match these records; Likewise, searches with a lower bound never match these records. This design allows a single query to retrieve both records with and without a <code>lastUpdate</code> metadata field.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0004-since-parameter.html#single-beneficiary-implementors-details","title":"Single Beneficiary Implementors Details","text":"<p>Partners can use the <code>_lastUpdated</code> parameter to get a single beneficiary's recently added FHIR resources. To do this without missing a resource, the partner remembers (or has it's client remember) the last returned Bundle's <code>lastUpdated</code> field. This date is the transaction time of the service database at the time of the last query.</p>  <p>For this use-case, the simplest implementation does not page the results of a search. This choice avoids the problems of changing transaction times between different pages. An absolutely correct implementation would add an upper bound timestamp to periodical poll, as is shown in the bulk export use-case.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0004-since-parameter.html#bulk-export-implementors-details","title":"Bulk Export Implementors Details","text":"<p>BFD partner's which are implementing bulk export can add the <code>_since</code> feature from the specification. Just as it was in the single beneficiary case, keeping track of timestamps is important to avoid missing data. The following sequence diagram shows how this interaction should work.</p>  <p>For each beneficiary in the export group, the partner searches within a time interval. The lower bound of the interval is the <code>_since</code> parameter time passed by the bulk-export client. The upper bound timestamp is called the <code>transactionTime</code>. To establish the <code>transactionTime</code> of the export job, the partner should query the BFD for it's transaction time. One way to obtain BFD's transaction time is by fetching a Bundle resource for a synthetic user as is done in the example. The client uses the <code>transactionTime</code> as the <code>_since</code> time of the next bulk-export. An example URL where the time period requested is a week is:</p> <pre><code>https://&lt;hostname&gt;/v1/fhir/ExplanationOfBenefit\n  ?patient=&lt;beneficiaryId&gt;\n  &amp;_lastUpdated=gt2018-11-22T14:01:01-05:00\n  &amp;_lastUpdated=le2018-18-22T15:00:00-05:00\n  &amp;_format=application%2Fjson%2Bfhir\n</code></pre> <p>The BFD maintains a partial index of the <code>lastUpdate</code> field for each resource that was updated in the last 30 days. The index lets the service to be fast and efficient when a query has an empty result set. Partners can query the BFD frequently if they are using recent time intervals. In other words, they can adopt a daily or hourly polling pattern without putting a significant load on the BFD service.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0004-since-parameter.html#bfd-implementation-details","title":"BFD Implementation Details","text":"<p>For all top level-tables in the BFD, the RFC adds a new column for <code>lastUpdated</code>, which reflects the time BFD loaded a RIF file from the CCW. Because of the large size of the BFD tables, the BFD's Postgres database does not index the <code>lastUpdated</code> column. This design avoids some of the indexing capacity problems that the BFD has experienced in the past.</p> <p>Most bulk-export clients intend to call CMS on at least a weekly basis or even a daily basis. Since only a small set of records change in a given week, the most common result of a search is an empty set. The BFD implements a filter that allows the BFD data server to avoid querying the database in this case.</p> <p>The BFD pipeline tracks the beneficiaries that it updates in each RIF file load along with the time interval of the write. The BFD data server uses this list of past RIF file loads and their associated beneficiaries to build its filters. The filter management process works on a background thread on the data server. It never interferes with the data serving process.</p>  <p>The filters internally use a Bloom filter data structure. Bloom filters are very memory efficient and commonly used in databases like Postgres [8]. In essence, the filter design takes an optimization out of the database and implements it in the data server.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0004-since-parameter.html#roster-change-corner-case","title":"Roster Change Corner Case","text":"<p>The resources returned by a group export operation is the current roster of the group at the time of an export call. A group's roster may change between successive export calls. At this time, the importer does not have any data for the added beneficiaries. So, how should a group export call with a <code>_since</code> parameter handle new beneficiaries added to the group? The FHIR specification states that export should only include data updated after the passed in <code>_since</code> parameter. However, the specification does not contemplate this use-case, nor does it hint on how to implement this use-case correctly.</p> <p>Since the BFD service does not track groups, the BFD partners have to work out solutions for this problem. The FHIR community is aware of this issue and is considering an amendment to Bulk Export specification.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0004-since-parameter.html#time-corner-cases","title":"Time Corner Cases","text":"<p>In production, the BFD service runs on multiple computers, each with a different clock. Time differences in these clocks lead to a class of computing problems known as clock skew that are hard to detect and solve.</p> <p>Moreover, the BFD service in production uses replica databases to scale up the capacity of the service. There is a delay between the writes to the master database and the data appearing on the replica. The effect of this replica delay is very similar to or the same as clock skew. In practice, replica delay is a more significant problem than clock skew; In the BFD service, clock skew measures to be a few milliseconds, while replica delay has spiked in the past to over 30 minutes.</p> <p>This RFC's design avoids most clock skew and replica delay problems by using ETL's process computer's clock for values of the <code>lastUpdated</code> field. This fact and the atomic nature of the BFD writes ensures that a Bundle's <code>lastUpdated</code> value, which represents the timestamp of the entire database, is consistent with the timestamp of single resources. A long as a client use time values that come from the service, it avoids clock skew issues.</p> <p>One place where consistency issues appear is due to differing replica delay between availability zones. Because of load balancing between zones, successive calls to the BFD may hit different replica databases. If each replica has a different data set, this behavior results in inconsistencies between successive bundles in an export job. Usually, this is very unlikely to happen because replica delays typically are under a second, and the database is only updated once a week. Furthermore, export jobs have an upper bound in their search intervals, the <code>transactionTime</code>. Nevertheless, clients should check that the transaction timestamp returned is after or equal to the upper bound timestamp of the call to prevent to ensure consistency in the result set of successive calls.</p> <p>Another time corner case happens because of the precision of a FHIR timestamp is one millisecond. Multiple writes may occur during this period, so a transaction timestamp does not resolve to a single write. The solution to this issue is same as the previous multiple availability-zone issue; use an upper bound time in the search, use an upper bound that is a known transaction time, and check that the transaction time timestamp returned from the query is after or equal to the upper bound timestamp.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0004-since-parameter.html#database-schema-corner-cases","title":"Database Schema Corner Cases","text":"<p>FHIR Resources are projections from the BFD's internal records, based on the CCW's RIF files. As a result, the FHIR Resources may have their <code>lastUpdated</code> field change when other fields do not change.</p> <p>Records created before this RFC do not have a last updated value. FHIR resources derived from these records do not have a <code>lastUpdated</code> field.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0004-since-parameter.html#alternatives-considered","title":"Alternatives Considered","text":"<p>Instead of optimizing at the BFD data server, an earlier design had the empty result set optimization done at the partner level. An ETL feed served by the BFD would allow the partner to implement the bloom filters now found in BFD data server. Although this design is slightly more efficient, the current design is simpler to run and requires less partner work.</p> <p>A simpler approach would be use a database index on the <code>lastUpdated</code> field in the database. This approach should be reconsidered if the database technology changes from Postgres on RDS.</p> <p>As mentioned in the Time Corner Cases section, using timestamps as a synchronization parameter is not correct in all cases. A better synchronization parameter is a revision number. Implementing revision numbers requires the ordering of writes to the database, a design choice that the current BFD design avoids and a choice that has scalability and performance penalties. Furthermore, revisions are not compatible with the FHIR bulk-export specification, so a revision based design was not implemented.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0004-since-parameter.html#future-possibilities","title":"Future Possibilities","text":"<p>This proposal should scale as BFD and it's partners serve more beneficiaries and clients. It should continue to work as BFD adds more partners and data sources.</p> <p>In future releases, BFD may receive claim data faster than the current weekly updates. Care has been taken to make sure the lastUpdated indexing works correctly during RIF file processing. This means that there should be no need to change the algorithm when the BFD moves to daily or even hourly batches.</p> <p>Much of the design choices in this RFC was done to avoid taxing the database at the center of the BFD. If the BFD database changes to allow the database to index <code>lastUpdated</code>, much of the optimizations done in this RFC can be removed.</p> <p>In discussions with DPC customers, they have asked for notification when the DPC has new beneficiary data. Instead of polling for updates, they would like to have the ability for a push update. Similarly, FHIR is developing a subscription model that supports webhooks [6]. If a BFD partner wants to develop these features, the file loaded tables can form a basis for this work.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0004-since-parameter.html#references","title":"References","text":"<p>The following references are required to fully understand and implement this proposal. They should be read before voting on this proposal.</p> <p> [1] FHIR - Fast Health Interoperability Resources: https://www.fhir.org</p> <p> [2] Rick Hawes: Conversations with customers of DPC and BCDA</p> <p> [3] Working copy of the Bulk Export specification: https://build.fhir.org/ig/HL7/bulk-data/export/index.html</p> <p> [4] FHIR Meta.lastUpdated definition: https://www.hl7.org/fhir/resource-definitions.html#meta.lastupdated</p> <p> [5] FHIR Search operation: https://www.hl7.org/fhir/search.html</p> <p> [6] FHIR Subscriptions: https://www.hl7.org/fhir/subscription.html</p> <p> [7] Original Confluence page with an implementation outline: https://confluence.cms.gov/pages/viewpage.action?pageId=189269516</p> <p> [8] Bloom Filter: https://en.wikipedia.org/wiki/Bloom_filter</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0005-mbi-search.html","title":"RFC Proposal","text":"<ul> <li>RFC Proposal ID: <code>0005-mbi-search</code></li> <li>Start Date: 2020-1-15</li> <li>RFC PR: </li> <li>JIRA Ticket(s):<ul> <li>BLUEBUTTON-1516</li> </ul> </li> </ul> <p>The addition of a search by Medicare Beneficiary Identifier (MBI) facility to the BFD's Patient search end-point.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0005-mbi-search.html#status","title":"Status","text":"<ul> <li>Status: Implemented</li> <li>Implementation JIRA Ticket(s):<ul> <li>BLUEBUTTON-1658</li> </ul> </li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0005-mbi-search.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>RFC Proposal</li> <li>Status</li> <li>Table of Contents</li> <li>Motivation</li> <li>Proposed Solution<ul> <li>Patient Search</li> <li>Include Identifiers</li> <li>Hash Algorithm</li> <li>Synthetic Beneficiaries</li> </ul> </li> <li>Prior Art</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0005-mbi-search.html#motivation","title":"Motivation","text":"<p>The Medicare Beneficiary Identifier (MBI) is replacing the Health Insurance Claim Identifier (HICN). See New Medicare Card Project for some background on this change. This change to the BFD adds to BFD the same capabilities for MBIs as currently exist for HICNs.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0005-mbi-search.html#proposed-solution","title":"Proposed Solution","text":"<p>The proposed solution consists of a new identifier for the Patient search FHIR end-point, a new include-identifiers projection, and the same hashing algorithm that BFD uses for HICN.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0005-mbi-search.html#patient-search","title":"Patient Search","text":"<p>The <code>Patient</code> end-point supports searching for resources with a matching MBI. The CMS classifies the MBI as personally identifiable information (PII). The request must hash the MBI to prevent sensitive PII from entering access logs. A new MBI based identifier system is recognized: <code>https://bluebutton.cms.gov/resources/identifier/mbi-hash</code>.</p> <p>An example request for a sythentic beneficiary <code>1S00A00AA00</code> is: <pre><code>curl 'https://&lt;bfd address&gt;:443/v1/fhir/Patient?identifier=https%3A%2F%2Fbluebutton.cms.gov%2Fresources%2Fidentifier%2Fmbi-hash%7C37c37d08d239f7f1da60e949674c8e4b5bb2106077cb0671d3dfcbf510ec3248&amp;_format=application%2Fjson%2Bfhir'\n</code></pre></p>","tags":["RFC's"]},{"location":"rfcs/implemented/0005-mbi-search.html#include-identifiers","title":"Include Identifiers","text":"<p>The returned patient resource includes the HICN and MBI identifiers if the request has the <code>IncludeIdentifiers</code> HTTP header set. BFD supports new values for the <code>IdentifierIncludes</code> header.</p>    value identifiers added Comments       Default value   true HICN and MBI MBI is added to the result   hicn HICN New value supported   mbi MBI New value supported   hicn,mbi HICN and MBI New value supported","tags":["RFC's"]},{"location":"rfcs/implemented/0005-mbi-search.html#hash-algorithm","title":"Hash Algorithm","text":"<p>Requests use a hashed version of the MBI instead of the actual MBI of the resource being requested. Hashing prevents the actual MBI from appearing in logs. For convenience, MBI hashing uses the existing HICN hashing algorithm, pepper and iterations.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0005-mbi-search.html#synthetic-beneficiaries","title":"Synthetic Beneficiaries","text":"<p>The BFD has a set of 30,000 synthetic beneficiary records for use for development and testing. These records now contain random MBIs. To distinguish these identifiers from real MBIs, they have <code>S</code> as the second letter. <code>1S00A00AA00</code> is an example of a synthetic MBI.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0005-mbi-search.html#prior-art","title":"Prior Art","text":"<p>The MBI searches follow the patterns set by the existing HICN search facilities.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0007-service-date-filter.html","title":"RFC Proposal","text":"<ul> <li>RFC Proposal ID: <code>0007-service-date-filter</code></li> <li>Start Date: 2020-10-15</li> <li>RFC PR: #376</li> <li>JIRA Ticket(s):<ul> <li>BCDA-3871</li> </ul> </li> </ul> <p>This proposal suggests adding an additional date range query on service completion date to EOB requests. This will allow users to guarantee that claims data falls within an expected date range.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0007-service-date-filter.html#status","title":"Status","text":"<ul> <li>Status: Implemented</li> <li>Implementation JIRA Ticket(s):<ul> <li>BCDA-3872</li> </ul> </li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0007-service-date-filter.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>RFC Proposal</li> <li>Status</li> <li>Table of Contents</li> <li>Motivation</li> <li>Proposed Solution<ul> <li>Proposed Solution: Detailed Design</li> <li>Proposed Solution: Unresolved Questions</li> <li>Proposed Solution: Drawbacks</li> <li>Proposed Solution: Notable Alternatives</li> </ul> </li> <li>Prior Art</li> <li>Future Possibilities</li> <li>Addendums</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0007-service-date-filter.html#motivation","title":"Motivation","text":"<p>BCDA needs to ensure that they are not returning beneficiary data to an ACO to which that beneficiary is not attributed. There are a variety of use-cases where this could occur if not handled properly: * End of year reassignment/attribution changes * Runouts</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0007-service-date-filter.html#proposed-solution","title":"Proposed Solution","text":"<p>The solution will allow a client to specify a new query parameter (<code>service-date</code>) when searching EOB resources. When specified, the search filters resources against the passed in date range.</p> <p>The table below captures the claim type field used in the service-date filter.</p>    Claim Type Date Field EOB Field CCW Field     Carrier Claim dateThrough billablePeriod#end CLM_THRU_DT   DME (Durable Medical Equipment) Claim dateThrough billablePeriod#end CLM_THRU_DT   HHA (Home Health Agency) Claim dateThrough billablePeriod#end CLM_THRU_DT   Hospice Claim dateThrough billablePeriod#end CLM_THRU_DT   Inpatient Claim dateThrough billablePeriod#end CLM_THRU_DT   Outpatient Claim dateThrough billablePeriod#end CLM_THRU_DT   PDE (Part D Event) Claim prescriptionFillDate item#servicedDate SRVC_DT   SNF (Skilled Nursing Facility) Claim dateThrough billablePeriod#end CLM_THRU_DT","tags":["RFC's"]},{"location":"rfcs/implemented/0007-service-date-filter.html#proposed-solution-detailed-design","title":"Proposed Solution: Detailed Design","text":"<p>Currently, EOB lookup by patients accepts an optional date range filter (<code>_lastUpdated</code>). The lookup would be expanded to support an additional date range filter (<code>service-date</code>).</p> <p>The <code>service-date</code> filter will be implemented at the application layer, not the data layer. No schema changes are needed. After querying the database by patientId and (optionally) lastUpdated, the <code>service-date</code> filter checks the claim entity's date field against the provided date range. All claim entities that pass the filter are returned to the caller.</p> <p>If the caller does not supply <code>service-date</code>, no post query filtering occurs.</p> <p>Similar to the <code>_lastUpdated</code> parameter, the <code>service-date</code> is supplied as a date range. Users can supply a lower bound, upper bound, or lower/upper bound. The supported parameters include: <code>ge</code>, <code>gt</code>, <code>le</code>, <code>lt</code>.</p> <p>Sample Requests: * https://\\&lt;hostname&gt;/v1/fhir/ExplanationOfBenefit?patient=\\&lt;beneficiaryId&gt;&amp;service-date=ge2020-01-01   * Returns claims data that occurred on or after 2020-01-01 * https://\\&lt;hostname&gt;/v1/fhir/ExplanationOfBenefit?patient=\\&lt;beneficiaryId&gt;&amp;service-date=lt2020-01-01   * Returns claims data that occurred before 2020-01-01 * https://\\&lt;hostname&gt;/v1/fhir/ExplanationOfBenefit?patient=\\&lt;beneficiaryId&gt;&amp;service-date=ge2020-01-01&amp;service-date=le2020-01-31   * Returns claims data that occurred between 2020-01-01 and 2020-01-31 (inclusive)</p> <p>If a claim entity's date field is unset/null and the caller supplies a <code>service-date</code>, then the claim will not be returned to the caller. Since we cannot guarantee that the claim falls within the supplied filter, we cannot return it.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0007-service-date-filter.html#proposed-solution-unresolved-questions","title":"Proposed Solution: Unresolved Questions","text":"<p>None at this time</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0007-service-date-filter.html#proposed-solution-drawbacks","title":"Proposed Solution: Drawbacks","text":"<ol> <li>Since the filtering is on the application layer, instead of the database layer, we may roundtrip claim entities that will be filtered out.</li> </ol>","tags":["RFC's"]},{"location":"rfcs/implemented/0007-service-date-filter.html#proposed-solution-notable-alternatives","title":"Proposed Solution: Notable Alternatives","text":"<ol> <li>To ensure that we are not returning beneficiary data to an ACO to which that beneficiary is not attributed, we considered using the <code>_lastUpdated</code> query parameter, capped at the end of the current plan year. While this approach would achieve the guarantee of restricting claims data, it can lead to false negatives. Claims data that were completed before the end of the year but written to CCW in the new year would not be returned.</li> <li>The <code>service-date</code> like filter was previously implemented, but removed due to lack of adoption. While BCDA has identified a use case for this functionality, it may be BCDA-specific and thus should be implemented on the BCDA side. Since BFD is the gatekeeper of data, server side filtering is appropriate.</li> </ol>","tags":["RFC's"]},{"location":"rfcs/implemented/0007-service-date-filter.html#prior-art","title":"Prior Art","text":"<p>Implementation of the <code>service-date</code> filter will be based on the approach taken for <code>_lastUpdated</code> filter. Since the claim types have the necessary data, no new fields need to be added to achieve the functionality.</p> <p>The <code>service-date</code> query parameter is based off of the <code>service-date</code> search parameter listed in the Carin BB specification.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0007-service-date-filter.html#future-possibilities","title":"Future Possibilities","text":"<ol> <li>Terminated ACOs - For terminated ACOs, we need to allow them to retrieve claims data up until their termination date. With the <code>service-date</code>, we can guarantee that they do not receive any data that completed after their termination date.</li> </ol>","tags":["RFC's"]},{"location":"rfcs/implemented/0007-service-date-filter.html#addendums","title":"Addendums","text":"<p>The following addendums are required reading before voting on this proposal:</p> <ul> <li>(none at this time)</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0009-pipeline-orchestration-baby-steps.html","title":"RFC Proposal","text":"<ul> <li>RFC Proposal ID: <code>0009-pipeline-orchestration-baby-steps</code></li> <li>Start Date: 2021-02-22</li> <li>RFC PR: CMSgov/beneficiary-fhir-data#462</li> <li>JIRA Ticket(s):<ul> <li>BFD-652: Story: Improve BFD Pipeline Job Orchestration</li> </ul> </li> </ul>  <p>The BFD Pipeline already has several ETL-ish tasks/jobs that it runs. Over the next year or two, it is likely to acquire several more,   including, for example the DC Geo data load. This proposal details architectural changes that   can and should be made in order to accomodate these requirements   (since the BFD Pipeline architecture was originally designed with only one task/job in mind).</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0009-pipeline-orchestration-baby-steps.html#status","title":"Status","text":"<ul> <li>Status: Implemented</li> <li>Implementation JIRA Ticket(s):<ul> <li>BFD-704</li> </ul> </li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0009-pipeline-orchestration-baby-steps.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>RFC Proposal</li> <li>Status</li> <li>Table of Contents</li> <li>Motivation</li> <li>Proposed Solution<ul> <li>Immediate Changes</li> <li>Additional Options/Phases</li> <li>Option: Do We Need To Run Jobs In Parallel?</li> <li>Option: Do We Need Better Obervability?</li> <li>Option: Do We Need To Run Jobs On More Than One Host?</li> <li>Option: Do We Want a More Off-the-Shelf Job Orchestrator?</li> <li>Option: Do We Need To \"Fan Out\" the Execution of a Single Job Across Multiple Workers?</li> <li>Proposed Solution: Detailed Design</li> <li>Immediate Changes</li> <li>Option: Do We Need To Run Jobs In Parallel?</li> <li>Option: Do We Need Better Obervability?</li> <li>Proposed Solution: Unresolved Questions</li> <li>Proposed Solution: Drawbacks</li> <li>Proposed Solution: Notable Alternatives</li> </ul> </li> <li>Prior Art</li> <li>Future Possibilities</li> <li>Addendums</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0009-pipeline-orchestration-baby-steps.html#motivation","title":"Motivation","text":"<p>We're at a first inflection point in the complexity of BFD's ETL jobs/tasks,   with the number of such jobs going past just a few. Accordingly, it's time to re-evaluate some of the architectural decisions in the BFD Pipeline,   to better accomodate the increasing number of jobs   and ensure that the application remains maintainable.</p> <p>By making modest architectural changes now,   and planning for further medium-term changes,   we can help ensure the continued success of our ETL efforts.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0009-pipeline-orchestration-baby-steps.html#proposed-solution","title":"Proposed Solution","text":"<p>As always, the trick here is to \"right-size\" our architectural choices;   we don't want to under-engineer things and end up with a bunch of things getting hacked in, in ugly ways;   we don't want to over-engineer things and thus end up with unnecessary long-term development and maintenance expenses. Accordingly, this proposal lays out some short-term steps that will unblock new ETL jobs immediately,   and also suggests further phases/options that can be pursued in the future, if/as needed.</p> <p>Folks tend to overthink ETL a bit. At its core, ETL is really just one or more tasks/jobs that run,   which happen to move data around. Those jobs often (though not always) have dependencies,   such that Job A has to complete before Job B should start. Those jobs need to be scheduled, such that they run when a trigger condition occurs,   though those conditions are often just along the lines of \"try to run every five minutes\". If you stop and think about it, this is all very similar to any other workflow orchestration problem:   figuring out what to run, when.</p> <p>Of course, much like the broader workflow orchestration field,   there are a lot of folks looking to push/sell very complicated solutions. Those are overkill for ETL just as often as they are for anything else. BFD's problems do not currently need a complex business rules engine,   nor do they currently need a complex ETL orchestration engine...   but I repeat myself.</p> <p>To be sure, that could happen in the future! But that would be a future where BFD has dozens of ETL jobs,   and one where those jobs have complex interdependencies. Not so this year, or next.</p> <p>What does BFD need then? First, let's talk about what ETL BFD currently does and how it might be quickly improved. The BFD Pipeline already has several different ETL jobs that it runs:</p> <ol> <li>The original <code>DatabaseSchemaManager</code> job, that updates the database schema if/as needed.</li> <li>The original <code>RifLoader.process()</code> job, that loads the CCW data from RIF files in S3.</li> <li>A newer <code>RifLoaderIdleTasks.fixupBeneficiaryExecutor()</code> job, that updates MBI hashes in the <code>Beneficiaries</code> table.</li> <li>A newer <code>RifLoaderIdleTasks.fixupHistoryExecutor()</code> job, that updates MBI hashes in the <code>BeneficiariesHistory</code> table.</li> </ol> <p>But aside from #2 in the list above, the other jobs are all a bit \"shoehorned\" in. Which is fine! ... for a few things,   but we will be adding more jobs this year and it will quickly become less fine.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0009-pipeline-orchestration-baby-steps.html#immediate-changes","title":"Immediate Changes","text":"<p>In the short term, this RFC proposes \"un-shoehorning\" the other three jobs, above (#1, #3, and #4). From some analysis, it appears that the BFD Pipeline already has   a solid foundation for a more robust job orchestration system;   with just some moderate refactoring it can easily support a number of additional jobs.</p> <p>This refactoring should result in a restructuring of the BFD Pipeline's modules,   ending with a set of modules like the following:</p> <ul> <li><code>bfd-pipeline-shared-utils</code>: The set of utility and framework code shared across the application, including:<ul> <li>The interface definition(s) for ETL jobs.</li> <li>The DB schema upgrade ETL job.</li> </ul> </li> <li><code>bfd-pipeline-ccw</code>: The ETL jobs related to the CCW, including #2, #3, and #4, above.</li> <li><code>bfd-pipeline-app</code>: The application launcher and job management/orchestration code.</li> </ul> <p>In the future, we should also expect modules like the following to exist:</p> <ul> <li><code>bfd-pipeline-synthetic-data</code>: The ETL job(s) related to synthethic data, including any data generated by Synthea.</li> <li><code>bfd-pipeline-dc-geo</code>: The ETL job(s) related to the DC Geo effort.</li> </ul> <p>With these modest short-term changes, the BFD Pipeline will be able to:</p> <ul> <li>Run multiple jobs, not in parallel.</li> <li>Simply implement new jobs, with only minor dependencies on the larger application.</li> <li>Isolate job code and execution, within the BFD Pipeline application and JVM.</li> <li>Scale compute vertically, if performance needs to be dialed up.</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0009-pipeline-orchestration-baby-steps.html#additional-optionsphases","title":"Additional Options/Phases","text":"<p>As/if needed, the following additional options/phases can be implemented,   to meet additional business/product goals,   as they become needed.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0009-pipeline-orchestration-baby-steps.html#option-do-we-need-to-run-jobs-in-parallel","title":"Option: Do We Need To Run Jobs In Parallel?","text":"<p>If one job's runtime is long enough to violate SLOs for other jobs,   then it may become necessary to run jobs in parallel,   such that Job B which does not depend on Job A will run at the same time as Job A.</p> <p>The simplest solution to this business need is to update the job management system such that:</p> <ul> <li>Job dependencies can be expressed and are honored (e.g. \"Job C should always run after Job B\").</li> <li>Jobs are run in an executor with more than one thread.</li> </ul> <p>That's it: nice &amp; simple.</p> <p>Please note that running jobs in parallel does not necessarily mean running jobs on multiple hosts. Multiple jobs can easily run in parallel on a single host.   particularly since most job threads will spend their time waiting around for I/O to complete. See the \"Option: Do We Need To Run Jobs On More Than One Host?\" section for details on that business need.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0009-pipeline-orchestration-baby-steps.html#option-do-we-need-better-obervability","title":"Option: Do We Need Better Obervability?","text":"<p>Perhaps engineers/operators often find themselves wondering things like:</p> <ul> <li>When did Job A last execute and how long did it take to complete?</li> <li>Did Job B fail last night?</li> <li>and so on...</li> </ul> <p>If so, there are a couple of potential solutions, including:</p> <ul> <li>Comprehensive logging,     with good documentation that tells operators     how to answer common questions from the logs.</li> <li>Storing job status and results in a database,     with good documentation that tells operators     how to answer common questions from the database.</li> </ul> <p>If something fancier is needed, choose the database option   and consider adding a simple web interface on top of it. But try to avoid that, as BFD would now have yet another piece of infrastructure to maintain and operate.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0009-pipeline-orchestration-baby-steps.html#option-do-we-need-to-run-jobs-on-more-than-one-host","title":"Option: Do We Need To Run Jobs On More Than One Host?","text":"<p>Perhaps, for performance reasons, vertical scaling of a single job-running-system is no longer sufficient. In that case, the next obvious step to improving performance would be to run jobs across multiple hosts,   such that Job A could run on Host \"Foo\" at the same time that Job B runs on Host \"Bar\".</p> <p>The simplest solution to this business need is to update the BFD Pipeline such that:</p> <ul> <li>It is auto-scaled across EC2 hosts.</li> <li>Jobs guard/lock against duplicate execution, using a shared system,     such as the BFD database.</li> <li>Jobs use the same shared system to track execution and check dependencies.</li> </ul> <p>Unlike the previous options, this solution starts to add a moderate amount of complexity. It's not complex! But it's also no longer a simple, easy change. This isn't to say that it's a bad idea;   just that this is where real investment starts to be required. In this case, it's 1-3 senior engineer-sprints worth of work.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0009-pipeline-orchestration-baby-steps.html#option-do-we-want-a-more-off-the-shelf-job-orchestrator","title":"Option: Do We Want a More Off-the-Shelf Job Orchestrator?","text":"<p>Honestly, this is not so much a business need, as a technical want. Accordingly, I'm inclinced to argue against it:   unless you can articulate (and prove) what such an off-the-shelf orchestrator,   e.g. Airflow,   will specifically get you,   and prove/demonstrate that said orchestrator is mature and reliable and flexible enough to meet our needs,   don't do this;   it's a bad idea.</p> <p>Nevertheless, if you're dead set on this, I'd suggest the following:</p> <ul> <li>Lean heavily towards mature distributed systems deployed widely at very large companies.<ul> <li>You will hate how complex/expensive they are to deploy and operate,     but you will appreciate the relative feature set, lack of bugs, etc.</li> <li>This means: more like Netflix's Conductor     and AWS' Step Functions;     less like Airflow.</li> </ul> </li> <li>Consider investing in Nomad     or Kubernetes at the same time,     so that you can avoid vendor lock-in     and also so developers can continue to benefit from being able to run code locally.<ul> <li>Note: this is another way of saying, \"AWS Step Functions\" are a great example of vendor lock-in.</li> </ul> </li> <li>Plan on this taking at least a full PI to fully prototype, implement, and deploy.</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0009-pipeline-orchestration-baby-steps.html#option-do-we-need-to-fan-out-the-execution-of-a-single-job-across-multiple-workers","title":"Option: Do We Need To \"Fan Out\" the Execution of a Single Job Across Multiple Workers?","text":"<p>Suppose that very tight SLOs are mandated,   such that a single ETL job needs to \"fan out\" its processing across multiple hosts   (not just threads). This pre-supposes that you've exhausted the limits of vertical scaling   (which, it's worth noting, seems unlikely).</p> <p>The proposed solution here is basically:   see the above \"Option: Do We Want a More Off-the-Shelf Job Orchestrator?\" section   and figure out how to fan-out on top of an existing, mature platform. While it sounds like fun (to weirdos like me),   implementing a bespoke system that can do this correctly is not for the faint of heart.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0009-pipeline-orchestration-baby-steps.html#proposed-solution-detailed-design","title":"Proposed Solution: Detailed Design","text":"<p>This detailed design is limited to just a few of the options, above. The other options/sections are more complex pieces of work and thus ought to be considered separately,   outside of this RFC.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0009-pipeline-orchestration-baby-steps.html#immediate-changes_1","title":"Immediate Changes","text":"<p>Digging into the current code a bit, the BFD Pipeline already has a lot of the boring machinery required to run multiple jobs:   a task executor instrumented for observability,   task wrappers to handle failures appropriately,   startup and shutdown routines,   etc. Except... that's all currently configured to only ever run one job,   and all of the other current jobs get kicked off by that main job before or after it runs its main logic. That machinery is captured in the <code>DataSetMonitor</code> class.</p> <p>Per this proposal, let's refactor <code>DataSetMonitor</code> in the short-term such that:</p> <ul> <li>It runs an ordered list of jobs (rather than a single one), one after the other.</li> <li>It has a more generic name that reflects its larger scope, e.g. <code>PipelineManager</code>.</li> <li>It gets restructured out of the CCW-specific <code>bfd-pipeline-rif-extract</code> module     into one that reflects its larger scope.</li> <li>All of the current four ETL tasks are run as generic, independent jobs,     rather than being shoehorned into the CCW data load job.</li> <li>Add a sample, disabled job that demonstrates how new jobs can be added and managed.</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0009-pipeline-orchestration-baby-steps.html#option-do-we-need-to-run-jobs-in-parallel_1","title":"Option: Do We Need To Run Jobs In Parallel?","text":"<p>Relationships between jobs would be expressed as a directed-acyclic graph (DAG). For the current jobs, that DAG would look like this:</p> <pre><code>            +----------------------------+\n            |                            |\n            | Database Schema Management |\n            |                            |\n            +--------------+-------------+\n                           ^\n                           |\n                           |\n                   +-------+------+\n                   |              |\n                   | CCW RIF Load |\n                   |              |\n                   +-+----------+-+\n                     ^          ^\n                     |          |\n                     |          |\n+--------------------+--+    +--+----------------------+\n|                       |    |                         |\n| Beneficiary MBI Fixup |    | Beneficiary History MBI |\n|                       |    |                         |\n+-----------------------+    +-------------------------+\n</code></pre> <p>That DAG will need to be expressed declaratively in the BFD code, e.g. <code>jobA.dependsOn(jobB, \"version 42\")</code>. Then the <code>PipelineManager</code> and related classes will need to be updated to honor it. Once that is complete, the <code>PipelineManager</code> likely won't have to do anything more exciting than   increasing the size of its main job threadpool from <code>1</code> to <code>N&gt;1</code>, and jobs will start running in parallel. Easy peasy, lemon squeezy.</p> <p>Fun fact: this option may also allow us to speed up the execution of the existing CCW RIF Load task,   if the loads for each RIF record type are run in parallel, where possible.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0009-pipeline-orchestration-baby-steps.html#option-do-we-need-better-obervability_1","title":"Option: Do We Need Better Obervability?","text":"<p>Observability is a big old rabbit hole; it all depends on what questions engineers and operators want answered. That said, we should start with a few relatively simple steps,   and see if that provides enough information to answer the most commong questions:</p> <ol> <li>Record the start and end of each job run in a database, but only for runs where the job actually has work to do.</li> <li>Publish those same start and end events to New Relic, as well.</li> <li>Publish those same start and end events to our logs (and thus Splunk), as well.</li> </ol> <p>Those runs/events should also include basic job status details,   e.g. whether or not the job succeeded. For failures, it'd be nice to also include a brief one-liner error message, as well.</p> <p>That should go a long way towards helping folks understand the state and behavior of our ETL systems.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0009-pipeline-orchestration-baby-steps.html#proposed-solution-unresolved-questions","title":"Proposed Solution: Unresolved Questions","text":"<p>No unresolved questions have been identified at this time.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0009-pipeline-orchestration-baby-steps.html#proposed-solution-drawbacks","title":"Proposed Solution: Drawbacks","text":"<p>The most common concern I hear with this single-codebase approach is something along the lines of,   \"I don't want to have to deal with someone else's framework/application;   I just want to create and deploy this new thing as a standalone task.\"</p> <p>This is a very understandable concern! There's definitely a tradeoff in keeping things in one codebase:   it lowers long-term maintenance costs by ensuring the BFD team only has one codebase to support,   but can increase the costs of initial development by forcing the new job developer(s) to become familiar with that codebase.</p> <p>However, those risks are very manageable, if addressed appopriately. Which is what we're trying to do here with this design:   we want to keep the \"surface area\" of contact between each job and the framework,   and between the different ETL jobs,   as minimal as possible. Inasmuch as we can manage it, each job will basically be just a Java <code>Runnable</code>   and can do whatever it needs to, however it wants to do it.</p> <p>Other concerns I hear with this approach basically boil down to,   \"but I want to use [language/platform X], instead.\" This is also a very understandable concern! There's definitely a tradeoff in restricting the programming language and platform used for ETL tasks:   it lowers long-term maintenance costs by ensuring the BFD team only has one language and platform to support,   but can increase the costs of initial development by forcing the new job developer(s) to become familiar with that language and platform.</p> <p>For the long-term health of the BFD platform, this is just the appropriate tradeoff to make. The BFD team is not and likely won't ever be particularly large,   and thus needs to aggressively limit the scope of the services that they support.</p> <p>Occassionally, this concern gets reframed as \"BFD should move all its ETL jobs to [language/platform X]   and this new job will just use it for now to prove why that's such a great idea.\" I've been guilty of this move sometimes, myself! But historical reality rather convincingly demonstrates that, no,   the BFD team will never have the resources and freedom to move everything to the new platform. Instead, the BFD team will now be saddled with the maintenance burdens of two platforms, rather than one,   with no corresponding increase in team size or budget. Unfortunately, unless the team proposing this is also proposing to migrate everything themselves,   this is not a realistic approach.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0009-pipeline-orchestration-baby-steps.html#proposed-solution-notable-alternatives","title":"Proposed Solution: Notable Alternatives","text":"<p>These questions are addressed as part of other sections, instead.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0009-pipeline-orchestration-baby-steps.html#prior-art","title":"Prior Art","text":"<p>The following discussion is very relevant to this RFC:   Hacker News: How to Become a Data Engineer in 2021.</p> <p>Here are my notes from the original article:</p> <ul> <li>Older tools such as Informatica, Pentaho, and Talend are characterized as legacy approaches.</li> <li>I don't really buy the assertin that data engineers need to be practiced with     a DB's underlying DB structures and algorithms, e.g. B-trees.   That said, I would agree that they should be at least passingly familiar with them.<ul> <li>I think the most important insight from this is really related to index caching:     modern DBs will try to keep \"hot\" index pages cached in memory     and may exhibit pathological behavior when they can't.   Which portions of the trees are likely to be \"hot\"?   The upper levels of the tree, as bounded by the system's page size.</li> <li>This insight has been particularly important when interacting with PostgreSQL's query planner.   If the DB determines that it can't keep \"enough\" index pages in memory, it will refuse to use the index.   In addition to being frustrating due to the poor visibility developers have into this behavior,     it also cautions against viewing table partiioning as a silver bullet:     there's no reason to assume that simply having more, smaller index trees will perform any better.   And on the flip side, it points towards DB sharding as potentially being necessary in the future.   If our indices ever outgrow what we can fit into memory on large RDS instances,     sharding seems like a likely (albeit expensive) solution.   To be clear, I'm still of the opinion that we're a long way away from needing to shard,     but it's worth keeping in mind for the future,     in addition to evaluating alternative DB platforms.</li> </ul> </li> <li>Both the article and discussion repeatedly make the point that SQL is an essential technology.   This rings true: it is still clearly the best tool for many data problems.</li> <li>The article calls out Python's poor performance as a concern.   I share this concern, but think it's nevertheless worth exploring Airflow and other Python-based options.</li> <li>When reading the article's \"Big Data Tools\" section it's worth keeping in mind     what problems we are trying to solve for BFD.</li> <li>Problems we don't have:<ul> <li>We can invent an event streaming problem for ourselves but we don't intrinsically need to apply that technique.</li> <li>We don't have much in the way of data processing to do.</li> <li>We don't need an analytics platform.</li> </ul> </li> <li>Problems we do have:<ul> <li>We're doing a massive amount of very simple ETL under modest time constraints.</li> <li>Actually, it's mostly \"EL\" not \"ETL\": we don't want to apply many data transformations at load time.   If we had to reload/reprocess all records every time we changed our mapping we would be in a very bad place.</li> </ul> </li> <li>It's also worth keeping in mind the scale of our systems:   We have terabytes of data but not petabytes.   Billions of records but not trillions.   We're not really a big data system, as such.   Instead, BFD is just a data-lake-sized online database, heavily optimized to support a limited number of query types.</li> </ul> <p>Here are my notes from the article's discussion on HN:</p> <ul> <li>Lots of mentions of Snowflake, though that doesn't seem germane to the problems we're looking at here.   (Worth considering later, though.)</li> <li>dbt sounds interesting, but again: we don't want to do much transformation prior to load.<ul> <li>If we ever wanted to dual-purpose the DB as an analytics platform, I think we should look at dbt.</li> </ul> </li> <li>Fivetran sounds interesting, but appears to not offer a hosted option,     and is thus a non-starter, unless/until they get FedRAMP'd.</li> <li>It references this,     Emerging Architectures for Modern Data Infrastructure,     which is interesting in general, but also has the a useful new (to me) acronym:     \"ELT\" for \"extract, then load, then transform\" and calls it out as being less brittle than traditional ETL.   Nice term for capturing what we do in BFD.</li> <li>A comment mentioned \"Data Vault\", which turned out to be an interesting read:     Data vault modeling.   I'm not sold on the suggested storage structure, but the underlying philosophy makes sense.</li> <li>These two comments ring true: https://news.ycombinator.com/item?id=25733701     and https://news.ycombinator.com/item?id=25732147.   Developers uncomfortable with SQL should be encouraged and supported to \"push through\" that.</li> <li>AWS Step Functions appear to be the preferred approach     when going serverless.</li> <li>As a complete sidenote, I wandered across this very useful article while reading this discussion and related items:     We\u2019re All Using Airflow Wrong and How to Fix It.   It makes the case that Airflow's built-in operators are buggy and hard to debug     and argues for instead using just the Kubernetes operator to run custom code for every task.   It's a compelling argument, especially since we could just as easily substitute in Docker, instead.</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0009-pipeline-orchestration-baby-steps.html#future-possibilities","title":"Future Possibilities","text":"<p>TODO</p> <p>Think about what the natural extension and evolution of your proposal would be and how it would affect the language and project as a whole in a holistic way. Try to use this section as a tool to more fully consider all possible interactions with the project and language in your proposal. Also consider how the this all fits into the roadmap for the project and of the relevant sub-team.</p> <p>This is also a good place to \"dump ideas\", if they are out of scope for the RFC you are writing but otherwise related.</p> <p>If you have tried and cannot think of any future possibilities, you may simply state that you cannot think of anything.</p> <p>Note that having something written down in the future-possibilities section is not a reason to accept the current or a future RFC;   such notes should be in the section on motivation or rationale in this or subsequent RFCs. The section merely provides additional information.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0009-pipeline-orchestration-baby-steps.html#addendums","title":"Addendums","text":"<p>TODO</p> <p>The following addendums are required reading before voting on this proposal:</p> <ul> <li>(none at this time)</li> </ul> <p>Please note that some of these addendums may be encrypted. If you are unable to decrypt the files, you are not authorized to vote on this proposal.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html","title":"RFC Proposal","text":"<ul> <li>RFC Proposal ID: <code>0011-separate-flyway-from-pipeline</code></li> <li>Start Date: 2022-02-02</li> <li>RFC PR: beneficiary-fhir-data/rfcs#0011</li> <li>JIRA Ticket(s):</li> <li>BFD-1483</li> </ul> <p>Database migrations in BFD are critical system events that will continue to be a common occurrence as the database schema undergoes enhancements for performance, maintainability, and support for new data fields. This RFC proposes separating the execution of Flyway migrations from the pipeline application in order to make the process of developing and deploying migrations more robust and efficient.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#status","title":"Status","text":"<ul> <li>Status: Implemented</li> <li>Implementation JIRA Ticket(s):</li> <li>BFD-1560</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>RFC Proposal</li> <li>Status</li> <li>Table of Contents</li> <li>Motivation</li> <li>Proposed Solution<ul> <li>Proposed Solution: Detailed Design</li> <li>Proposed Solution: Unresolved Questions</li> <li>Proposed Solution: Drawbacks</li> <li>Proposed Solution: Notable Alternatives</li> </ul> </li> <li>Prior Art</li> <li>Future Possibilities</li> <li>Addendums</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#motivation","title":"Motivation","text":"<p>Database migrations are a complex topic. Before delving into the details of the motivation and proposal it will be helpful to define some terminology, identify the relevant system components, and differentiate between different types of migrations.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#background-on-bfd-applications","title":"Background on BFD Applications","text":"<p>It is assumed that readers of this proposal are generally familiar with two open source tools that BFD depends on for database operations:</p> <ul> <li>Flyway -- for executing database schema migration scripts</li> <li>Hibernate -- for object-relational mapping between the applications and the database</li> </ul> <p>The two applications that make up BFD will be considered in terms of their roles in database migration and database operations:</p> <p>BFD Pipeline Application: - Invokes Flyway on startup (which will execute any new migrations) - Runs hibernate validation on startup - Reads and writes data to the database</p> <p>BFD API Server Application: - Runs hibernate validation on startup - Reads data from the database (no writes)</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#background-on-database-migrations","title":"Background on Database Migrations","text":"<p>Within this proposal we will consider a database migration to be a group of one or more of the following types of changes that are deployed together:</p> <ol> <li>A database schema change consisting of one or more SQL migration scripts executed by Flyway</li> <li>An application change in the ORM layer (typically in relation to a change in the schema)</li> </ol> <p>When reasoning about different types of database migrations, it is necessary to consider that both the schema and the application may change. We will refer to the schema and application as they exist immediately prior to the deployment of the database migration as the old schema and the old application, and the schema and application that result from running the database migration to completion as the new schema and the new applications. Old and new components are only discussed in the context of a particular deployment (and not in a cumulative manner across multiple deployments). It is assumed that the old application is compatible with the old schema (since that is what is current running on the system) and that the new application is compatible with the new schema (ensuring this is a basic testing requirement of any change to the system). The interactions between old and new components though is important in understanding this proposal and leads to some classifications of migrations based on the compatibility of the schema change with old and new versions of the application:</p> <ul> <li>A backward-compatible database migration is one where the new schema is compatible with the old applications (otherwise it is considered to be backward-incompatible)</li> <li>A forward-compatible database migration is one where the old schema is compatible with the new applications (otherwise it is considered to be forward-incompatible)</li> <li>A fully-compatible database migration is one that is both backward-compatible and forward-compatible</li> <li>A fully-incompatible database migration is one that is both backward-incompatible and forward-incompatible</li> </ul> <p>Frequently, database migrations include both a schema change and an accompanying application change although there are some database migrations that consist only of a schema change. Strictly speaking, a database migration that does not contain a schema change is just an application change like any other application change. It is useful though for the purposes of this RFC for application changes deployed independently of a schema change to also be considered database migrations. Of interest in the next section is the fact that as defined above, a database migration that consists only of an application change is fully-compatible because it must work with both old and new schema (which are the same) to be a valid change at all.</p> <p>An example of a database migration that consists of both a schema change and an application change is adding a new column to a table and modifying the application to start referencing that column. This database migration can be decomposed into two database migrations: one consisting of the change to add a column to a table, and another to modify the application to reference the new column. The next section shows that decomposing migrations can affect the compatibility.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#examples-of-common-migrations-in-bfd-and-their-compatibility-status","title":"Examples of common migrations in BFD and their compatibility status","text":"","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#adding-a-new-column-to-an-existing-table-and-changing-the-application-to-use-that-column","title":"Adding a new column to an existing table and changing the application to use that column","text":"<ul> <li>This is backward-compatible because the old application will be unaffected by new columns that it does not reference.</li> <li>This is forward-incompatible because the new application requires a column that is not present in the old schema.</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#renaming-a-column-and-updating-the-application-references-for-that-column","title":"Renaming a column and updating the application references for that column","text":"<ul> <li>This is a fully-incompatible migration because old applications still reference the column by its old name (so will not work with the new schema) and the new applications reference the column by the new name (so will not work with the old schema).</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#dropping-a-column-and-changing-the-application-so-that-it-no-longer-references-that-column","title":"Dropping a column and changing the application so that it no longer references that column","text":"<ul> <li>This is backward-incompatible because old applications reference the column which will not be present in the new schema.</li> <li>This is forward-compatible because new applications work properly with or without the column.</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#adding-a-new-column-or-table-and-not-changing-the-application-to-use-it","title":"Adding a new column or table and NOT changing the application to use it","text":"<ul> <li>This is fully-compatible.</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#updating-only-the-application-to-start-using-a-column-or-table-that-already-exists","title":"Updating ONLY the application to start using a column or table that already exists","text":"<ul> <li>This is fully-compatible.</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#dropping-a-column-from-the-schema-only-that-the-application-does-not-reference","title":"Dropping a column from the schema ONLY (that the application does not reference)","text":"<ul> <li>This is fully-compatible.</li> </ul> <p>Of note here is that a migration that is fully-incompatible (like renaming a column in the schema and the application) can be accomplished in a manner that is fully-compatible by breaking it into multiple migrations (which would have to be deployed separately).</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#background-on-types-of-bfd-deployments","title":"Background on types of BFD deployments","text":"<p>There are three types of deployments that come up when considering how to deploy database migrations:</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#jenkins-deployment","title":"Jenkins deployment","text":"<ul> <li>Fully automated deployment via Jenkins</li> <li>Typical deployment option for almost all changes (migration or otherwise)</li> <li>New applications are deployed with a period of overlap with the old applications</li> <li>Requires no downtime</li> <li>In-place deployment that requires no additional hardware</li> <li>Has constraints on the type of migrations that can be deployed (discussed in detail below)</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#abbreviated-deployment-diagram","title":"Abbreviated Deployment Diagram","text":"Diagram Notes  As with most graphical representations, it can be difficult to strike the appropriate balance between information density and accuracy. The `Jenkins` participant is shorthand for our existing \"BFD - Multibranch and Multistage Pipeline.\" Participants like `bfd-pipeline` and `bfd-server` are composites of the Jenkins deployment stage (generalized for the three existing `test`, `prod-sbx`, and `prod` environments), necessary AWS API endpoints that accommodate the deployment of these resources via terraform, **and** the resultant BFD resources running in each environment.","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#cloned-deployment","title":"Cloned deployment","text":"<ul> <li>A cloned environment handles traffic while the primary instance undergoes the deployment and then traffic is redirected back to the primary</li> <li>Manual deployment</li> <li>Requires no downtime</li> <li>Has fewer constraints on the types of migrations that can be deployed than the standard Jenkins deployment</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#downtime-deployment","title":"Downtime deployment","text":"<ul> <li>Service is interrupted for some period of time</li> <li>Has additional coordination and communication requirements and acceptance of downtime window and risk mitigation plan</li> <li>Manual or Automated deploy are possible during the downtime window</li> <li>Can be used when downtime is required or desired due to other factors</li> </ul> <p>In general, a Jenkins deployment is preferred because it is a fully automated process that results in no downtime, has low risk of human error, no additional hardware costs, and requires no coordination or communication apart from what is otherwise required for any particular change. The other types of deployments can be used for special situations where it is determined to be preferable for reasons of cost, risk mitigation, or otherwise. This RFC will focus on optimizing the Standard Jenkins deployment since it is the preferred deployment and is most commonly used. Optimizing the Jenkins deployment can also make it a more viable option for certain complex migrations that otherwise would be candidates for a cloned deployment or a downtime deployment.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#constraints-on-migrations-that-are-deployed-via-jenkins-deployment","title":"Constraints on migrations that are deployed via Jenkins deployment","text":"<p>During a Jenkins deployment the old API server continues to run and serve traffic during the deployment of the new applications until the new API server is fully deployed and available. This means that it is possible that the old API server will attempt to read from the database after the new schema is in place. This leads to a constraint on BFD migrations:</p> <ul> <li>Database migrations must be backward-compatible</li> </ul> <p>During a Jenkins deployment the new versions of the two applications are deployed simultaneously. This means that the order of those deployments is indeterminate and the new API server may come online before the new schema changes are in place. This leads to another constraint:</p> <ul> <li>Database migrations must be forward-compatible (one way to accomplish this is to separate the schema and application changes into different deployments)</li> </ul> <p>Lastly, due to BFD auto-scaling of the API servers, it is possible that additional API servers running the old software may come online and perform hibernate validation against the new database schema. Even if the migration is backward-compatible this can lead to errors. This adds one more constraint:</p> <ul> <li>Hibernate validation must be turned off in a deployment prior to running a database migration (and so turned back on in a deployment following the migration deployments).</li> </ul> <p>Combining these constraints yields the current standard practice for deploying a BFD database migration:</p> <ol> <li>Deploy a PR that disables hibernate validation.</li> <li>Deploy a PR that consists of just the schema portion of the migration which must be backward-compatible.</li> <li>Deploy a PR that consists of just the application portion of the migration.</li> <li>Deploy a PR that enables hibernate validation.</li> </ol> <p>With this standard practice there is no requirement for forward-compatibility but backward-compatibility is a requirement. Through decomposition of migrations, backward-compatible migrations can be used for most changes that commonly occur so we accept this as a general requirement for BFD migrations.</p> <p>When followed correctly, the four-step process above provides a safe way of performing backward-compatible migrations. However, the need for four PRs for a single database migration increases the effort to develop and deploy the change significantly. Having more PRs also complicates the review process and results in a single logical change being fragmented into multiple commits.</p> <p>The proposed change of moving Flyway migrations out of the BFD pipeline application will allow any backward-compatible migration to be deployed as a single PR using a Jenkins deployment.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#background-on-execution-time-characteristics-of-migrations","title":"Background on execution time characteristics of migrations","text":"<p>Execution time of a database migration plays a part in deciding how it will be deployed. We will consider three categorizations: * Short-running (often seconds, but arbitrarily anything less than one hour) * Long-running asynchronous (run time is hours or days but can be run in the background by the DB server) * Long-running synchronous (run time is hours or days and cannot be run in the background)</p> <p>Short-running migrations are preferred when there are options available although frequently the task determines whether a short-running migration can be used. Examples of short-running migrations include creating new empty tables, adding, dropping, or renaming columns, granting permissions, and performing index/constraint builds on table that are empty or have a very small number of rows.</p> <p>For long-running migrations, asynchronous is preferred but not available for many operations. An example is index building in Postgres by specifying the CONCURRENT keyword. Care must be given to the fact that the application will be online while the operation is running (so dropping an index that the application needs while building a new one asynchronously is not acceptable).</p> <p>Long-running synchronous migrations are the least desirable and require the most care. Examples include creating a new table as a copy of an existing large table, altering the datatype of a column in a large table, creating an index in a large table without specifying the CONCURRENT keyword. Due to the Flyway migration being performed during the startup of the BFD Pipeline application, a long-running synchronous migration that is deployed with a Jenkins deployment will block the Pipeline application from consuming data for the duration of the migration. The impact of this limitation will become more significant as the pipeline RDA job goes into production.</p> <p>This RFC proposal addresses the concern of long-running synchronous migrations blocking the BFD Pipeline as well as improvements to the Jenkins deployment pipeline to better support these long-running migrations when they are necessary.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#other-shortcomings-that-can-be-addressed-by-this-proposal","title":"Other shortcomings that can be addressed by this proposal","text":"<p>In addition to optimizing the development and deployment of database migrations, the proposed changes can also solve other issues related to database migrations:</p> <ul> <li>Database object privileges and high level roles (not individual user roles) are not set consistently or in a way that supports database credential rotations. Ideally these privileges and roles would be developed and then deployed across environments in a Flyway migration so that their creation is automated and embedded in the source code. This cannot currently be done because the Flyway invocation is part of the BFD Pipeline Application startup which must run as a specific role that is appropriate for the pipeline but does not have the privileges to create roles or set privileges on existing objects.</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#proposed-solution","title":"Proposed Solution","text":"<p>The proposed solution is to remove the invocations of Flyway and Hibernate validation from the current BFD applications and instead run Flyway and Hibernate validation as a step in the deployment that must complete successfully before continuing with the deployment of the BFD Pipeline application and BFD FHIR Server.</p> <p>By moving the Flyway migrations to a step before all other applications are deployed, it no longer is necessary to deploy a schema change separately from its accompanying application change or have forward-compatible changes (since the new application will only ever start up after the new schema has been deployed). Backward-compatibility remains a requirement because the old applications will still coexist with the new schema for a brief period of time.</p> <p>Moving Hibernate validation out of the BFD Pipeline and BFD FHIR Server avoids the issue of old applications coming up during an auto-scaling event and failing Hibernate validation.</p> <p>Long-running migrations will no longer block the BFD Pipeline application processing because the old pipeline application will continue to run and process data until the schema migration completes.</p> <p>The new step that runs the Flyway migrations will be configured to run as a user that has the appropriate privileges to run migrations without having to elevate the privileges of the BFD Pipeline. This will allow schema migrations that create (non-user) database roles and grant privileges to run in Flyway.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#proposed-solution-detailed-design","title":"Proposed Solution: Detailed Design","text":"<p>Application design:</p> <ul> <li>A new Java application 'bfd-db-migrator' will be introduced that performs these functions:</li> <li>Invokes Flyway (and thereby runs all pending migrations)</li> <li>Runs Hibernate validation against the BFD ORM (after Flyway finishes)</li> <li> <p>Exits with a return code that indicates whether all operations were successful or not</p> </li> <li> <p>Flyway invocations and hibernate validation will be removed from the BFD Pipeline and BFD API Server applications.</p> </li> <li> <p>Test infrastructure will be altered to mimic the deployment by running the new application prior to starting the local BFD server for tests that run the BFD server as a standalone process.</p> </li> <li> <p>Test infrastructure will be altered to invoke the new application logic via method call prior to running tests that require a BFD database but do not launch a BFD server.</p> </li> <li> <p>The new Java application code will reside in the existing BFD repo as another sub-module of bfd-parent so that it will be built and tested as part of the current Github actions application workflow and Jenkins App build stage.</p> </li> </ul> <p>Deployment design:</p>  Diagram Notes Like the previous diagram, this illustrates the deployment sequence with some broad definitions. This adds the `bfd-db-migrator` participant that includes a blocking deployment stage for each environment that must complete **before** proceeding to apply updates to the `bfd-pipeline` and `bfd-server` resources for each environment.  <ul> <li> <p>A new migrator service account and associated migrator database role that has all required privileges for running migrations and hibernate validation will be created.</p> </li> <li> <p>The Jenkins APP AMI build phase will build a new migrator AMI that includes the new application.</p> </li> <li> <p>The migrator AMI and migrator service account will be used to run the migrator application on a new EC2 instance as part of a new Jenkins pipeline stage that will be inserted prior to each of the three stages that deploy to TEST, PROD-SBX, PROD.</p> </li> <li> <p>The Jenkins pipeline will not proceed to the next stage (deploying the other applications) until the migrator application has exited with a status indicating success. In the event that this application status indicates that not all operations were successful, the Jenkins deployment will halt and be marked as a failure. Flyway migrations are typically run within a transaction so a failure will cause a rollback and not lead to downtime.</p> </li> <li> <p>The new application will produce log files that contain the Flyway and Hibernate logging that is currently captured in the BFD Pipeline application logs. These new application logs will be sent to Splunk and Cloudwatch.</p> </li> <li> <p>The migrator EC2 instance could be powered off or terminated once the migrator application exits and all artifacts have been gathered.</p> </li> </ul> <p>New Flyway migration once the above is in place:</p> <ul> <li>A new Flyway migration will be developed that creates all non-user roles and sets privileges correctly on all existing database objects.</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#proposed-solution-unresolved-questions","title":"Proposed Solution: Unresolved Questions","text":"<p>What postgres privileges are required to run all possible migrations? Database owner? Or does something else suffice?</p> <p>Is EC2 the right fit for this or would Lambda or other options be a better fit?</p> <p>Do we need to continue to invoke Hibernate validation at all? What benefit do we derive from this check that is not already derived from running the unit and integration tests? What sorts of problems would not be caught by the tests that would be caught by hibernate validation?</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#proposed-solution-drawbacks","title":"Proposed Solution: Drawbacks","text":"<p>This adds time to the deployment, currently estimated at 10-12 minutes total.</p> <p>This makes the Jenkins deployment more complicated and adds another failure mode.</p> <p>This makes the application more complicated and introduces a new point of failure.</p> <p>This makes debugging more complex. There will be new places (log files, possibly directories) where debugging information will be kept.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#proposed-solution-notable-alternatives","title":"Proposed Solution: Notable Alternatives","text":"<p>Instead of running Flyway and Hibernate validation in a new application, Flyway could be invoked from the command line instead. These leaves the question as to where to invoke Hibernate validation. If it continues to be invoked in the applications, it is possible to encounter validation errors if auto-scaling of an old application occurs just after a new schema has been deployed. One possible answer is to not invoke Hibernate validation at all (see open questions).</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#prior-art","title":"Prior Art","text":"<p>Gitlab migration style guide: https://docs.gitlab.com/ee/development/migration_style_guide.html</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#future-possibilities","title":"Future Possibilities","text":"<p>Don't run the migration step if there are no new migrations to run.</p> <p>Make it possible to deploy other changes (not ones with migrations) during a long-running migration.</p> <p>Support a way of specifying that a certain migration should be run as a post-deploy migration, possibly in the background.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0011-separate-flyway-from-pipeline.html#addendums","title":"Addendums","text":"<p>The following addendums are required reading before voting on this proposal:</p> <ul> <li>(none at this time)</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html","title":"RFC Proposal","text":"<ul> <li>RFC Proposal ID: <code>0013-bfd-server-startup-check</code></li> <li>Start Date: 2022-04-24</li> <li>RFC PR: beneficiary-fhir-data/rfcs#0013</li> <li>JIRA Ticket(s):<ul> <li>BFD-1663</li> </ul> </li> </ul> <p>This RFC proposes a minor change to existing bfd-server on-host health checking to satisfy the requirements of a reliable startup check.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html#status","title":"Status","text":"<ul> <li>Status: Implemented</li> <li>Implementation JIRA Ticket(s):<ul> <li>BFD-1685</li> </ul> </li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>RFC Proposal</li> <li>Status</li> <li>Table of Contents</li> <li>Motivation<ul> <li>Issue</li> <li>Discussion</li> <li>Supporting Infrastructure</li> <li>Host-Based Health Checks</li> <li>Health Check Review</li> </ul> </li> <li>Proposed Solution<ul> <li>Visualize the Proposal<ul> <li>Existing Health Checks and Startup</li> <li>Proposed Health Checks and Readiness</li> </ul> </li> <li>Proposed Solution: Detailed Design</li> <li>Proposed Solution: Unresolved Questions</li> <li>Proposed Solution: Drawbacks</li> <li>Proposed Solution: Notable Alternatives</li> <li>Health Check Alternatives in Elastic Load Balancing</li> <li>More Complex Solutions<ul> <li>Reverse Proxying with Nginx or Otherwise</li> <li>Split Control Plane</li> </ul> </li> </ul> </li> <li>Prior Art</li> <li>Future Possibilities<ul> <li>Deployments</li> <li>Readiness Checking</li> <li>DevSecOps</li> </ul> </li> <li>Addendums</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html#motivation","title":"Motivation","text":"","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html#issue","title":"Issue","text":"<p>AWS Classic Load Balancers have limited health check functionality leading to scenarios where traffic can be routed to unhealthy (unsuccessfully started) hosts. In the best case, bfd-server deployments may exhibit brief periods of downtime. In the worst case, the bfd-server may oscillate between healthy and unhealthy application states, resulting in preventable deployment failures.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html#discussion","title":"Discussion","text":"","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html#supporting-infrastructure","title":"Supporting Infrastructure","text":"<p>The Beneficiary FHIR Data Server (bfd-server) is hosted in the Amazon Web Services (AWS) public cloud across a collection of Elastic Compute Cloud (EC2) instances that comprise an Auto Scaling Group (ASG). The ASG is responsible for automatically and horizontally scaling bfd-server out to meet performance goals as well as scaling these instances back in to meet budgetary requirements. In addition to scaling, the ASG uses a combination of status checks provided by EC2 and health checks reported by the load balancer (discussed in the following paragraph) to automatically replace unhealthy instances.</p> <p>To avoid saturating any single instance in the ASG with excessive traffic, requests intended for the bfd-server are balanced by Elastic Load Balancing (ELB) through a Classic Load Balancer (CLB). As mentioned above, the status of the CLB health check is used in part by the ASG to determine whether a host needs to be replaced or not. Additionally, the CLB uses the status of these health checks to determine which hosts should or should not receive traffic. As of this RFC, the health check is effectively a TCP ping against each bfd-server instance port, 7443. The health check designates hosts as healthy when the TCP ping is successful. Because the bfd-server application starts listening on port 7443 almost immediately after starting (but importantly, before reaching a fully started state), the CLB health check can designate non-started hosts as healthy. This is especially common during instance replacement as part of automated deployments. When unprepared hosts receive bfd-server traffic, the requests result in an error.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html#host-based-health-checks","title":"Host-Based Health Checks","text":"<p>In addition to the health checks external to the bfd-server application instance, the instances themselves have an internal health check mechanism running inside the service startup script, encoded in the <code>stop_service_if_failing</code> function. Today, this performs local, authenticated queries against a set of pre-defined API endpoints to determine whether the application is behaving normally. If any errors are detected, it works as a kill switch and stops the service.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html#health-check-review","title":"Health Check Review","text":"<p>So far in this discussion, the term health check has been used to generally describe methods for determining the operating state of an application instance. However, this term is somewhat under specified. There are a variety of health checks that assist in achieving reliability targets. To be a little more precise, it might be helpful to adopt the health check (or probe) concepts from the kubernetes ecosystem, despite the obvious difference in hosting strategies. Kubernetes supports probes for liveness, readiness, and startup where they each answer different questions about a service's health throughout the application instance's life cycle.</p> <p>Liveness can be applicable throughout the lifetime of the application instance and answers the question: \"is the application alive or dead?\" In kubernetes, this relates to whether or not a container should be restarted. Liveness probes are configured at the operator's discretion and support a variety of probing methods including command execution, HTTP requests, TCP requests, and gRPC requests. As this relates to the bfd-server, both the EC2 status checks and the CLB TCP health check help answer the liveness question for the ASG: should a given instance be replaced?</p> <p>Readiness is also applicable throughout the lifetime of the application instance, but it answers a slightly different question: \"is the application ready to receive requests?\" In kubernetes, the status of this check may be transitory (e.g. during startup, spikes in saturation), where operators neither want to restart/replace the running instance, nor should traffic be routed to a non-ready instance. A readiness probe can also be helpful if liveness and readiness criteria are different, and there is significant difference (significance varies, but generally greater than the latency service level objective) between the first successful liveness probe and first successful readiness probe. Because the intent of the bfd-server's <code>stop_service_if_failing</code> is to prevent client traffic from reaching a faulty instance, it was tempting to compare this to a readiness probe. However, it only functions at the startup; naturally, it's more directly comparable to a startup probe. To be clear, there is no analog to the readiness probe in the existing infrastructure, noted as an area for future possibilities below.</p> <p>Startup is most relevant to the early stages of an application instance's lifetime. Startup answers the question: \"has the application started?\" Startup probes continues to ask this until they succeed (or reach a failure threshold). Once successful, liveness and readiness probes take over. The status of this check is similar to a grace period (<code>initialDelaySeconds</code> settings in kubernetes for the relevant probes) and they can be helpful when an application is slow to start. It's especially helpful when there is any variability in startup times. Specifically in kubernetes, startup probes interact with liveness and readiness probes to prevent them from prematurely restarting or recreating application instances before they have fully started. Again, the <code>stop_service_if_failing</code> function runs at startup and can intuitively be compared to a startup probe. Additionally, startup probes are somewhat similar to grace periods, so it's tempting to directly compare them with the ASG's <code>health check grace period</code>. However, startup probes have additional application state awareness to prevent other probes from functioning until the application has started.</p> <p>Again, the BFD systems are not hosted with kubernetes, and outside of kubernetes the comparisons are challenging to make. Still, the concepts help direct the conversation. The table below summarizes the comparison of kubernetes-style probes to the existing bfd-server health checking strategy.</p>    bfd-server Component Type Implementation State     EC2 Instance startup <code>stop_service_if_failing</code> function partial   Classic Load Balancer liveness TCP Ping on Instance port 7443 complete   EC2 Status Checks liveness AWS Default complete   Auto Scaling Group startup 430s health check grace period partial","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html#proposed-solution","title":"Proposed Solution","text":"<p>The bfd-server needs a health check that goes beyond mere liveness. It needs to reliably identify each application instance as fully started only after the instance is fully provisioned and prepared to process requests. In short, bfd-server needs a real startup check. The existing <code>stop_service_if_failing</code> function already meets many of the requirements for such a check. It just needs the ability to signal the instance's startup state to the CLB. This signaling can be achieved through the introduction of a single, temporary firewall rule that initially disallows external traffic to the instance's service port, 7443, and subsequently removes the rule once all of the existing health criteria have been satisfied.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html#visualize-the-proposal","title":"Visualize the Proposal","text":"","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html#existing-health-checks-and-startup","title":"Existing Health Checks and Startup","text":"<p>Below is a simplified depiction of the existing health checks in the bfd-server. The instance is regarded as healthy as soon as port 7443 is available, but that can be before the service has fully started.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html#proposed-health-checks-and-readiness","title":"Proposed Health Checks and Readiness","text":"<p>Below depicts the proposed health checks in the bfd-server. The instance is continues to be regarded as healthy as soon as port 7443 is available. This solution leverages that simplicity and delays the availability of 7443 to external traffic until the internal startup health check succeeds. In other words, traffic is only permitted when the internal health check is satisfied.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html#proposed-solution-detailed-design","title":"Proposed Solution: Detailed Design","text":"<p>Extending the <code>stop_service_if_failing</code> function to fully satisfy the requirements of a startup check requires the following: 1. Augment the bfd-server service account with sudoers access, restricted to <code>/sbin/iptables</code> via ansible definition 2. Adjust the templated <code>bfd-server.sh.j2</code> file with the following changes:     - At the beginning of the <code>stop_service_if_failing</code> function, disallow external traffic on port 7443:</p> <pre><code>    ```bash\n    sudo iptables -A INPUT -p tcp -i eth0 --dport 7443 -j REJECT\n    ```\n\n- Where the script currently produces the log message \"Server started properly\", allow external traffic on port 7443:\n\n    ```bash\n    sudo iptables -D INPUT -p tcp -i eth0 --dport 7443 -j REJECT\n    ```\n</code></pre>","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html#proposed-solution-unresolved-questions","title":"Proposed Solution: Unresolved Questions","text":"<ul> <li> Arguments for and against certain solutions hinge in part on the notion that lazily loaded application data sources could have a negative impact on the performance. However, the evidence is unclear. Do the initial requests executed in the on-host health check ward off a cold-start or similar?   The arguments against alternative solutions may be a little weaker upon further investigation. The institutional memory surrounding the original implementation of the <code>stop_service_if_failing</code> suggests that it was not necessarily intended to avoid cold-start issues. If it does avoid these issues, it might be considered a happy accident.</li> <li> What is the recommended firewall on amazon Linux 2 in 2022? Is <code>iptables</code> still auspicious?   <code>iptables</code> continues to be an obvious, acceptable solution.</li> <li><code>ufw</code> can be used as a frontend for utilities like <code>iptables</code></li> <li><code>ufw</code> is available via epel which in turn is available via <code>amazon-linux-extras</code></li> <li><code>epel</code> does not appear to be enabled in the CMS base images by default</li> <li><code>iptables</code> appears to be installed in base CMS images</li> <li><code>iptables</code> is also part of <code>@amzn2-core</code> and easily added if missing</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html#proposed-solution-drawbacks","title":"Proposed Solution: Drawbacks","text":"<p>N/A.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html#proposed-solution-notable-alternatives","title":"Proposed Solution: Notable Alternatives","text":"","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html#health-check-alternatives-in-elastic-load-balancing","title":"Health Check Alternatives in Elastic Load Balancing","text":"<p>If it were only an issue of a lossy health check with what amounts to a TCP ping, AWS CLBs do support HTTP and HTTPS checks. While it might seem like a viable alternative, there are clear reasons to avoid this strategy.</p> <p>Ultimately, such a solution will rely on the existing <code>stop_service_if_failing</code> function or something like it. Because the bfd-server authentication strategy involves mutual TLS (mTLS) and the CLB health check mechanism cannot be supplied with a certificate for authentication, bfd-server would need to support endpoints that respond with an HTTP status code of 200 without authentication. This alone would require a non-trivial amount of work, but even if the bfd-server could respond this way, such endpoints would still rely on the functionality of  <code>stop_service_if_failing</code> or similar as the definitive startup source. This is because the checks involved are masking and/or compensating for some complexity in bfd-server. <code>stop_service_if_failing</code>, through the set of endpoints that it evaluates, eagerly loads otherwise lazily loaded application data sources. Without these first synthetic requests, the first real client requests may exhibit unacceptable performance. Additionally, these requests exercise the hot code paths involving the database which provides confidence that the first real client traffic will succeed in fetching the requested database resources.</p> <p>Much of the same applies to the seemingly obvious alternative: the Network Load Balancer (NLB). While NLBs could satisfy the load balancing requirements, including pass through for mTLS, the additional sophistication within Target Group health checking still falls short of providing holistic startup checks for this system, ultimately relying on host-based, scripted health checks. The Application Load Balancer on the other hand, would not only be similarly limited, but additionally unable to satisfy the mTLS requirements.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html#more-complex-solutions","title":"More Complex Solutions","text":"","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html#reverse-proxying-with-nginx-or-otherwise","title":"Reverse Proxying with Nginx or Otherwise","text":"<p>Dedicated reverse proxies like HAProxy or those implemented in popular web servers like Nginx are easy to configure and offer solutions to a number of problems including SSL/TLS offloading and termination; selective routing or advanced load balancing; and rate limiting to name a few. Health checks are first-class members in Nginx and could easily augment the existing CLB strategy. Unfortunately, many of the problems that something like Nginx might address already have successful solutions in place, and much of the additional functionality that a tool like Nginx could provide would go unused. In short, it's overkill when compared to the proposed solution, which satisfies the requirements and introduces little in the way of maintenance overhead. Like the other alternatives mentioned above, introducing Nginx or similar would only be partial solution, still relying on some other mechanism for identifying a successful startup.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html#split-control-plane","title":"Split Control Plane","text":"<p>Unlike other, similarly configured systems, bfd-server utilizes a largely immutable infrastructure pattern with little in the way of ongoing updates, automated or otherwise, post-deployment. When developers or operators wish to make changes to the system, the happy path involves rebuilding the system with the desired changes via deployment, rather than mutating the existing one. However, if a control plane should exist here, it might only consist of the automated Jenkins deployment pipeline, the application of terraform-defined infrastructure-as-code within it, and the suite of AWS API endpoints the pipeline uses. With this, the EC2 instances are created and automatically registered with the CLB. This is easy to maintain and simple to reason about. However, it's possible to forego the automatic EC2 instance registration with the CLB via the ASG and instead leave the registration up to scripted workflows on the instances themselves. In this way, the instances could register with the appropriate services only after they have more fully started, which satisfies the spirit of the proposed startup check. While this strategy would work, it's hard to imagine that increasing the individual EC2 instances privileges to directly interact with the hosting infrastructure is wise. What's more, splitting the roles and responsibilities could make the system more difficult to fully understand.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html#prior-art","title":"Prior Art","text":"<p>N/A</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html#future-possibilities","title":"Future Possibilities","text":"","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html#deployments","title":"Deployments","text":"<p>Improvements to the existing on-host health check to make it a more specific startup check and will inspire confidence in the existing deployment process. What's more, a reliable startup check is a prerequisite to the development of alternative deployment strategies, such as blue-green deployments and feature toggle deployments. The adoption of these or other deployment strategies may dramatically decrease deployment risk and toil while further increasing deployment confidence, reliability, and frequency.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html#readiness-checking","title":"Readiness Checking","text":"<p>What's missing from the proposal is the introduction of reliable readiness checks. This will require further consideration moving forward to ensure that the traffic is handled most effectively and that instances aren't suffering from periods of saturation as a result of mismanaged load balancing.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html#devsecops","title":"DevSecOps","text":"<p>This proposal is very simplistic and the initial feedback to this proposal included concern over the targeted privilege escalation for <code>iptables</code>, which could be avoided by re-imagining aspects of the startup script as cooperative systemd units, discretely responsible for health checking and port management. There could be other benefits from such a solution, however, they may be considered out of scope for this specific proposal.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0013-bfd-server-startup-check.html#addendums","title":"Addendums","text":"<p>The following addendums are required reading before voting on this proposal:</p> <ul> <li>(none at this time)</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html","title":"RFC Proposal","text":"<ul> <li>RFC Proposal ID: <code>0015-centralized-configuration-management</code></li> <li>Start Date: 2022-07-05</li> <li>RFC PR: beneficiary-fhir-data/rfcs#0015</li> <li>JIRA Ticket(s):<ul> <li>BFD-1641</li> </ul> </li> </ul> <p>This RFC proposes BFD's adoption of: - AWS Systems Manager (SSM) Parameter Store as the primary method for managing environment and system-specific configuration - A set of conventions to dramatically improve the strategy for storing and supplying configuration to BFD services and supporting systems</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#status","title":"Status","text":"<ul> <li>Status: Approved</li> <li>Implementation JIRA Ticket(s):<ul> <li>BFD-1639</li> </ul> </li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>RFC Proposal</li> <li>Status</li> <li>Table of Contents</li> <li>Motivation</li> <li>Proposed Solution<ul> <li>Parameter Formatting</li> <li>Project Scope</li> <li>Conventions</li> <li>Environment Scope</li> <li>Conventions</li> <li>Examples</li> <li>Group Scope</li> <li>Conventions</li> <li>Examples</li> <li>Sensitivity Scope and the Leaf Node</li> <li>Conventions</li> <li>Examples</li> <li>Proposed Solution: Detailed Design</li> <li>Base Environment Definitions in Terraform</li> <li>Access Controls<ul> <li>AWS SSM Access</li> <li>KMS Access</li> <li>Terraform tfstate</li> </ul> </li> <li>Adoption and Parameter Precedence</li> <li>Proposed Solution: Unresolved Questions</li> <li>Proposed Solution: Drawbacks</li> <li>Proposed Solution: Notable Alternatives</li> <li>AWS SSM AppConfig</li> <li>HashiCorp Vault+Consul</li> </ul> </li> <li>Prior Art</li> <li>Future Possibilities<ul> <li>Ergonomic Encryption Mechanism</li> <li>Future Automation Opportunities</li> </ul> </li> <li>Addendums</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#motivation","title":"Motivation","text":"<p>Despite BFD's relatively modest infrastructure footprint, the existing configuration strategy is complex. How, where, and when configuration is used informs us of the tools and locations of supporting code that may need to be updated to accommodate a change. Supported use-cases like renaming or adding new configuration may require one or more updates to the following locations:</p> <ol> <li>packer definitions</li> <li>sensitive ansible variables protected with <code>ansible-vault</code></li> <li>nonsensitive ansible variables in plain-text</li> <li>ansible build playbooks</li> <li>ansible launch playbooks</li> <li>ansible role variables</li> <li>ansible role definitions</li> <li>sensitive terraform input variables stored as tfvars files in <code>keybase</code></li> <li>nonsensitive terraform input variables passed between Jenkins job stages</li> <li>terraform templated cloud-init user data definitions</li> <li>terraform module definitions</li> <li>nonsensitive Jenkins job definitions stored in Jenkins</li> <li>nonsensitive Jenkins job definitions stored in various Jenkinsfiles</li> <li>sensitive Jenkins definitions stored with the Jenkins credential plugin</li> </ol> <p>The ability to supply configuration to each tool should be an enabling feature. Instead, the sprawling configuration has historically discouraged us from adopting worthy changes, contributed to operators' inadvertent addition of errors, and represents an unfriendly introduction to new contributors who wish to make what would otherwise be simple updates to BFD configuration.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#proposed-solution","title":"Proposed Solution","text":"<p>AWS Systems Manager (SSM) Parameter Store is a hierarchical, key-value data store typically used for centrally managed sensitive and nonsensitive configuration data. In addition to the AWS Console, the Parameter Store is accessed through the Parameter Store API, and can be even more accessible through various tools, including: - the AWS CLI via <code>aws ssm</code> (single parameter and path lookups) sub-command - terraform resources and data sources (single parameter and path lookups) with the aws provider - ansible workflows through the <code>aws_ssm</code> lookup plugin</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#parameter-formatting","title":"Parameter Formatting","text":"<p>We take advantage of Parameter Store's hierarchical nature by formatting the hierarchies into 4-tuple prefixes, culminating in a named leaf node. Together, the 4-tuple prefix and leaf make up a single parameter: <code>/${project}/${env}/${group}/${sensitivity}/${leaf}</code>. Each member of the tuple represents a scoped level or tier providing increasingly specific context for BFD services and supporting systems. The scopes lend themselves well to crafting simple queries and tailored access policies that target the necessary hierarchies of configuration data only. The primary use cases for these parameters include infrastructure-as-code, automated-configuration-management, and release-management strategies in <code>terraform</code>, <code>ansible</code>, and <code>jenkins</code> tooling, respectively. This proposed format does not apply to other potential Parameter Store use-cases.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#project-scope","title":"Project Scope","text":"<p>Project scope (<code>${project}</code>) is the parameter name root. In these hierarchies, this is practically an invariant that resolves to our project: <code>bfd</code>. Nevertheless, this is necessary to separate bfd-specific resources from other potential hierarchies managed by external teams supporting wider CMS projects.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#conventions","title":"Conventions","text":"<ul> <li>No leaves are allowed within this scope, e.g. <code>/bfd/foo</code> is illegal</li> <li>Must resolve to <code>bfd</code></li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#environment-scope","title":"Environment Scope","text":"<p>Environment scope (<code>${env}</code>) includes typical Software Development Life Cycle (SDLC) Environments like production (<code>prod</code>), those used along the path-to-production like <code>test</code> and <code>prod-sbx</code>, as well as higher order environments that fall outside of the typical BFD release process, i.e. <code>mgmt</code>. This fundamental scope separates production hierarchies from non-production ones and facilitates simple, least-privilege role-based access controls for developers and services.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#conventions_1","title":"Conventions","text":"<ul> <li>No leaf nodes are allowed within this scope, e.g. <code>/bfd/prod/foo</code> is illegal</li> <li>Must be one of <code>mgmt</code>, <code>prod</code>, <code>prod-sbx</code>, or <code>test</code></li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#examples","title":"Examples","text":"<ul> <li><code>/bfd/mgmt/</code></li> <li><code>/bfd/prod/</code></li> <li><code>/bfd/prod-sbx/</code></li> <li><code>/bfd/test/</code></li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#group-scope","title":"Group Scope","text":"<p>Group scope (<code>${group}</code>) largely provides the service-level configuration. The more inclusive group concept is used instead of service because hierarchies may include scopes that don't map to any one service.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#conventions_2","title":"Conventions","text":"<ul> <li>No leaf nodes are allowed within this scope, e.g. <code>/bfd/test/common/foo</code> is illegal</li> <li>Must be one of <code>jenkins</code>, <code>common</code>, <code>migrator</code>, <code>pipeline</code>, or <code>server</code></li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#examples_1","title":"Examples","text":"<ul> <li><code>/bfd/mgmt/jenkins/</code></li> <li><code>/bfd/prod-sbx/migrator/</code></li> <li><code>/bfd/prod/server/</code></li> <li><code>/bfd/test/common/</code></li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#sensitivity-scope-and-the-leaf-node","title":"Sensitivity Scope and the Leaf Node","text":"<p>Sensitivity scope (<code>${sensitivity}</code>) defines whether a leaf node (<code>${leaf}</code>) in the hierarchy is stored as an ordinary <code>String</code> or encrypted with AWS KMS as a <code>SecureString</code>. Explicit inclusion of this scope keeps secrets handling and access obvious and simple.</p> <p>The leaf node is the final member of the hierarchy and maps to the configuration value.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#conventions_3","title":"Conventions","text":"<ul> <li><code>${leaf}</code> must be unique per hierarchy, formatted as lower_snake_case, and generally be translatable/consumable as posix shell variable</li> <li>Sensitive parameters must be stored under a <code>sensitive</code> hierarchy in order to be encrypted as appropriate</li> <li>Nonsensitive parameters must be stored under the <code>nonsensitive</code> hierarchy to remain in plain-text</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#examples_2","title":"Examples","text":"<ul> <li>Sensitive Parameters</li> <li><code>/bfd/mgmt/jenkins/sensitive/bfd_aws_account_id</code></li> <li><code>/bfd/test/common/sensitive/rds_master_password</code></li> <li>Nonsensitive Parameters</li> <li><code>/bfd/test/common/nonsensitive/rds_cluster_id</code></li> <li><code>/bfd/prod-sbx/migrator/nonsensitive/instance_type</code></li> <li><code>/bfd/test/pipeline/nonsensitive/rda_job_enabled</code></li> <li><code>/bfd/prod/server/nonsensitive/new_relic_app_name</code></li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#proposed-solution-detailed-design","title":"Proposed Solution: Detailed Design","text":"","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#base-environment-definitions-in-terraform","title":"Base Environment Definitions in Terraform","text":"<p>The SDLC environment hierarchies are managed by a terraform module responsible for the base definition of each environment Environments that fall outside of the typical BFD SDLC process are managed similarly, but because the configuration in e.g. the <code>mgmt</code> environment has a wider, more global impact, values defined there may be consumed here to inform the deployment process Necessarily, they're managed externally to this process.</p> <p>The parameters are organized in two different, ultimately yaml-encoded files. Similar to the existing <code>ansible-vault</code> strategy, each hierarchy has encrypted and plaint-text variants for sensitive and nonsensitive values, respectively. Each file is named for its environment under a <code>./values</code> directory. The parameters are encoded as key:value mappings of fully-qualified SSM Parameter Name keys to Parameter values.</p> <pre><code># from notional ./values/test.yaml\n---\n/bfd/test/common/nonsensitive/env_name_std: test\n# [..snipped..]\n</code></pre> <p>The terraform-workspace enabled module derives the environment name from the workspace. For the <code>test</code> environment and <code>test</code> workspace, terraform finds <code>.values/test.eyaml</code> and <code>.values/test.yaml</code> for sensitive and nonsensitive values, respectively.</p> <p>For nonsensitive values exclusively stored in <code>.values/test.yaml</code>, terraform decodes the yaml and produces SSM Parameters.</p> <pre><code># from notional ./main.tf\n# [..snipped..]\n\nlocals {\n  env          = terraform.workspace\n  nonsensitive = yamldecode(file(\"${path.module}/values/${local.env}.yaml\")\n# [..snipped..]\n}\n\nresource \"aws_ssm_parameter\" \"nonsensitive\" {\n  for_each = local.nonsensitive\n  name     = each.key\n  type     = \"String\"\n  value    = each.value\n}\n# [..snipped..]\n</code></pre> <p>For sensitive values exclusively stored in <code>.values/test.eyaml</code>, terraform relies on a script stored in <code>./scripts/read-and-decrypt-eyaml.sh</code> for in-process decryption. In turn, this script is used by an External Provider data source <code>data.external.sensitive</code>.</p> <p>Of course, the implementation details of <code>read-and-decrypt-eyaml.sh</code> depend on the encryption mechanism. For now, a decryption script that uses the existing <code>ansible-vault</code> strategy is not complicated, but it's not immediately obvious, either. The <code>read-and-decrypt-eyaml.sh</code> can be broken down into two steps:</p> <ol> <li>decrypt using <code>ansible-vault decrypt --output -</code> to pass data process-to-process</li> <li>format the yaml as json for the external provider by piping data to <code>yq eval -o=j</code></li> </ol> <p>Terraform iterates over this well-formatted sensitive data as <code>data.external.sensitive.result</code> and creates the appropriate SSM Parameters.</p> <pre><code># from notional ./main.tf\n# [..snipped..]\n\ndata \"external\" \"sensitive\" {\n  program = [\"${path.module}/scripts/read-and-decrypt-eyaml.sh, \"${localenv}.eyaml\"]\n}\n\nresource \"aws_ssm_parameter\" \"sensitive\" {\n  for_each = data.external.sensitive.result\n  type     = \"SecureString\"\n  name     = each.key\n  value    = each.value\n}\n</code></pre> <p>The process for introducing or updating values is mainly CI-driven, with Jenkins supporting a new stage responsible for defining or redefining the base environment. The nonsensitive value updates are intuitive, requiring adjustments to the values in-place under plain-text yaml files. While <code>ansible-vault</code> remains the encryption mechanism, updating encrypted values remains a familiar process of fetching the encryption password, decrypting via e.g. <code>ansible edit</code>, and making necessary updates.</p> <p>Though a CI process is involved, the terraform module cooperates with some temporary, exceptional use-cases, such as those involving extra-terraform or manual, out-of-band changes. For nonsensitive values, this module supports temporary overrides and additional configuration. Further, this uses an <code>aws.ssm_parameters_by_path</code> data source to query nonsensitive hierarchies for the given environment. In so doing, it captures the state of the all parameters in the hierarchy for visibility. Together, the queried SSM hierarchies, environmental yaml, and any locally-defined overrides are merged to create a simple low, medium, high precedence, respectively.</p> <p>The happy path is depicted in the following sequence diagram:</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#access-controls","title":"Access Controls","text":"<p>This solution manages both sensitive and nonsensitive values. This has a few areas concerning access control: 1. AWS SSM Access   - General __write_ access to AWS SSM Parameter Store   - Read access to sensitive values under <code>arn:aws:ssm:us-east-1:${bfd_account_id}:parameter/bfd/*/*/sensitive/*</code> 2. AWS Key Management Service (KMS) Access   - policies for encryption and parameter store write access   - policies for decryption and parameter store read access 3. AWS S3 Access   - Specific access to terraform tfstate file under <code>s3://bfd-tf-state/env:/*/services/base/</code></p>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#aws-ssm-access","title":"AWS SSM Access","text":"<p>Services in each environment are able to read from the nonsensitive common hierarchy as well as their respective group hierarchy.</p>    ssm policy access resources usage     <code>bfd-${env}-migrator-ssm-read</code> read <code>/bfd/${env}/common/nonsensitive/*</code>, <code>/bfd/${env}/migrator/*</code> migrator role   <code>bfd-${env}-pipeline-ssm-read</code> read <code>/bfd/${env}/common/nonsensitive/*</code>, <code>/bfd/${env}/pipeline/*</code> pipeline role   <code>bfd-${env}-server-ssm-read</code> read <code>/bfd/${env}/common/nonsensitive/*</code>, <code>/bfd/${env}/server/*</code> server role    <p>Jenkins and BFD Engineers have wider, cross environment access in addition to write permissions:</p>    ssm policy access resources usage     <code>bfd-mgmt-bfd-ssm-read</code> read <code>/bfd/*</code> jenkins, engineers   <code>bfd-mgmt-bfd-ssm-write</code> write <code>/bfd/*</code> jenkins, on-call engineers","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#kms-access","title":"KMS Access","text":"<p>Re-iterating the above, each parameter including a sensitive value, e.g. <code>/bfd/*/*/sensitive/*</code>, is stored as a <code>SecureString</code>. These values are encrypted by the appropriate Customer Managed Key. As result, personas regularly reading or writing sensitive hierarchies also require decrypt and encrypt permissions to each key at issue.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#terraform-tfstate","title":"Terraform tfstate","text":"<p>Access not just to this tfstate file, used by the base environment definition, but any tfstate file that uses a sensitive value in pursuing infrastructure-level changes needs to be restricted to personas executing terraform.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#adoption-and-parameter-precedence","title":"Adoption and Parameter Precedence","text":"<p>So long as the configuration strategy for a given group or service is consistent across environments, even partial adoption of AWS SSM Parameter Store provides value. However, to reduce confusion and potential complexity, once we've started the process of transitioning, we endeavor to complete the transition as quickly as feasible.</p> <p>For this solution's adoption and implementation to be most obvious, we must have a clear stance on hierarchical configuration data and precedence at the tool level. This is especially important for <code>ansible</code> with its support for &gt;20 levels of variable precedence as well as <code>terraform</code> where we have an number of required input variables. The desired end state is thus: automation tools and their respective configuration data are primarily informed by values derived from AWS SSM Parameter Store. This entails that this configuration strategy will have the following characteristics when fully implemented: - <code>ansible</code>: variables files defined as defaults in roles or variable files in playbooks no longer exist - <code>terraform</code>: required, non-null terraform variables are used only by exception</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#proposed-solution-unresolved-questions","title":"Proposed Solution: Unresolved Questions","text":"<p>N/A</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#proposed-solution-drawbacks","title":"Proposed Solution: Drawbacks","text":"<ul> <li>This solution continues to rely on <code>ansible-vault</code> as the encryption mechanism, which has known limitations.</li> </ul>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#proposed-solution-notable-alternatives","title":"Proposed Solution: Notable Alternatives","text":"","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#aws-ssm-appconfig","title":"AWS SSM AppConfig","text":"<p>AWS Systems Manager AppConfig could be the next logical step in managing configuration, but adoption would mean less augmentation of the existing system and more replacement.</p> <p>Unlike Parameter Store where resources are single, configuration values stored as parameters, AppConfig's approach is more holistic with awareness and storage of whole configurations for applications across SDLC environments. It provides more sophistication surrounding validating application configuration and facilitates deployments when configurations are updated. AppConfig is an especially helpful service when configuration is highly dynamic with liberal use of feature flags. However, unlike Parameter Store, adopting AppConfig is a deeper investment into AWS Systems Manager and may require changes at the application-level to fully enable AppConfig.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#hashicorp-vaultconsul","title":"HashiCorp Vault+Consul","text":"<p>Vault by Hashicorp, is a non-starter due to the cost associated with the solution, be it self-managed or third-party management.</p> <p>Along with Consul as part of the reference Architecture, a Vault+Consul strategy has a number of overlapping features with the proposed strategy. Because it supports a variety of authentication methods and secrets engines, the combination can be fashioned to be a near drop-in replacement for AWS SSM Parameter Store. However, unlike AWS SSM Parameter Store, this requires additional managed infrastructure or cost for third-party management of that infrastructure. The effort involved in self-managing a highly-available, secure system like this may rival the complexity in managing the existing BFD systems. Beyond the maintenance, the infrastructure footprint would require at least three EC2 instances for Vault and at least three nodes for Consul. While hourly billing for a production-worthy, managed HashiCorp Cloud Platform offering, pricing starts at $1.578/hr. In both cases, unlike usage-based pricing in AWS SSM Parameter Store, we'd be paying for the infrastructure or the management of the infrastructure, regardless of utilization.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#prior-art","title":"Prior Art","text":"<p>N/A</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#future-possibilities","title":"Future Possibilities","text":"","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#ergonomic-encryption-mechanism","title":"Ergonomic Encryption Mechanism","text":"<p>We might consider adopting something like <code>hiera-eyaml</code> and <code>hiera-eyaml-kms</code> for encrypting and decrypting configuration values.</p> <p>This solution initially proposes the continued use of the existing <code>ansible-vault</code> for securing values in VCS. This isn't very ergonomic and it continues to represent a maintenance burden for the team in a few different ways: 1. values are encrypted at the file level resulting in less meaningful change set diffs 2. encrypt and decrypt functions requires a password in plain-text 3. password requires rotation after certain personnel changes 4. generally, access control for an encryption password is problematic   - it's difficult to answer who has access to the password   - it's impossible to know when files have been decrypted with the password   - onboarding engineers requires multiple steps</p> <p>We turn to the wider industry in configuration management to search for similar tools that might solve the challenges that motivated this RFC. In doing so, we re-discover that Puppet does. Through its explicit separation of configuration logic (written in puppet) from its hierarchical configuration data or Hiera, Puppet strongly support code re-use.</p> <p>Hiera is a complex, Puppet-specific, pluggable software system that supports a variety of merge strategies for hiera data, helping operators apply relevant configuration hierarchies from the widest global contexts to the narrowest individual node-level contexts, and anything in between. Historically, the separation of hiera data from puppet code meant physical separation from the puppet code base, often outside of VCS all together. When savvier operators used VCS (or delivery teams adopted DevOps), they were forced to make a choice: accept sensitive data in their repository, support an alternative strategy for incorporating sensitive values out-of-band, or adopt a hiera backend like <code>hiera-gpg</code> for encryption and decryption.</p> <p>The <code>hiera-gpg</code> strategy moved operations in a positive direction, allowing teams to secure sensitive data with GPG keys. However, It's not without its own challenges, some of which are the same that face BFD today with our usage of <code>ansible-vault</code>. For those using puppet, things have gotten better with <code>hiera-eyaml</code>.</p> <p><code>hiera-eyaml</code> is a pluggable, software system that extends Hiera. By default, it supports encryption using general PKCS7 certificates. While it's ostensibly supported by the VoxPupuli Puppet Community, it's the de facto standard for handling sensitive hiera data with Puppet. It enjoys some degree of first-party support from Puppet with both VoxPupuli and <code>hiera-eyaml</code> appearing in Puppet's official documentation. For our use, <code>hiera-eyaml</code> provides a general-purpose command line interface utility that supports field-level encryption of yaml files, which makes commit diffs much more meaningful. What's more, the <code>hiera-eyaml-kms</code> plugin allows operators to use keys stored in AWS KMS which have role based access control provided by Key and IAM policies.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#future-automation-opportunities","title":"Future Automation Opportunities","text":"<p>We could consider adopting more AWS Systems Manager services to all but eliminate the lengthy deployments that only consist of configuration changes.</p> <p>Today, automation in BFD infrastructure is informed by an immutable pattern, which generally requires deploying new artifacts for even the most basic configuration changes. (It's important to note that a critical aspect of this strategy isn't as immutable as advertised: logic and configuration values in ansible are not part of the artifact)</p> <p>This pattern has been fairly successful. By effectively providing a single, happy path for change management along the path-to-production, it's simple to answer the question: \"How do we get that change in?\" You just rebuild and redeploy everything. The resources used in pursuing this strategy are quite modest, too. It's really just a single, build-deploy pipeline in Jenkins that does all the things. We're not supporting long-living instances that need to abide by patching cadence. No ansible tower or similar with this strategy, and the maintenance thereof. BFD's orchestration needs are few.</p> <p>However, for exceptional use-cases, especially during incident response, this can be a frustrating process. Again, this would have operators rebuild and redeploy everything, even for a single line, configuration change. The appeal of out-of-band (OOB) changes is undeniable under these circumstances. While this process doesn't make those exceptional OOB changes any harder, it doesn't make them any easier, either. - reconstructing the time line for incident response when OOB changes are involved can be difficult - communicating while engaged in high-touch OOB change can be distracting and lead to errors - operators need to ensure they avoid other automation that might undo their change - sometimes, we forget to encode changes made this way into the enduring automation - OOB changes can temporary, but may persist well after the incident is considered resolved</p> <p>With AWS SSM Parameter Store engaged, we may be able to optimize for the OOB updates and make them more attractive to be done in a more observable, automated way. Using something like AWS SSM Documents, AWS SSM Run Command, and signaling via AWS EventBridge, we could craft automated routines to run on changes to specific parameters in AWS SSM Parameter Store. Another area of investigation would be incorporation of AWS SSM AppConfig, potentially a logical next step in configuration management for BFD.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/0015-centralized-configuration-management.html#addendums","title":"Addendums","text":"<p>The following addendums are required reading before voting on this proposal:</p> <ul> <li>AWS Naming and Tagging Conventions</li> </ul> <p>Please note that some of these addendums may be encrypted. If you are unable to decrypt the files, you are not authorized to vote on this proposal.</p>","tags":["RFC's"]},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html","title":"RFC Proposal","text":"<ul> <li>RFC Proposal ID: <code>0000-coverage-fhir-r4-carin</code></li> <li>Start Date: TBD</li> <li>RFC PR: TBD</li> <li>JIRA Ticket(s):</li> <li>BFD-265</li> </ul> <p>This RFC proposes that our team migrate from BFD v1 STU3 Coverage FHIR resource to the FHIR release, R4 and implement CARIN Coverage profile.</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html#status","title":"Status","text":"<ul> <li>Status: Implemented</li> <li>Implementation JIRA Ticket(s):<ul> <li>BFD-344</li> </ul> </li> </ul>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>RFC Proposal</li> <li>Status</li> <li>Table of Contents</li> <li>Motivation</li> <li>Proposed Solution<ul> <li>Detailed Design</li> <li>Current State</li> <li>Future State</li> <li>Differential View</li> <li>Unresolved Questions</li> <li>Drawbacks</li> <li>Notable Alternatives</li> </ul> </li> <li>Prior Art</li> <li>Future Possibilities</li> <li>Addendums</li> </ul>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html#motivation","title":"Motivation","text":"<p>This change will make BFD's Coverage resource increase the system's conformance with FHIR R4 and industry alliance ecosystems like CARIN. This will also make integration and interoperability with all our peer partners seamless by enabling to provide FHIR-R4 validated and conformant Coverage resources (Beneficaries Coverage and Eligibility data).</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html#proposed-solution","title":"Proposed Solution","text":"<p>BFD proposes to provide a new v2 endpoint to support Coverage FHRI-R4 and CARIN conformant resource. </p> <p>https://localhost:1337/v2/fhir/Coverage?beneficiary=-19990000000001&amp;_format=json</p> <p>Coverage FHIR R4 (US-Core, Normative release): http://hl7.org/fhir/R4/Coverage.html</p> <p>CARIN Coverage Profile: http://build.fhir.org/ig/HL7/carin-bb/StructureDefinition-CARIN-BB-Coverage.html</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html#detailed-design","title":"Detailed Design","text":"<p>HAPI Version to be used: HAPI FHIR 4.1.0</p> <p>HAPI 4.1.0 Coverage Data Model class to be used: org.hl7.fhir.r4.model.Coverage</p> <p>POC Code snippet: </p> <pre><code>  // Slicing implementation for Coverage Class\n  coverageSlcing = new Coverage();\n\n  coverageSlcing.addClass_().setValue(\"Medicare\").getType().addCoding().setCode(\"subgroup\")\n      .setSystem(\"http://terminology.hl7.org/CodeSystem/coverage-class\").setDisplay(\"SubGroup\");\n  coverageSlcing.addClass_().setValue(\"Part C\").getType().addCoding().setCode(\"subplan\")\n      .setSystem(\"http://terminology.hl7.org/CodeSystem/coverage-class\").setDisplay(\"SubPlan\");\n</code></pre> <p>Below are the payload changes being proposed in Coverage resource:</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html#changes","title":"Changes:","text":""},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html#slicing","title":"Slicing:","text":"<p>Current State: No Slicing.</p> <p>Future State: The element Coverage.class is sliced based on the values of value:type.</p> <p>Slicing Implementation: POC code above. </p> <p>Slicing codes are used from the below list:</p> <p>http://terminology.hl7.org/CodeSystem/coverage-class version 4.0.1  </p> <p>Code \u2003  Definition subgroup: \u00a0 \u00a0    A sub-group of an employee group   subplan: \u00a0 \u00a0     Coverage Medicaid number    </p> <p>Slicing Rules: https://www.hl7.org/fhir/valueset-resource-slicing-rules.html</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html#extensionscode-system-changes","title":"Extensions/Code System Changes:","text":"<p>Coverage.status     \u2022   Change value set       From:         http://hl7.org/fhir/ValueSet/fm-status      To:          http://hl7.org/fhir/ValueSet/fm-status|4.0.1  </p> <p>Coverage.relationship     \u2022   http://terminology.hl7.org/CodeSystem/subscriber-relationship  </p> <p>Coverage.type     \u2022   http://hl7.org/fhir/valueset-coverage-type.html  </p> <p>Coverage.class.type     \u2022   http://terminology.hl7.org/CodeSystem/coverage-class  </p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html#coveragestatus-field","title":"Coverage.status field","text":"<p>Type boolean</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html#mandatory-new-fields-to-be-added","title":"MANDATORY New Fields to be Added:","text":"<p>Coverage.class.type  </p> <p>Type of class such as 'group' or 'plan' (CodeableConcept). E.g. Medicare.  </p> <p>Coverage.class.value  </p> <p>Value associated with the type (String). E.g \u201cPart D\u201d.  </p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html#additional-carin-mandatory-new-fields-to-be-added","title":"Additional CARIN MANDATORY New Fields to be Added:","text":"<p>Subscriber Id:</p> <p>ID assigned to the subscriber.</p> <p>Relationship:</p> <p>Beneficiary relationship to the subscriber. The relationship between the Subscriber and the Beneficiary. http://hl7.org/fhir/ValueSet/subscriber-relationship</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html#cardinality-rule-changes-from-stu3-to-r4","title":"Cardinality Rule Changes (From STU3 to R4)","text":"<p>Coverage.status</p> <p>Min Cardinality changed from 0 to 1</p> <p>Coverage.beneficiary</p> <p>Min Cardinality changed from 0 to 1</p> <p>Coverage.payor</p> <p>Min Cardinality changed from 0 to 1.</p> <p>Coverage.class</p> <p>Max Cardinality changed from 1 to *</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html#renamed-items","title":"Renamed Items:","text":"<p>Coverage.class (Renamed from grouping to class)</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html#deleted-elements","title":"Deleted Elements:","text":"<p>Coverage.grouping</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html#r4-code-systems-changes","title":"R4 Code Systems Changes:","text":"<p>Coverage.status     \u2022   Change value set from:         http://hl7.org/fhir/ValueSet/fm-status      To:         http://hl7.org/fhir/ValueSet/fm-status|4.0.1  </p> <p>Coverage.relationship     \u2022   http://terminology.hl7.org/CodeSystem/subscriber-relationship  </p> <p>Coverage.type     \u2022   http://hl7.org/fhir/valueset-coverage-type.html  </p> <p>Coverage.class.type     \u2022   http://terminology.hl7.org/CodeSystem/coverage-class  </p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html#r4-non-conformance-errors","title":"R4 Non-Conformance Errors:","text":"<p>FHIR-R4 Coverage Resource Validation errors:</p> <p>Next issue ERROR - null - cvc-complex-type.2.4.b: The content of element 'Coverage' is not complete. One of '{\"http://hl7.org/fhir\":id, \"http://hl7.org/fhir\":meta,  \"http://hl7.org/fhir\":implicitRules,  \"http://hl7.org/fhir\":language,  \"http://hl7.org/fhir\":text,  \"http://hl7.org/fhir\":contained,  \"http://hl7.org/fhir\":extension,  \"http://hl7.org/fhir\":modifierExtension,  \"http://hl7.org/fhir\":identifier,  \"http://hl7.org/fhir\":status}' is expected.  </p> <p>CARIN Validation errors:</p> <p>Next issue ERROR - Coverage - Unable to locate profile http://hl7.org/fhir/us/carin/StructureDefinition/carin-bb-coverage Next issue ERROR - Coverage - Unable to locate profile  http://hl7.org/fhir/us/carin/StructureDefinition/carin-bb-Coverage Next issue INFORMATION - Coverage.extension[0] - Unknown extension</p> <p>http://hl7.org/fhir/us/core/StructureDefinition/us-core-race</p> <p>Next issue INFORMATION - Coverage.extension[1] - Unknown extension http://hl7.org/fhir/us/core/StructureDefinition/us-core-ethnicity</p> <p>Next issue INFORMATION - Coverage.extension[2] - Unknown extension  http://hl7.org/fhir/us/core/StructureDefinition/us-core-birthsex</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html#carin-ig","title":"CARIN IG:","text":"<p>https://build.fhir.org/ig/HL7/carin-bb/StructureDefinition-CARIN-BB-Coverage.html  </p> <p>Summary Mandatory: 2 elements Must-Support: 1 element  </p> <p>Slices This structure defines the following Slices: The element Coverage.identifier is sliced based on the values of value:type, value:system</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html#current-state","title":"Current State","text":"<p>BFD v1: Current State Coverage Resource</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html#future-state","title":"Future State","text":"<p>BFD v2: Future State Coverage Resource</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html#differential-view","title":"Differential View","text":"<p>Differential_view_1</p> <p>Differential_view_2</p> <p>Differential_view_3</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html#unresolved-questions","title":"Unresolved Questions","text":"<p>The following questions need to be resolved prior to merging this RFC:</p> <ol> <li>Can raw un-hashed MBI be used a Subscriber Id? YES</li> </ol> <p>RESOLVED</p> <ol> <li>What code for Coverage \u201ctype\u201d can be used  from this Code System:</li> </ol> <p>http://terminology.hl7.org/CodeSystem/v3-ActCode</p> <p>A code from above code system below and if this can be used? YES</p> <p>SUBSIDIZ:   subsidized health program  </p> <p>Definition: A government health program that provides coverage for health services to persons meeting eligibility criteria such as income, location of residence, access to other coverages, health condition, and age, the cost of which is to some extent subsidized by public funds.</p> <p>RESOLVED </p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html#proposed-solution-drawbacks","title":"Proposed Solution: Drawbacks","text":"<p>Supporting both v1 and v2 in parallel may cause slight performance issues.</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html#proposed-solution-notable-alternatives","title":"Proposed Solution: Notable Alternatives","text":"<p>NA.</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html#prior-art","title":"Prior Art","text":"<p>BFD project already has a reasonable amount of experience with FHIR DSTU3/R3, those experiences have been very positive.</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html#future-possibilities","title":"Future Possibilities","text":"<p>No future possibilities are being seriously considered at this time.</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-coverage.html#addendums","title":"Addendums","text":"<p>The following addendums are required reading before voting on this proposal: None</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html","title":"RFC Proposal","text":"<ul> <li>RFC Proposal ID: <code>0001-bfd-v2-patient</code></li> <li>Start Date: 2020-08-03</li> <li>RFC PR: BFD-264-v2_Patient_RFC #313</li> <li>JIRA Ticket(s): BFD-264</li> </ul> <p>This RFC proposes that our team migrate from BFD v1 STU3 Patient FHIR resource to the first normative FHIR release, R4 and implement CARIN Patient profile.</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html#status","title":"Status","text":"<ul> <li>Status: Implemented</li> <li>Implementation JIRA Ticket(s):<ul> <li>BFD-293</li> </ul> </li> </ul>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>RFC Proposal</li> <li>Status</li> <li>Table of Contents</li> <li>Motivation</li> <li>Proposed Solution<ul> <li>Detailed Design</li> <li>Current State</li> <li>Future State</li> <li>Differential View</li> <li>Unresolved Questions</li> <li>Drawbacks</li> <li>Notable Alternatives</li> </ul> </li> <li>Prior Art</li> <li>Future Possibilities</li> <li>Addendums</li> </ul>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html#motivation","title":"Motivation","text":"<p>This change will make BFD's Patient FHIR resource increase the system's conformance with FHIR R4 and industry alliance like CARIN. This will help BFD move its platform to use the first normative release of FHIR R4. This will also make integration and  interoperability with all our peer partners seamless by enabling to provide FHIR R4 validated and conformant Patient resources (Beneficiaries demographic data).</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html#proposed-solution","title":"Proposed Solution","text":"<p>BFD proposes to provide a new v2 endpoint based on Patient FHRI-R4 and CARIN BB Profile. </p> <p>Sample indicative Patient v2 endpoint: https://localhost:1337/v2/fhir/Patient?_id=-19990000000001&amp;_format=json</p> <p>Patient FHIR R4 (US-Core, Normative release): http://hl7.org/fhir/R4/patient.html</p> <p>CARIN Patient Profile: http://build.fhir.org/ig/HL7/carin-bb/StructureDefinition-CARIN-BB-Patient.html</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html#detailed-design","title":"Detailed Design","text":"<p>HAPI Version to be used: HAPI FHIR 4.1.0</p> <p>HAPI 4.1.0 Patient Data Model class to be used: org.hl7.fhir.r4.model.Patient</p> <p>POC Code snippet: </p> <pre><code>// Slicing for Patient Identifier\n  Patient patientSlicing = new Patient();\n\n  patientSlicing.addIdentifier().setValue(\"-20140000010000\")\n      .setSystem(\"https://bluebutton.cms.gov/resources/variables/bene_id\").getType().addCoding()\n      .setCode(\n      \"PI\")\n      .setSystem(\"http://hl7.org/fhir/us/carin-bb/CodeSystem/IdentifierTypeCS\")\n      .setDisplay(\"Patient Internal Identifier\");\n\n  patientSlicing.addIdentifier()\n      .setValue(\"-2b034220943953861f7b17963091ea962c13548f4b1d5f4c1013ee1779d621f4\")\n      .setSystem(\"https://bluebutton.cms.gov/resources/identifier/mbi-hash\").getType()\n      .addCoding().setCode(\"MC\").setSystem(\"http://hl7.org/fhir/us/carin-bb/CodeSystem/IdentifierTypeCS\")\n      .setDisplay(\"Patient's Medicare Number\");\n</code></pre> <p>Below are the payload changes being proposed in Patient resource:</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html#changes","title":"Changes:","text":""},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html#slicing","title":"Slicing:","text":"<p>Current State: No Slicing.</p> <p>Future State: The element Patient.identifier is sliced based on the values of value:type, value:system.</p> <p>Slicing Implementation: POC code above. Slicing codes are used from the below list:</p>    Code Display     MR Medical record number   MA Patient Medicaid number   PI Patient internal identifier   PT Patient external identifier   SN Subscriber Number"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html#extensionscode-system-changes","title":"Extensions/Code System Changes:","text":"<p>Race:</p> <p>CMS/CCW Race Codes will continue to be provided. In addition US-Core OMB Race extension will also be provided. CCW Race codes will be mapped as UNK (Unknown) Race code for ALL races to support US-Core Race Code System (this may undergo change based on FINAL decision). CCW Race codes will continue to be provided to accurately reflect Race in the system.</p> <p>CMS BFD Code System URL:</p> <p>https://bluebutton.cms.gov/resources/variables/race/</p> <p>US Core Race Code Systems URL:</p> <p>http://hl7.org/fhir/us/core/StructureDefinition/us-core-race</p> <p>CCW Race Codes:</p>    Code Code value     0 UNKNOWN   1 NON-HISPANIC WHITE   2 BLACK (OR AFRICAN-AMERICAN)   3 OTHER   4 ASIAN/PACIFIC ISLANDER   5 HISPANIC   6 AMERICAN INDIAN / ALASKA NATIVE"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html#patientactive-field","title":"Patient.Active field","text":"<p>Type boolean</p> <p>If the patient is not deceased and has any coverage mark them active, else mark them false. We will use Death_Dt to populate this field. If deceased date is not present set Active flag to true else false.</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html#mandatory-new-fields-to-be-added","title":"MANDATORY New Fields to be Added:","text":"<p>None as part of FHIR-R4 Spec. But CARIN mandates some mandatory Patient fields:  </p> <p>\u2022   Identifier \u2022   Name \u2022   Gender  </p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html#cardinality-rule-changes-from-stu3-to-r4","title":"Cardinality Rule Changes (From STU3 to R4)","text":"<p>NONE.</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html#renamed-items","title":"Renamed Items:","text":"<p>NONE.</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html#deleted-elements","title":"Deleted Elements:","text":"<p>N/A (Patient.Animal)</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html#r4-code-systems-changes","title":"R4 Code Systems Changes:","text":"<p>Patient.identifier:</p> <p>\u2022   https://bluebutton.cms.gov/resources/variables/bene_id/ \u2022   http://hl7.org/fhir/us/carin/ValueSet/carin-bb-identifier-type \u2022   http://terminology.hl7.org/CodeSystem/v2-0203  </p> <p>Patient.gender:</p> <pre><code>* Change value set from http://hl7.org/fhir/ValueSet/administrative-gender to http://hl7.org/fhir/ValueSet/administrative-gender|4.0.1\n</code></pre>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html#not-supported","title":"Not Supported:","text":"<p>\u2022   Telecom field - we don\u2019t send, not in CCW. \u2022   Email field \u2013 we don\u2019t send, not in CCW. \u2022   Ethnicity - we don\u2019t send, not in CCW. \u2022   Communication/Language - we don\u2019t send, not in CCW. \u2022   Contact/Next of Kin - we don\u2019t send, not in CCW.  </p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html#r4-non-conformance-errors","title":"R4 Non-Conformance Errors:","text":"<p>Next issue ERROR - Patient - Unable to locate profile  http://hl7.org/fhir/us/carin/StructureDefinition/carin-bb-patient Next issue INFORMATION - Patient.extension[0] - Unknown extension</p> <p>http://hl7.org/fhir/us/core/StructureDefinition/us-core-race</p> <p>Next issue INFORMATION - Patient.extension[1] - Unknown extension http://hl7.org/fhir/us/core/StructureDefinition/us-core-ethnicity</p> <p>Next issue INFORMATION - Patient.extension[2] - Unknown extension http://hl7.org/fhir/us/core/StructureDefinition/us-core-birthsex</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html#carin-ig","title":"CARIN IG:","text":"<p>https://build.fhir.org/ig/HL7/carin-bb/StructureDefinition-CARIN-BB-Patient.html</p> <p>Summary: \u2022   Mandatory: 2 elements \u2022   Must-Support: 1 element  </p> <p>Slices: This structure defines the following Slices: \u2022   The element Patient.identifier is sliced based on the values of value:type, value:system</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html#current-state","title":"Current State","text":"<p>BFD v1: Current State Patient Resource</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html#future-state","title":"Future State","text":"<p>BFD v2: Future State Patient Resource</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html#differential-view","title":"Differential View","text":"<p>Differential_view</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html#unresolved-questions","title":"Unresolved Questions","text":"<p>The following questions need to be resolved prior to merging this RFC:</p> <ol> <li>Hash Identifiers in Patient resource:</li> </ol> <p>Do we need to return hash identifiers? Use the identifier flag in the header to determine what to send \u2013 MBI. HICN, Hashes? We do send out BENE_ID as local patient (beneficiary) identifier (medical record number).</p> <p>RESOLVED: Hashed MBI\u2019s will not be provided.</p> <ol> <li>Race code mapping for Hispanic:</li> </ol> <p>What is the best approach in CMS-Blue Button to map Hispanic race to US-Core Race code (see below):</p> <pre><code>Code            Display\n</code></pre> <p>\u2022   1002-5          American Indian or Alaska Native \u2022   2028-9          Asian \u2022   2054-5          Black or African American \u2022   2076-8          Native Hawaiian or Other Pacific Islander \u2022   2106-3          White \u2022   UNK         Unknown \u2022   ASKU            Asked but no answer  </p> <p>RESOLVED:  CCW Race codes will be mapped as UNK race code for ALL races to support US-Core race Code System. CCW/SSA Race codes will continue to be provided to accurately reflect race in the system.      </p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html#proposed-solution-drawbacks","title":"Proposed Solution: Drawbacks","text":"<p>Supporting both v1 and v2 in parallel may cause slight performance issues.</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html#proposed-solution-notable-alternatives","title":"Proposed Solution: Notable Alternatives","text":"<p>NA.</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html#prior-art","title":"Prior Art","text":"<p>BFD project already has a reasonable amount of experience with FHIR DSTU3/R3, those experiences have been very positive.</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html#future-possibilities","title":"Future Possibilities","text":"<p>No future possibilities are being seriously considered at this time.</p>"},{"location":"rfcs/implemented/BFDv2/0001-bfd-v2-patient.html#addendums","title":"Addendums","text":"<p>The following addendums are required reading before voting on this proposal: None</p>"},{"location":"rfcs/implemented/BFDv2/BFDv2%20RFC.html","title":"RFC Proposal","text":"<ul> <li>RFC Proposal ID: <code>0000-bfd-v2-fhir-r4-carin</code></li> <li>Start Date: TBD</li> <li>RFC PR: TBD</li> <li>JIRA Ticket(s):</li> <li>BFD-285</li> </ul> <p>This RFC proposes that our team migrate from BFD v1 platform currently on FHIR STU3 to BFD v2 platform supporting the first normative FHIR release, R4 and implement CARIN BB profiles.</p>"},{"location":"rfcs/implemented/BFDv2/BFDv2%20RFC.html#status","title":"Status","text":"<ul> <li>Status: Implemented</li> <li>Implementation JIRA Ticket(s):<ul> <li>BFD-907</li> </ul> </li> </ul>"},{"location":"rfcs/implemented/BFDv2/BFDv2%20RFC.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>RFC Proposal</li> <li>Status</li> <li>Table of Contents</li> <li>Motivation</li> <li>Proposed Solution<ul> <li>Detailed Design<ul> <li>HAPI Version</li> <li>HAPI Data Model</li> <li>Maven Dependency BFDv2</li> <li>FHIR R4 Server</li> <li>Spring Configuration</li> <li>BFDv2 Resource Providers</li> </ul> </li> <li>Current State</li> <li>Future State</li> <li>Differential View</li> <li>Unresolved Questions</li> <li>Drawbacks</li> <li>Notable Alternatives</li> </ul> </li> <li>Prior Art</li> <li>Future Possibilities</li> <li>Addendums</li> </ul>"},{"location":"rfcs/implemented/BFDv2/BFDv2%20RFC.html#motivation","title":"Motivation","text":"<p>BFDv2 will make BFD's FHIR resources increase the system's conformance with FHIR R4 and industry alliance like CARIN. This will help BFD move its platform to use the first normative release of FHIR (R4). This will also make integration and  interoperability with all our peer partners seamless by enabling to provide FHIR-R4 validated and conformant resources for Patient, Coverage and Explanation Of Benefits (EOB's).</p>"},{"location":"rfcs/implemented/BFDv2/BFDv2%20RFC.html#proposed-solution","title":"Proposed Solution","text":"<p>BFD proposes to provide a new v2 endpoint URL (future state) and will make no structural or architectural change to the v1 endpoint (current state). The goal is for partners to not have to make any changes in their existing integration points with BFD.</p> <p>BFD v2 WILL NOT be backward compatible with BFD v1 and Peer Parrtners will not be able to use their existing v1 connectors/interfaces/integration points for v2 consumption.</p>  <p>BFD v2 to take a middle approach and make best efforts to minimize redundancy:</p> <ul> <li>Identify Common components and cross cutting concerns which are independent of HAPI data models and which can be safely abstracted for both v1 and v2.</li> <li>Have version specific Utiliy components which are tied to HAPI data models (stu3 and r4).</li> <li>Have FHIR/HAPI version specific Resource providers and Transformers.</li> <li>V1 is not largely\" going to change. No new fields. Only production bug fixes and SLO adherence.</li> <li>v2 will be a work in progress based on business and industry standard needs.</li> <li>Explore and implement shared \"resource provider\" components between v1 and v2.</li> </ul> <p>BFD v2 endpoint will support the following FHIR resources:  </p> <ul> <li>Patient (BFD v2, FHIR r4)</li> <li>Coverage (BFD v2, FHIR r4)</li> <li>ExplanationOfBenefit (BFD v2, FHIR r4)</li> </ul> <p>An idicative v2 endpoint is below:  </p> <p>https://localhost:1337/v2/fhir/Patient?_id=-19990000000001&amp;_format=json</p> <p>Link to FHIR R4 and CARIN BB Profiles is below:  </p> <p>FHIR R4 (US-Core, Normative release): http://hl7.org/fhir/R4</p> <p>CARIN BB Profiles: http://build.fhir.org/ig/HL7/carin-bb</p>"},{"location":"rfcs/implemented/BFDv2/BFDv2%20RFC.html#detailed-design","title":"Detailed Design","text":""},{"location":"rfcs/implemented/BFDv2/BFDv2%20RFC.html#hapi-version","title":"HAPI Version","text":"<pre><code>HAPI FHIR 4.x\n</code></pre>"},{"location":"rfcs/implemented/BFDv2/BFDv2%20RFC.html#hapi-data-model","title":"HAPI Data Model","text":"<pre><code>    * org.hl7.fhir.r4.model.Patient\n    * org.hl7.fhir.r4.model.Coverage\n    * org.hl7.fhir.r4.model.ExplanationOfBenefit\n</code></pre>"},{"location":"rfcs/implemented/BFDv2/BFDv2%20RFC.html#maven-dependency-bfdv2","title":"Maven Dependency BFDv2","text":"<pre><code>    &lt;dependency&gt;\n        &lt;!-- At least one \"structures\" JAR must also be included --&gt;\n        &lt;groupId&gt;ca.uhn.hapi.fhir&lt;/groupId&gt;\n        &lt;artifactId&gt;hapi-fhir-structures-r4&lt;/artifactId&gt;\n        &lt;version&gt;${hapi-fhir.version}&lt;/version&gt;\n    &lt;/dependency&gt;\n</code></pre>"},{"location":"rfcs/implemented/BFDv2/BFDv2%20RFC.html#fhir-r4-server","title":"FHIR R4 Server","text":"<p>Constructs a new R4Server instance:</p> <pre><code>public R4Server() {\n    super(FhirContext.forR4());\n    setServerAddressStrategy(ApacheProxyAddressStrategy.forHttp());\n    configureServerInfoMetadata();\n}\n\nRegister the new HAPI R4 server with the Servlet Context to support v2 endpoints and add the mapping for v2 traffic:\n\nR4Server r4Servlet = new R4Server();\ncxfServletReg = servletContext.addServlet(\"r4Servlet\", r4Servlet);\ncxfServletReg.setLoadOnStartup(1);\ncxfServletReg.addMapping(\"/v2/fhir/*\");\n</code></pre>"},{"location":"rfcs/implemented/BFDv2/BFDv2%20RFC.html#spring-configuration","title":"Spring Configuration:","text":"<p>Defines a new Spring Bean in the Spring Configuration for the Spring IoC container to support v2 traffic to BFD.</p> <p>/*    * @param r4PatientResourceProvider the application's {@link R4PatientResourceProvider} bean    * @param r4CoverageResourceProvider the application's {@link R4CoverageResourceProvider} bean    * @param r4EobResourceProvider the application's {@link R4ExplanationOfBenefitResourceProvider}    *     bean    * @return the {@link List} of R4 {@link IResourceProvider} beans for the application    /   @Bean(name = BLUEBUTTON_R4_RESOURCE_PROVIDERS)   public List r4ResourceProviders(       R4PatientResourceProvider r4PatientResourceProvider,       R4CoverageResourceProvider r4CoverageResourceProvider,       R4ExplanationOfBenefitResourceProvider r4EobResourceProvider) {     List r4ResourceProviders = new ArrayList();     r4ResourceProviders.add(r4PatientResourceProvider);     r4ResourceProviders.add(r4CoverageResourceProvider);     r4ResourceProviders.add(r4EobResourceProvider);     return r4ResourceProviders;   } <p>Each IResourceProvider adds support for a specific FHIR resource.</p> <pre><code>List&lt;IResourceProvider&gt; resourceProviders =\n    springContext.getBean(SpringConfiguration.BLUEBUTTON_R4_RESOURCE_PROVIDERS, List.class);\nsetResourceProviders(resourceProviders);\n</code></pre>"},{"location":"rfcs/implemented/BFDv2/BFDv2%20RFC.html#bfdv2-resource-providers","title":"BFDv2 Resource Providers:","text":"<p>BFDv2 will have it own resource providers and supporting transformers to support FHIR R4 and CARIN conformance needs. It will use HAPI R4 Data Structures as mentioned above.</p> <pre><code>    gov.cms.bfd.server.war.r4.providers.R4PatientResourceProvider\n    gov.cms.bfd.server.war.r4.providers.R4CoverageResourceProvider\n    gov.cms.bfd.server.war.r4.providers.R4ExplanationOfBenefitResourceProvider\n</code></pre>"},{"location":"rfcs/implemented/BFDv2/BFDv2%20RFC.html#carin-ig","title":"CARIN IG","text":"<p>http://build.fhir.org/ig/HL7/carin-bb/</p> <p>Summary: Mandatory: 2 elements Must-Support: 1 element Slices: This structure defines the following Slices: The element Patient.identifier is sliced based on the values of value:type, value:system</p>"},{"location":"rfcs/implemented/BFDv2/BFDv2%20RFC.html#current-state","title":"Current State","text":"<p>Please see Patient v2 RFC for an indicative and sample CURRENT state resource.</p>"},{"location":"rfcs/implemented/BFDv2/BFDv2%20RFC.html#future-state","title":"Future State","text":"<p>Please see Patient v2 RFC for an indicative and sample FUTURE state resource .</p>"},{"location":"rfcs/implemented/BFDv2/BFDv2%20RFC.html#differential-view","title":"Differential View","text":"<p>Please see Patient v2 RFC for an indicative and sample differential Payload view between v1 and v2 resource.</p>"},{"location":"rfcs/implemented/BFDv2/BFDv2%20RFC.html#unresolved-questions","title":"Unresolved Questions","text":"<p>The following question needs to be resolved:</p> <pre><code>How long will v1 and v2 be supported in parallel?\n\n**RESOLVED:** Dec 31, 2021\n</code></pre>"},{"location":"rfcs/implemented/BFDv2/BFDv2%20RFC.html#proposed-solution-drawbacks","title":"Proposed Solution: Drawbacks","text":"<ol> <li>Supporting both v1 and v2 in parallel may cause slight performance issues.</li> </ol>"},{"location":"rfcs/implemented/BFDv2/BFDv2%20RFC.html#proposed-solution-notable-alternatives","title":"Proposed Solution: Notable Alternatives","text":"<p>NA.</p>"},{"location":"rfcs/implemented/BFDv2/BFDv2%20RFC.html#prior-art","title":"Prior Art","text":"<p>BFD project already has a reasonable amount of experience with FHIR DSTU3/R3 and CARIN engagement, those experiences have been very positive.</p>"},{"location":"rfcs/implemented/BFDv2/BFDv2%20RFC.html#future-possibilities","title":"Future Possibilities","text":"<p>No future possibilities are being seriously considered at this time.</p>"},{"location":"rfcs/implemented/BFDv2/BFDv2%20RFC.html#addendums","title":"Addendums","text":"<p>The following addendums are required reading before voting on this proposal: None</p>"},{"location":"rfcs/proposed/0010-custom-system-uris.html","title":"RFC Proposal","text":"<ul> <li>RFC Proposal ID: <code>0010-custom-system-uris</code></li> <li>Start Date: 2021-11-30</li> <li>RFC PR: beneficiary-fhir-data/rfcs#0010</li> <li>JIRA Ticket(s):<ul> <li>BFD-1459</li> </ul> </li> </ul>  <p>The question being considered by this RFC is this:   what base URI/URL should the BFD API use in its data payloads for custom/non-standard FHIR <code>system</code>s? This RFC makes the recommendation,   as explained and justified in later sections,   that BFD should stick with a base of   <code>https://bluebutton.cms.gov/resources/</code> for all custom URIs.</p> <p>Before we go further, a quick note on terminology:   this RFC talks a lot about   URIs and   URLs,   which are similar but nevertheless distinct concepts. Briefly, a URI is a \"magic string\" in a specific format that uniquely identifies something. Whereas a URL is also a \"magic string\" but one that web browsers and other software applications can use to retrieve something. Often, URLs start with \"http://\", indicating that a web browser can likely open it. In the vein of \"a square is a rectangle but not all rectangles are squares\",   a URL is a URI but not all URIs are URLs.</p> <p>BFD's data payloads include a number of custom URIs for FHIR   Coding.systems,   Identifier.systems,   and other such data types. These URIs are serve two functions: Firstly, they are \"magic strings\" that end user applications will use to select the pieces of our data payload that their applications care about. Second, they can also be treated as URLs that provide developer documentation, e.g. data field definitions. While many of the elements in our data payload can use industry-standard URIs for industry-standardized data,   a large percentage of our data elements/codings are custom to CMS and thus must use custom URIs.</p> <p>As an example, consider the following sample <code>Patient</code> resource payload from BFD:   https://github.com/CMSgov/beneficiary-fhir-data/blob/master/apps/bfd-server/bfd-server-war/src/test/resources/endpoint-responses/v2/patientRead.json. If an application wanted to select the <code>Patient.identifier</code> entry that had a beneficiary's MBI,   they would search for the entry with a <code>system</code> value of <code>http://hl7.org/fhir/sid/us-mbi</code>. HL7, a standards body, has said that <code>system</code> should be used for all MBIs,   in all FHIR-compliant APIs, and so BFD does not need a custom URI there. If, however, an application wants to select the <code>Patient.identifier</code> entry that contains a beneficiary's CCW beneficiary ID,   they would search for the entry with a <code>system</code> value of <code>https://bluebutton.cms.gov/resources/variables/bene_id</code>. The \"CCW beneficiary ID\" field is only used within/by CMS,   so no standards body has assigned it a standard URI,   and BFD must provide a custom URI for it.</p>","tags":["RFC's"]},{"location":"rfcs/proposed/0010-custom-system-uris.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>RFC Proposal</li> <li>Table of Contents</li> <li>Motivation</li> <li>Proposed Solution<ul> <li>Proposed Solution: Detailed Design</li> <li>Proposed Solution: Unresolved Questions</li> <li>Proposed Solution: Drawbacks</li> <li>Proposed Solution: Notable Alternatives</li> </ul> </li> <li>Prior Art</li> <li>Future Possibilities</li> <li>Addendums</li> </ul>","tags":["RFC's"]},{"location":"rfcs/proposed/0010-custom-system-uris.html#motivation","title":"Motivation","text":"<p>At the end of the day here,   our goal is to arrive at an API design decision that makes things easiest for our users. We may have other, internal, concerns (mostly organizational/political),   but providing the best possible API to our users is our primary concern.</p> <p>Along those lines, it's very important to note that...   this is not an important issue to our users,   unless we do something that makes their life more difficult.</p> <p>From the perspective of their applications,   our URIs are just magic strings that they will copy-paste into their code. From the perspective of their engineers when building out their application,   it's helpful for our URIs to be descriptive,   such that they can guess at what field they correspond to. From the perspective of their business decision makers,   the only thing that really matters is that these URIs are stable:   that we do not change our API in ways that are backwards-incompatible with their existing applications. That's about the sum total of our users' concerns:</p> <ul> <li>Ensure that our URIs are documented,     such that they can be copy-pasted into their application code.</li> <li>Ensure that our URIs are descriptive,     such that engineers building applications against our APIs can make reasonable guesses as to their meaning.</li> <li>Ensure that our APIs continue to work with their applications.</li> </ul> <p>As a bonus, it's helpful if our URIs are also URLs for documentation relating to themselves,   such that engineers can copy-paste them into their browsers and get detailed information on the field they represent/identify.</p> <p>Those are our first-order concerns. Internally, we of course have other goals.</p> <p>Primary among those is a natural desire for our URLs to match our product and/or organizational structure. For example, if the Awesome FHIR API is being offered by the Office of Fun,   in the Entertaining Parties (EP) agency,   which is a part of the Department of Population Happiness (DPH),   which is part of the UK government,   a base URI like the following might make sense:   <code>https://awesomefhir.fun.ep.dph.gov.uk/</code>. Or variants of it. For BFD, that could look something like this:   <code>https://bfd.dasg.oeda.cms.gov/</code>. Or variants of it.</p>","tags":["RFC's"]},{"location":"rfcs/proposed/0010-custom-system-uris.html#proposed-solution","title":"Proposed Solution","text":"<p>This RFC makes the recommendation that BFD should stick with a base of   <code>https://bluebutton.cms.gov/resources/</code> for all custom URIs,   at least until a v3 of the API.</p> <p>The primary justification here is simplicity for end users:   having separate base URIs for new fields would make the API slightly more complex for end users to interact with. If we're going to use a different custom URI, we should make that change all at once, across the whole API.</p> <p>Changing the URIs in v1 or v2 would be a major backwards-incompatibility,   it would break almost all existing customer applications,   which is something we strive very hard to avoid doing.</p> <p>Additionally, it's hard to come up with a different answer that is both future-proof and would deliver any real user value,   even in a future version of the API, e.g. v3. * A base URI such as <code>https://bfd.dasg.oeda.cms.gov/</code> would capture details of CMS' current internal organizational chart,     in ways that would be awkward in the face of potential future re-organizations.   If, for example, the Data and Analytics Strategy Group (DASG) gets renamed,     would we want to change the URI again? * A base URI such as <code>https://claims-data-api.bfd.cms.gov/</code> is perhaps a safer and more ontologically-correct choice,     but it's hard to imagine any existing user caring such that they'd feel it justifies changing     the URIs already baked into the v1/v2 version of their application.   It's also perhaps a bit of an organizational overreach,     as BFD is hardly the only \"claims data API\" at CMS. * A base URI such as <code>https://bfd.cms.gov/</code> is perhaps a safer and more ontologically-correct choice,     but it's hard to imagine any existing user caring such that they'd feel it justifies changing     the URIs already baked into the v1/v2 version of their application.</p>","tags":["RFC's"]},{"location":"rfcs/proposed/0010-custom-system-uris.html#proposed-solution-detailed-design","title":"Proposed Solution: Detailed Design","text":"<p>Taking a look at BFD's   TransformerConstants   file reveals the following base URI structure:</p> <pre><code>https://bluebutton.cms.gov/resources/\n  codesystem/\n    adjudication/\n      &lt;various EOB.adjudication.category discriminators&gt;\n    information/\n      &lt;various EOB.information.category discriminators&gt;\n    variables/\n      &lt;all sorts of stuff, but mostly EOB.extension discriminators&gt;\n  identifier/\n    &lt;several identifier.system discriminators&gt;\n</code></pre> <p>Most of the leaf URI \"path\" components refer to variables found in the   CCW Data Dictionaries. As such, the URI structure should be extended for incorporation of future, non-CCW fields. For PACA, something like the following would be recommended,   which would ensure proper namespacing of similar-yet-actually-distinct fields:</p> <pre><code>https://bluebutton.cms.gov/resources/\n  fiss/\n  mcs/\n  rda/\n</code></pre> <p>Similarly, new CCW fields should use a base URI of <code>https://bluebutton.cms.gov/resources/ccw/</code>.</p> <p>Any backwards-compatible changes to terminologies represented by these URIs should keep the same URIs.</p>","tags":["RFC's"]},{"location":"rfcs/proposed/0010-custom-system-uris.html#proposed-solution-unresolved-questions","title":"Proposed Solution: Unresolved Questions","text":"<p>The primary unresolved question is whether or not this decision should be revisited in a future v3 of the API. This RFC recommends reconsidering it, but framing the decision in terms of user value,   e.g. \"Is this worth doing if it adds burden to application developers that we will already have a hard time convincing to adopt the new version of our API?\"</p> <p>An additional unresolved question is how to handle backwards-incompatible changes to terminologies / code sets. That question is not urgent, though, and should not block the adoption of this RFC's other recommendations. See this Slack thread for details: https://cmsgov.slack.com/archives/CMT1YS2KY/p1642702722039300.</p>","tags":["RFC's"]},{"location":"rfcs/proposed/0010-custom-system-uris.html#proposed-solution-drawbacks","title":"Proposed Solution: Drawbacks","text":"<p>Most of the potential drawbacks/concerns with this solution have already been discussed,   in previous sections, e.g. concerns around modeling organizational/product structure. Beyond those concerns,   the real drawback here is potential confusion from users on seeing \"bluebutton\" in API responses,   for non-Blue Button 2.0 APIs. If we do receive that feedback from users,   it is likely easy enough to resolve with a brief explanation in the APIs' documentation, e.g.:</p>  <p>Note: The custom URIs used in this API are prefixed with   <code>https://bluebutton.cms.gov/resources/</code> for compatibility reasons.</p>","tags":["RFC's"]},{"location":"rfcs/proposed/0010-custom-system-uris.html#proposed-solution-notable-alternatives","title":"Proposed Solution: Notable Alternatives","text":"<p>Discussed elsewhere.</p>","tags":["RFC's"]},{"location":"rfcs/proposed/0010-custom-system-uris.html#prior-art","title":"Prior Art","text":"<p>Not particularly relevant to this issue.</p>","tags":["RFC's"]},{"location":"rfcs/proposed/0010-custom-system-uris.html#future-possibilities","title":"Future Possibilities","text":"<p>Discussed elsewhere.</p>","tags":["RFC's"]},{"location":"rfcs/proposed/0010-custom-system-uris.html#addendums","title":"Addendums","text":"<p>The following addendums are required reading before voting on this proposal:</p> <ul> <li>(none at this time)</li> </ul>","tags":["RFC's"]},{"location":"rfcs/proposed/0013-synthea-future-claims-and-automation.html","title":"RFC Proposal","text":"<ul> <li>RFC Proposal ID: <code>0013-Synthea-future-claims-and-automation</code></li> <li>Start Date: 2022-04-08</li> <li>RFC PR: beneficiary-fhir-data/rfcs#0013</li> <li>JIRA Ticket(s):</li> <li>BFD-1616</li> </ul> <p>The demand for realistic synthetic beneficiaries with recent claim data, for BFD users, continues to grow after each Synthea release. While the process to generate, load, test, and release synthetic data has become more refined and convenient, it is prone to human error that can cause significant delays and miscommunication during release. This RFC proposes a streamlined, automated process for generating, loading, testing, and releasing Synthea data to BFD users, while taking into account changes to synthetic benficiary properties with recent and future claim dates.</p>","tags":["RFC's"]},{"location":"rfcs/proposed/0013-synthea-future-claims-and-automation.html#status","title":"Status","text":"<ul> <li>Status: Proposed</li> <li>Implementation JIRA Ticket(s):</li> <li>BFD-XXXX</li> </ul>","tags":["RFC's"]},{"location":"rfcs/proposed/0013-synthea-future-claims-and-automation.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>RFC Proposal</li> <li>Status</li> <li>Table of Contents</li> <li>Motivation<ul> <li>Background on Synthea</li> </ul> </li> <li>Proposed Solution<ul> <li>Automated Recurring Generation</li> <li>Automated Recurring Loading</li> <li>Large Synthetic Data Generation</li> <li>Required BFD Application Changes</li> <li>Proposed Solution: Detailed Design</li> <li>Proposed Solution: Unresolved Questions</li> <li>Proposed Solution: Drawbacks</li> <li>Proposed Solution: Notable Alternatives</li> </ul> </li> <li>Prior Art</li> <li>Future Possibilities</li> <li>Addendums</li> </ul>","tags":["RFC's"]},{"location":"rfcs/proposed/0013-synthea-future-claims-and-automation.html#motivation","title":"Motivation","text":"<p>As previous synthetic data releases were separated by 3 months, and only 20,000 synthetic beneficiaries released, end-users have expressed interest in a larger synthetic dataset to work with, that has consistently current claim dates. Additionally, there is inefficiency involved with producing and releasing synthetic data when done without automation. Manual process are prone to human error, and can cause incidents, such as accidental overlap of beneficiary IDs between different synthetic data batches. There are more than enough reasons to look into streamlining and scaling up synthetic data production to maximize the benefit to users and partners. This involves automated recurring Synthea data generation, automated recurring loading, large Synthea data generation, and potential required changes to the BFD server and pipeline application.</p> <p>For those unfamiliar with Mitre's Synthea application for generating synthetic beneficiary and claim data, before discussing a proposed solution, here is some background on Synthea and how BFD uses it.</p>","tags":["RFC's"]},{"location":"rfcs/proposed/0013-synthea-future-claims-and-automation.html#background-on-synthea","title":"Background on Synthea","text":"<p>Mitre Synthea:</p> <ul> <li> <p>Synthea -- for generating synthetic beneficiary and claim data to load into BFD DB.</p> </li> <li> <p>How To Differentiate Synthea And Real Data:</p> <ul> <li>Synthetic beneficiary IDs are a negative number.</li> <li>Synthetic MBIs have the character 'S' in a certain position.</li> </ul> </li> </ul> <p>Generating &amp; Loading Synthea Data: - BFD downloads the master branch of the Synthea codebase, generates the RIF files via the <code>run_Synthea</code> command, where the batch size and geography can be set, as well as a Synthea.properties file, which is important, for defining numeric ranges of key claim and beneficiary properties i.e. beneficiary ID, claim group ID, claim date, etc. - The RIF files are copied from the Synthea output folder to a local master branch of the BFD codebase in <code>bfd-model/bfd-model-rif-samples/src/main/resources/rif-synthea</code> for load testing into the local BFD database. - If there are no errors loading, the RIF files are uploaded to AWS S3 and loaded into TEST, PROD SBX, PROD databses via the same ETL process with CCW RIF files. - Synthetic data is now available to BFD users.</p> <p>In addition to these steps, BFD follows a thorough Synthea Test Plan  before generating or loading data into TEST, PROD SBX, or PROD environments.</p>","tags":["RFC's"]},{"location":"rfcs/proposed/0013-synthea-future-claims-and-automation.html#proposed-solution","title":"Proposed Solution","text":"","tags":["RFC's"]},{"location":"rfcs/proposed/0013-synthea-future-claims-and-automation.html#automated-recurring-generation","title":"Automated Recurring Generation","text":"<p>Automating the generation and loading of Synthea data will remove a lot of error-prone manual work and also help ensure that users are regularly receiving the benefits of the ongoing improvements to Synthea. This operation will be scheduled to run once per quarter.</p> <ul> <li> <p>Quarterly - Synthetic data for current and future claims will be generated with a set batch size using the up-to-date master branch of the Synthea codebase in a hosted cloud instance, along with the Synthea end-state properties file from the most recent generated batch, which is hosted in AWS S3. The end-state properties file is critical for making sure the next batch of Synthea data will not overlap with released Synthea data.</p> </li> <li> <p>Special data parameters of interest: One parameter, which has not changed in previous releases of Synthea is Part D contract ID. Despite monotonically increasing other parameters i.e. bene ID, claim group ID, PDE event ID, the value of the PDE contract ID will not need to change on a regular basis, unless there is a specific customer use case presented. Other parameters of interest, are in the end-state properties, and Synthea.properties files i.e. claim id, claim group id starts, etc, which do change, and can cause collisions.</p> </li> </ul>","tags":["RFC's"]},{"location":"rfcs/proposed/0013-synthea-future-claims-and-automation.html#automated-recurring-loading","title":"Automated Recurring Loading","text":"<ul> <li> <p>The synthetic data will be tested in isolation by (on the same system that was used to generate the data):</p> <ol> <li>Cloning the latest version of BFD from GitHub.</li> <li>Launching a local PostgreSQL database via Docker.</li> <li>Downloaing the RIF files from the previous quarterly batch of Synthea data from AWS S3, and running the BFD build and integration tests, using the newly generated Synthea data and previous batch data, to verify that the data loads as expected.</li> </ol> </li> <li> <p>Once loaded, a script will run that modifies the new batch's generated manifest file, and uploads the RIF files, end-state properties, and manifest files to S3 via command line file transfer. This will trigger the BFD ingestion pipeline in TEST so that the data is stored in the database.</p> </li> <li> <p>The separate RIF files associated with future claim data for the given release will be uploaded to a designated folder in S3 for staging until loading in the future. Updates to the pipeline application will be made to scan for these folders and trigger an incoming load job each weekend, for the claims that have \"matured\" that week (similar to how new claims are loaded each week in <code>prod</code>).</p> </li> </ul> <p>Additional testing - As of 07/05/2022, 6 million de-duped non-synthetic benes, along with ~10-15 3 digit benes that were used for some other testing in the past have been removed. The cleanup removed the 6 million non-negative non-synthetic benes, along with the 3 digit benes, and left the existing 50k Synthea benes. Claim data from this cleanup was largely responsible for the certain parameter collisions that have taken place in the past, when loading new Synthea data. There's also ongoing efforts to clean up other data ranges to avoid all future collisions, with properties such as claim group id. Beneficiary id, claim group id, and other properties will still have potential for collision, as most of the data that remains will have distinct Synthea characteristics i.e. negative beneficiary ids. With these cleanups in place, it will likely take many years for there to be enough data to cause overlap. The amount and queries and checks in the script that automates the Synthea test plan will not have to be as thorough.</p>","tags":["RFC's"]},{"location":"rfcs/proposed/0013-synthea-future-claims-and-automation.html#large-synthetic-data-generation","title":"Large Synthetic Data Generation","text":"<ul> <li> <p>Each PI (Program Increment), approximately every 8 weeks, 10 - 60 million Synthea beneficiaries will be generated and available for on-demand performance and load testing on TEST and PROD SBX databases. The number of beneficiaries in the dataset size is meant to reflect the size and shape of production. The process will be similar to that of automated recurring generation, however the memory and computational power of the AWS instance will be larger for time and cost effectiveness.</p> </li> <li> <p>Using a m5a.24xlarge EC2 instance, it took 6 hours to generate 1 million synthetic beneficiaries. If one extrapolates from this benchmark, it would take 60 hours or a little under 3 days to generate 10 million, and 360 hours, or 15 days to generate 60 million beneficiaries.</p> </li> <li> <p>A new instance will be spun up with Terraform and Jenkins to avoid introducing security vulnerbilities and patching with a reusuable instance.</p> </li> </ul>","tags":["RFC's"]},{"location":"rfcs/proposed/0013-synthea-future-claims-and-automation.html#required-bfd-application-changes","title":"Required BFD Application Changes","text":"<p>The following changes to existing behavior will be implemented:</p> <pre><code>1. RIF manifests will be updated with optional attributes to indicate:\n    1. When a data set should have back-dated beneficiary filtering disabled.\n    2. When a data set has a \"do not process until\" date.\n2. The BFD Pipeline application will be updated to accept, parse, and honor those attributes.\n</code></pre>","tags":["RFC's"]},{"location":"rfcs/proposed/0013-synthea-future-claims-and-automation.html#proposed-solution-detailed-design","title":"Proposed Solution: Detailed Design","text":"<p>Automated Generation &amp; Load Plan:</p> <p>On a quarterly basis a cron job within Jenkins will be set up to spin up and SSH into a provisioned AWS EC2 instance, and run an Ansible script that takes a batch size and generation year, and will execute other scripts that:   - Run currently manual steps in the <code>Synthea Test Plan</code> such as accessing and comparing the latest Synthea end-properties file, running queries in PROD SBX to determine database ranges for beneficiary and claim properities, and ensuring the next batch of data will not overlap with existing data ranges.   - Generate batch containing both synthetic historical and future data   - Load into AWS S3 for ingestion in TEST, PROD SBX, and PROD environments.</p> <p>For future claims and new Synthea data to be ingested, the manifest.xml file will need additional fields for both a timestamp and whether there Synthea data. The pipeline will then check for these two fields. For future claims, the data that has been staged in S3 will be ingested once it is the date in the timestamp in the manifest. With these changes in place, on a weekly basis, future claims will be loaded into the database.</p> <p>Large Synthetic Data Generation:   - Every PI, a m5a.24xlarge, or more powerful instance will be spun up to generate 10 - 60 million beneficiaries.   - In order to distiguish load test data when stored in TEST or PROD SBX, the beneficiary ID range will start 1 million less than the smallest released Synthea beneficary ID   - After the load tests are complete, and data is reported, the data will persist in the database. If at some point Synthea's codebase involves major changes to beneficiary and claim data properties, this data will need be deleted and re-generated the newer version of Synthea. There needs to be a tool for deleting this data that generates and executes DELETE and SELECT statements in the database environments. This tool will take the upper and lower bounds of the benficiary, claim, claim group, and PDE ID ranges found in the RIF files that were loaded as an input. To ensure the data is properly deleted, SELECT statements with the ranges used to delete the data will be run, and the final output of the tool will be the counts of beneficiary and various claim tables. The counts in the output should be zero to indicate the data cannot be found.</p>","tags":["RFC's"]},{"location":"rfcs/proposed/0013-synthea-future-claims-and-automation.html#proposed-solution-unresolved-questions","title":"Proposed Solution: Unresolved Questions","text":"<ul> <li>Which of these is a more viable failsafe/revert option if data fails to be loaded:<ul> <li>Revert to an earlier backup of the EC2 instance.</li> <li>Create a unique marker for each Synthea dataset to easily query and delete the data.</li> <li>DELETE query tool.</li> </ul> </li> </ul>","tags":["RFC's"]},{"location":"rfcs/proposed/0013-synthea-future-claims-and-automation.html#proposed-solution-drawbacks","title":"Proposed Solution: Drawbacks","text":"<ul> <li>The need for regular TEST, PROD SBX, and PROD backups will be imperative in case issues arise with loading and removing data to prevent overlap, and when running in idempotent mode with the pipeline does not work.</li> </ul>","tags":["RFC's"]},{"location":"rfcs/proposed/0013-synthea-future-claims-and-automation.html#proposed-solution-notable-alternatives","title":"Proposed Solution: Notable Alternatives","text":"","tags":["RFC's"]},{"location":"rfcs/proposed/0013-synthea-future-claims-and-automation.html#prior-art","title":"Prior Art","text":"","tags":["RFC's"]},{"location":"rfcs/proposed/0013-synthea-future-claims-and-automation.html#future-possibilities","title":"Future Possibilities","text":"<ul> <li>Pipelining multiple EC2 instances to generate data more efficiently is something to explore at a later point.</li> </ul>","tags":["RFC's"]},{"location":"rfcs/proposed/0013-synthea-future-claims-and-automation.html#addendums","title":"Addendums","text":"<p>The following addendums are required reading before voting on this proposal:</p> <ul> <li>(none at this time)</li> </ul>","tags":["RFC's"]},{"location":"rfcs/proposed/0014-dsl-driven-rda-code-generation.html","title":"RFC Proposal","text":"<ul> <li>RFC Proposal ID: <code>0014-dsl-driven-rda-code-generation</code></li> <li>Start Date: 2021-10-11</li> <li>Revised Date: 2022-04-07</li> <li>RFC PR: CMSgov/beneficiary-fhir-data#1047</li> <li>Obsolete RFC PR: CMSgov/beneficiary-fhir-data#788</li> <li>JIRA Ticket(s):</li> <li>DCGEO-196</li> <li>PACA-280</li> <li>Proof of Concept PR: CMSgov/beneficiary-fhir-data#1049</li> </ul> <p>RDA API fields are changing rapidly as the RDA API moves towards its production release. Each API change triggers BFD code changes in several places. An automated process to generate BFD code based on a simple metadata file would eliminate tedious and error-prone coding and keep all important details about the RDA data in a single place. Doing so would replace a large amount of handwritten code with a single maven plugin to generate code that enforces RDA coding conventions and ensures generated code correctly matches the data. Areas affected by this process could include hibernate entities, data transformations to copy protobuf objects into entities, database migration SQL, randomized synthetic data production, and data transformations to copy synthea data into protobuf objects.</p> <p>In addition, the existing RIF annotation processor that generates code based on a combination of data in a spreadsheet and hardcoded values in the java source can be replaced by the same DSL based code generator.</p>","tags":["RFC's"]},{"location":"rfcs/proposed/0014-dsl-driven-rda-code-generation.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>RFC Proposal</li> <li>Table of Contents</li> <li>Motivation</li> <li>Proposed Solution<ul> <li>DSL for RDA API Data</li> <li>DSL For RIF Data</li> <li>Proposed Solution: Detailed Design</li> <li>DSL Syntax</li> <li>Maven Plugin Design</li> <li>Generating JPA Entity classes</li> <li>Generating Transformation class</li> <li>Field Transformations</li> <li>Array Transformation (Detail records)</li> <li>Proposed Solution: Unresolved Questions</li> <li>Proposed Solution: Drawbacks</li> <li>Proposed Solution: Notable Alternatives</li> </ul> </li> <li>Prior Art</li> <li>Future Possibilities</li> <li>Addendums</li> </ul>","tags":["RFC's"]},{"location":"rfcs/proposed/0014-dsl-driven-rda-code-generation.html#motivation","title":"Motivation","text":"<p>The RDA API is evolving rapidly and adding new fields as it moves towards production. Even after a production release is completed more fields will be added rapidly as that API development evolves from a first release focused on reliability and features to followup releases filling in more and more of the data available in the backend systems.</p> <p>Initially code to handle RDA API data was handwritten as there were relatively few fields at that time and the RDA API team had not yet established conventions for mapping the data into protobuf. Now the number of fields is growing and those conventions are well established. When estimating the ultimate size of the RDA API message objects the team indicated that there might ultimately be 2-5 times as many fields as now.</p> <p>With handwritten code the addition of new fields by the RDA API team can trigger many code changes across the project. These can include: - SQL migration scripts. - Hibernate database entity classes. - Data transformation/validation code to copy data from protobuf messages into entity objects. - Random synthetic data generation classes. - Data transformation code to copy data from Synthea data files into protobuf messages.</p> <p>Changes made to each of these areas require careful attention to ensure the logic, data types, and validation rules are correct. These changes have to be made consistently in different places in the code. And yet most of this code is repetitive since the fields follow established conventions. For example, every maximum field length in the database must be properly reflected in the database entities, enforced in the data validation code, and honored in the synthetic data generators. This can certainly be done with handwritten code but is error-prone and requires developer time to write/modify the code and review the associated PR.</p> <p>This RFC proposes replacing all of the handwritten code with a code generator that creates code using metadata in a YAML file. The code generator would remove the need to write and maintain large amounts of code. In the proof of concept the code generator replaced seven handwritten entity classes (over 1,000 LOC) and two transformer classes (over 1,400 lines) with a single 741 line YAML file. In addition, over 1,500 LOC of unit tests for the handwritten transformers could be eliminated and replaced with much smaller tests that verify the reusable transformation components used by the code generator work properly.</p> <p>The YAML file is much simpler to understand than the handwritten code and less error-prone. A full implementation would add further savings by replacing still more handwritten code.</p> <p>For an illustration of the code savings consider the difference in complexity between the YAML DSL files from the POC and these handwritten classes:</p> <ul> <li>PreAdjFissClaims hand written entity</li> <li>FissClaimTransformer hand written transformer class</li> <li>RandomFissClaimGenerator hand written synthetic data class</li> </ul> <p>The POC also replicates the output of the current RIF annotation processor using the DSL as input. The YAML DSL files for RIF data are also available for review in GitHub.</p> <p>Note: Code examples in this document are taken from proof of concept work performed in the <code>paca-394-rif-dsl-prototype-rebase</code> branch of the BFD repo.  The code in that branch is functional though incomplete and provides insight into the issues involved in following this recommendation.</p>","tags":["RFC's"]},{"location":"rfcs/proposed/0014-dsl-driven-rda-code-generation.html#proposed-solution","title":"Proposed Solution","text":"","tags":["RFC's"]},{"location":"rfcs/proposed/0014-dsl-driven-rda-code-generation.html#dsl-for-rda-api-data","title":"DSL for RDA API Data","text":"<p>A code generator processes YAML based metadata files to create all of the code required to work with the RDA API data messages, objects, and fields. The YAML describes in a declarative way what every RDA API message is, what table that message is stored in, what columns are in the table, and how to transform the data from the RDA API messages into values in those columns. For example:</p> <pre><code>  - id: McsClaim\n    messageClassName: gov.cms.mpsm.rda.v1.mcs.McsClaim\n    entityClassName: gov.cms.bfd.model.rda.PreAdjMcsClaim\n    transformerClassName: gov.cms.bfd.pipeline.rda.grpc.source.McsClaimTransformer2\n    table:\n      name: McsClaims\n      schema: pre_adj\n      primaryKeyColumns:\n        - idrClmHdIcn\n      columns:\n        - name: idrClmHdIcn\n          sqlType: varchar(15)\n          nullable: false\n        - name: sequenceNumber\n          sqlType: bigint\n        - name: idrClaimType\n          sqlType: varchar(1)\n        - name: idrDtlCnt\n          sqlType: int\n        - name: idrBeneLast_1_6\n          sqlType: varchar(6)\n        - name: idrBeneFirstInit\n          sqlType: varchar(1)\n        - name: idrBeneMidInit\n          sqlType: varchar(1)\n        - name: idrCoinsurance\n          sqlType: decimal(7,2)\n        - name: idrClaimReceiptDate\n          sqlType: date\n    transformations:\n      - from: idrClmHdIcn\n        optional: false\n      - from: sequenceNumber\n      - from: idrClaimType\n        transformer: MessageEnum\n        transformerOptions:\n          enumClass: gov.cms.mpsm.rda.v1.mcs.McsClaimType\n      - from: idrDtlCnt\n      - from: idrBeneLast16\n        to: idrBeneLast_1_6\n      - from: idrBeneFirstInit\n      - from: idrBeneMidInit\n      - from: idrCoinsurance\n      - from: idrClaimReceiptDate\n</code></pre> <p>This example illustrates some of the advantages of using a declarative file:</p> <ul> <li>Standard conventions can be supported as defaults.  For example:</li> <li>The <code>to</code> only needs to be defined if it differs from the <code>from</code>.  Generally columns have the same name as their RDA API field but not always.</li> <li>A <code>javaType</code> only needs to be defined if it differs from the default for the column's data type.</li> <li>A <code>transformer</code> only needs to be defined if it differs from the default for the column's data type.</li> <li>Transformers can be easily created and added to the plugin.  Each has a name used to reference it in the YAML.</li> <li>The code generator follows a simple set of rules to choose a default transformation if no <code>transformer</code>  is provided in the YAML.</li> <li>A single field can have multiple transformations.  For example the MBI field can be copied directly to a column and also used to store a hashed value in another column.</li> <li>Transformers can have their own specific options if their behavior is modifiable from default settings.</li> <li>A few structural transforms can be specified using a virtual <code>from</code> that triggers the transform.  These are useful for columns whose values are known at runtime but not taken directly from the messages (like array indexes, the current timestamp, parent primary key column value, etc).</li> </ul> <p>A code generator can ensure the relationships between tables in JPA are handled correctly. In JPA detail tables require properly defined composite key class and the parent requires a special collection field with appropriate annotation. These relationships can be trivially defined as <code>array</code>s in the YAML and the code generator takes care of getting the implementation details correct:</p> <pre><code>  - id: FissClaim\n    messageClassName: gov.cms.mpsm.rda.v1.fiss.FissClaim\n    entityClassName: gov.cms.bfd.model.rda.PreAdjFissClaim\n    transformerClassName: gov.cms.bfd.pipeline.rda.grpc.source.FissClaimTransformer2\n    table:\n      name: FissClaims\n      schema: pre_adj\n      primaryKeyColumns:\n        - dcn\n      columns:\n        - name: dcn\n          sqlType: varchar(23)\n          nullable: false\n        - name: sequenceNumber\n          sqlType: bigint\n          nullable: false\n        - name: hicNo\n          sqlType: varchar(12)\n          nullable: false\n        # ... skipping lots of columns for the sake of this example ...\n    transformations:\n      - from: dcn\n        optional: false\n      - from: seq\n        to: sequenceNumber\n        optional: false\n      - from: hicNo\n        optional: false\n      # ... skipping lots of transforms for the sake of this example ...\n    arrays:\n      # Every FissClaim can have multiple procedure codes, diagnosis codes, and payers\n      - from: fissProcCodes\n        to: procCodes\n        mapping: FissProcCode\n      - from: fissDiagCodes\n        to: diagCodes\n        mapping: FissDiagnosisCode\n      - from: fissPayers\n        to: payers\n        mapping: FissPayer\n        namePrefix: \"payer\"\n</code></pre> <p>The example illustrates the three detail tables associated with each <code>FissClaim</code>. The RDA API sends these as <code>repeated</code> fields in the protobuf definition and the plugin maps them to detail entities in the JPA classes. Each one references the field in the protobuf message and the entity class and the mapping used to define the detail table.</p> <p>The code generator is implemented as a maven plugin. This has several advantages: - The plugin fits seamlessly into the BFD build process.  Nothing in the BFD build or release process has to change. - Code is automatically generated and made available to the compiler. - IDEs can display and debug the generated code without any special settings.  This includes allowing breakpoints within the generated code if needed.</p>","tags":["RFC's"]},{"location":"rfcs/proposed/0014-dsl-driven-rda-code-generation.html#dsl-for-rif-data","title":"DSL For RIF Data","text":"<p>The same DSL can be used to generate code for processing RIF data. Using the DSL driven code generator for RIF data has several advantages over the existing RIF annotation processor. - The same DSL serves to define all data ingested by BFD.  This reduces the learning curve for new developers. - The YAML files contain all of the meta-data for each entity and eliminate the knowledge split between an Excel spreadsheet and java code in the existing process. - The YAML files are plain text.  They can be easily diffed by GitHub during PR reviews. - Maven plugins are easier to debug in an IDE (using <code>mvnDebug</code>) than Java annotation processors. - Maven plugins appear to integrate better with IDEA than the annotation processor.</p> <p>The current POC expands on the original to add full support for generating the following code for RIF data: - entity classes (parent and, when needed, line) for each RIF file type (Beneficiary, DME, Carrier, etc) - column enum class for each RIF file - parser to convert RIF records into entities - CSV writer to convert entities into arrays of header/value pairs</p> <p>Except for the parser the code generated by the POC is nearly identical to that generated by the current annotation processor. Differences include: - entity classes from the POC use lombok where possible - DSL includes comments taken from the Excel spreadsheet and add them as javadoc comments in the entities - parser classes use the same framework as RDA transformer classes rather than generating code identical to the annotation processor</p> <p>Adding RIF support required extending the DSL slightly to allow specific features to be enabled or disabled. Support was also added for generating code that takes values from RIF data in Lists rather than protobuf generated message classes.</p> <p>The concept of arrays from the RDA API mapped well to lines in RIF data. RIF objects just have one array each rather than many but otherwise work similarly.</p>","tags":["RFC's"]},{"location":"rfcs/proposed/0014-dsl-driven-rda-code-generation.html#proposed-solution-detailed-design","title":"Proposed Solution: Detailed Design","text":"","tags":["RFC's"]},{"location":"rfcs/proposed/0014-dsl-driven-rda-code-generation.html#dsl-syntax","title":"DSL Syntax","text":"<p>The DSL file is a YAML file containing an array of mappings. Each mapping defines one RDA API message object. Top level properties in a mapping are: - id: Unique identifier (name) of the mapping. - sourceType: Specifies how to access values from message objects.  Possible values are <code>Grpc</code> or <code>RifCsv</code>. - messageClassName: Name of the protobuf message class as defined in the RDA API proto files or the CSV adaptor class for RIF data. - entityClassName: Name of the generated JPA entity class to store objects of this type. - entityInterfaces: Array of interface names that entity class should implement. - transformerClassName: Name of the generated class to implement the transformation of protobuf message objects into JPA entities. - table: Object defining details about the database table that the JPA entity maps to. - enumTypes: Array of objects defining any <code>enum</code> classes that the generator should create for use in the transformation. - transformations: Array of objects defining what transformations to apply to an RDA API message to copy its data into a JPA entity. - arrays: Array of objects defining detail objects (one-to-many relationships) contained within the message. - joins: Array of objects defining specific join relationships between this entity and some other entity.</p> <p>The <code>table</code> objects contain the following properties: - name: Name of the table as it appears in the database. - schema: Name of the schema containing the table (optional). - primaryKeyColumns: Array of column names for the primary key. - quoteNames: When true indicates schema, database, and column names should be quoted in JPA entities. - equalsNeeded: When true indicates entity classes need to include equals/hashCode methods. - compositeKeyClassName: Name of static inner class used for composity keys (usually LineId or PK). - columns: Array of objects defining the columns in the table.  Columns have the following properties:   - name: Name of the column as it appears in the database and entity class.   - dbName: Name of the column in the database (if different than the field name in the entity).   - sqlType: Type of the column as it would appear in SQL DDL defining the column.   - javaType: Type of the field in the JPA entity class that holds this column's value.   - javaAccessorType: Type of the field's accessor methods if it differs frm field type (for long/String in RIF).   - nullable: Boolean indicating if the column is nullable.   - comment: Arbitrary text to add to fields as javadoc in entity classes.</p> <p>The <code>enumType</code> objects are somtimes used as flags in mapping messages to entities. For example the RDA API sends data in one of two possible sub-objects for payer details in FissPayer messages and the transformer copies data from whichever one has data and sets an enum column in the entity to indicate which of the two it copied the data from. The properties of these objects in the DSL are: - name: Name of the enum class in java. - values: Array of enum value names added to the enum in the java code. They are also used in RIF mappings to trigger creation of the column enums.</p> <p>The <code>transformation</code> objects define how to copy data from a field in an RDA API message to a field in the JPA entity. Generally each field has one transformation applied to it, but the DSL allows multiple transformations to be applied. A simple example of needing multiple transformations is a the <code>mbi</code> field. It requires two transformations: one to copy the field as is and another to store its hashed value. The properties of <code>transformation</code> objects in the DSL are: - from: Name of the field to transform as it appears in an RDA API message. - to: Name of the field in the JPA entity to store the the transformed value into. - optional: Boolean indicating whether a missing field value in the RDA API message is allowed. - transformer: Specifies the name of the transformation to apply.  These are predefined in the code generator. - transformerOptions: Optional object containing key value pairs of configuration options for the transformer.  These settings are specific to the particular transformer.</p> <p>The <code>array</code> objects contain the following properties: - from: Name of the field containing the array in the RDA API message. - to: Name of the field containing the collection of entities in the JPA entity. - mapping: Id of a mapping that will be used to transform each of the messages in the array. - namePrefix: Additional prefix added to field names for array elements when logging transformation errors.  These are optional but they make the error messages much more readable.</p> <p>The <code>join</code> objects contain the following properties: - fieldName: Name of the java field in the entity class. - entityClass: Name of the entity being joined. - collectionType: Type of collection to use for field (List or Set). - readOnly: When true no setter is generated for the field. - joinType: Type of JPA join annotation to use (OneToMany, ManyToOne, OneToOne). - mappedBy: value for mappedBy parameter in JPA annotation. - orphanRemoval: value for orphanRemoval parameter in JPA annotation. - fetchType: value for fetchType parameter in JPA annotation. - cascadeTypes: array of values for cascade parameter in JPA annotation.</p> <p>Most of these properties have reasonable defaults to handle the most common use cases.</p>","tags":["RFC's"]},{"location":"rfcs/proposed/0014-dsl-driven-rda-code-generation.html#maven-plugin-design","title":"Maven Plugin Design","text":"<p>Every maven plugin implements one or more goals. Each goal defines some operation specific to the plugin. The code generation plugin has a goal for each type of code to be generated.</p> <ul> <li>The <code>entities</code> goal generates Hibernate database entity classes with all appropriate annotations, getters, setters, equals/hashCode, and entity relationship mappings.</li> <li>The <code>transformers</code> goal generates data transformation/validation classes to copy data from RDA API protbuf messages into database entity classes.</li> <li>The <code>csv-writers</code> goal generates RIF CsvWriter classes for each parent entity.</li> <li>The <code>sql</code> goal generates sample SQL DDL for each table that can be used as the basis for creating Flyway migration files.</li> <li>The <code>random-data</code> goal generates random data generation classes to create randomized data of appropriate size and type for each object/field in the RDA API messages. (Not in POC)</li> <li>The <code>synthea-bridge</code> goal generates data transformation classes to copy data from Synthea RIF data files into protobuf messages. (Not in POC)</li> </ul> <p>All of the goals provided by the plugin follow the same basic steps:</p> <ul> <li>Read the mapping files using a library such as Jackson to map the file's contents into java beans.</li> <li>Process the mappings in a goal specific way to generate java code using a library such as javapoet to generate the java files.</li> <li>Write the generated class files to a directory specified by the <code>pom.xml</code> file.</li> </ul> <p>Different goals use different subsets of the metadata in the DSL file to generate different types of source code. - The <code>entities</code> goal only processes the <code>table</code> object since it generates the relevant Hibernate entities and all of the data required to do so is defined in the <code>table</code>. - A <code>sql</code> goal would also process the <code>table</code> object to generate SQL <code>CREATE TABLE</code> and <code>ALTER TABLE</code> DDL code that a developer could use as the basis of a Flyway migration file. - The <code>transformers</code> goal additionally needs to process the <code>transformations</code> and <code>arrays</code> sections since these define the relationships between the RDA API message data and the fields in the Hibernate entities.</p> <p>The plugin itself is easily executed in any module by adding a <code>&lt;plugin&gt;</code> element to the <code>&lt;build&gt;</code> element of the module's <code>pom.xml</code> file. The <code>pom.xml</code> file specifies where to store the generated source code and the plugin automatically adds the directory to the maven compiler's source path.</p> <p>For example:</p> <pre><code>    &lt;build&gt;\n        &lt;plugins&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;gov.cms.bfd&lt;/groupId&gt;\n                &lt;artifactId&gt;bfd-model-dsl-codegen-plugin&lt;/artifactId&gt;\n                &lt;version&gt;${project.version}&lt;/version&gt;\n                &lt;configuration&gt;\n                    &lt;mappingFile&gt;${mapping.directory}/mapping.yaml&lt;/mappingFile&gt;\n                    &lt;outputDirectory&gt;${project.build.directory}/generated-sources/transformers&lt;/outputDirectory&gt;\n                &lt;/configuration&gt;\n                &lt;executions&gt;\n                    &lt;execution&gt;\n                        &lt;goals&gt;\n                            &lt;goal&gt;transformers&lt;/goal&gt;\n                        &lt;/goals&gt;\n                    &lt;/execution&gt;\n                &lt;/executions&gt;\n            &lt;/plugin&gt;\n        &lt;/plugins&gt;\n    &lt;/build&gt;\n</code></pre> <p>The classes generated by the plugin rely on a few utility classes that are defined in a separate library module. This library is added as a dependency in the modules that require them.</p>","tags":["RFC's"]},{"location":"rfcs/proposed/0014-dsl-driven-rda-code-generation.html#generating-jpa-entity-classes","title":"Generating JPA Entity classes","text":"<p>The <code>entities</code> goal generates JPA/Hibernate entities that are virtually identical (minus javapoet's code indentation, etc) to what we are currently maintaining by hand (or generating via annotation processor for RIF). This includes the use of the same lombok, JPA, and hibernate annotations. The existing integration tests continue to pass with the generated entities.</p> <p>This simplified example illustrates how a table and its columns can be defined in YAML:</p> <pre><code>    entityClassName: gov.cms.bfd.model.rda.PreAdjMcsDiagnosisCode\n    table:\n      name: McsDiagnosisCodes\n      schema: pre_adj\n      primaryKeyColumns:\n        - idrClmHdIcn\n        - priority\n      columns:\n        - name: idrClmHdIcn\n          sqlType: varchar(15)\n          nullable: false\n        - name: priority\n          sqlType: smallint\n          javaType: short\n          nullable: false\n        - name: idrDiagIcdType\n          sqlType: varchar(1)\n        - name: idrDiagCode\n          sqlType: varchar(7)\n        - name: lastUpdated\n          sqlType: timestamp with time zone\n</code></pre> <p>Some details illustrated by this example:</p> <ul> <li>Specifying a fully defined <code>entityClassName</code> ensures that the plugin makes no assumptions about what packages the code it generates will live in.</li> <li>The <code>schema</code> would be optional and associated annotations only added to the entity if it has been defined.</li> <li>The <code>name</code> and <code>sqlType</code> would be required.</li> <li>All other values have reasonable defaults.  For example any <code>varchar(n)</code> or <code>char(n)</code> defaults to a <code>String</code> as the <code>javaType</code>.  Similarly <code>timestamp</code> maps to <code>Instant</code> and <code>date</code> maps to <code>LocalDate</code> by default.</li> <li>All columns default to being <code>nullable</code> unless otherwise set using <code>nullable: false</code> since almost all RDA API fields are optional.</li> <li>The <code>primaryKeyColumns</code> are used to add <code>Id</code> annotations to those fields in the entity classes as well as to define the <code>hashCode</code> and <code>equals</code> methods following Hibernate rules.</li> <li>Tables with multiple <code>primaryKeyColumns</code> automatically generate a static class for the composite key object associated with the table.</li> </ul> <p>Each of the most commonly used <code>sqlType</code>s have an associated default <code>javaType</code> and appropriate logic for defining the generated <code>Column</code> annotation in the entity class. For example the code generator knows how to parse a max length out of the <code>varchar(n)</code> and <code>char(n)</code> types. Also it knows that it needs to add a <code>columnDefinition</code> value for <code>decimal(m,n)</code> types but not for most other types.</p> <p>Each <code>array</code> definined in the YAML file results in a <code>Set&lt;TEntity&gt;</code> field being created in the parent entity.  For example:</p> <pre><code>  - id: FissClaim\n    messageClassName: gov.cms.mpsm.rda.v1.fiss.FissClaim\n    entityClassName: gov.cms.bfd.model.rda.PreAdjFissClaim\n    transformerClassName: gov.cms.bfd.pipeline.rda.grpc.source.FissClaimTransformer2\n    table:\n      name: FissClaims\n      schema: pre_adj\n      primaryKeyColumns:\n        - dcn\n      columns:\n        - name: dcn\n          sqlType: varchar(23)\n          nullable: false\n    transformations:\n      - from: dcn\n        optional: false\n        # ... skipping lots of transformations for the sake of this example ...\n    arrays:\n      - from: fissProcCodes\n        to: procCodes\n        mapping: FissProcCode\n</code></pre> <p>generates code like this in the <code>PreAdjFissClaim</code> class:</p> <pre><code>@Entity\n@Getter\n@Setter\n@Builder\n@AllArgsConstructor\n@NoArgsConstructor\n@EqualsAndHashCode(\n    onlyExplicitlyIncluded = true\n)\n@FieldNameConstants\n@Table(\n    name = \"`FissClaims`\",\n    schema = \"`pre_adj`\"\n)\npublic class PreAdjFissClaim {\n  @Column(\n      name = \"`dcn`\",\n      nullable = false,\n      length = 23\n  )\n  @Id\n  @EqualsAndHashCode.Include\n  private String dcn;\n  @OneToMany(\n      mappedBy = \"dcn\",\n      fetch = FetchType.EAGER,\n      orphanRemoval = true,\n      cascade = CascadeType.ALL\n  )\n  @Builder.Default\n  private Set&lt;PreAdjFissProcCode&gt; procCodes = new HashSet&lt;&gt;();\n</code></pre> <p>If a table has multiple primary key columns the plugin knows to also generate a java bean for the composite key. For example:</p> <pre><code>  - id: McsDiagnosisCode\n    messageClassName: gov.cms.mpsm.rda.v1.mcs.McsDiagnosisCode\n    entityClassName: gov.cms.bfd.model.rda.PreAdjMcsDiagnosisCode\n    table:\n      name: McsDiagnosisCodes\n      schema: pre_adj\n      primaryKeyColumns:\n        - idrClmHdIcn\n        - priority\n      columns:\n        - name: idrClmHdIcn\n          sqlType: varchar(15)\n          nullable: false\n        - name: priority\n          sqlType: smallint\n          javaType: short\n          nullable: false\n        - name: idrDiagIcdType\n          sqlType: varchar(1)\n    transformations:\n      - from: PARENT\n        to: idrClmHdIcn\n      - from: INDEX\n        to: priority\n      - from: idrDiagIcdType\n        transformer: MessageEnum\n        transformerOptions:\n          enumClass: gov.cms.mpsm.rda.v1.mcs.McsDiagnosisIcdType\n          unrecognizedNameSuffix: EnumUnrecognized\n      - from: idrDiagCode\n      - from: NOW\n        to: lastUpdated\n</code></pre> <p>Generate code like this:</p> <pre><code>@Entity\n@Getter\n@Setter\n@Builder\n@AllArgsConstructor\n@NoArgsConstructor\n@EqualsAndHashCode(\n    onlyExplicitlyIncluded = true\n)\n@FieldNameConstants\n@Table(\n    name = \"`McsDiagnosisCodes`\",\n    schema = \"`pre_adj`\"\n)\n@IdClass(PreAdjMcsDiagnosisCode.PK.class)\npublic class PreAdjMcsDiagnosisCode {\n  @Column(\n      name = \"`idrClmHdIcn`\",\n      nullable = false,\n      length = 15\n  )\n  @Id\n  @EqualsAndHashCode.Include\n  private String idrClmHdIcn;\n\n  @Column(\n      name = \"`priority`\",\n      nullable = false\n  )\n  @Id\n  @EqualsAndHashCode.Include\n  private short priority;\n\n  @Column(\n      name = \"`idrDiagIcdType`\",\n      length = 1\n  )\n  private String idrDiagIcdType;\n\n  @Data\n  @NoArgsConstructor\n  @AllArgsConstructor\n  public static final class PK implements Serializable {\n    private String idrClmHdIcn;\n\n    private short priority;\n  }\n}\n</code></pre>","tags":["RFC's"]},{"location":"rfcs/proposed/0014-dsl-driven-rda-code-generation.html#generating-transformation-class","title":"Generating Transformation class","text":"<p>The current code base has a handwritten transformer class for each type of claim returned by the RDA API (i.e. <code>FissClaimTransformer</code> and <code>McsClaimTransformer</code>). These classes contain code to copy RDA API data from protobuf messages into corresponding JPA entities. Field values are copied using a <code>DataTransformation</code> object that provides methods for validating and parsing the individual field values. There is a <code>DataTransformer</code> method for each type of field. These methods support the conventions RDA API uses when mapping its data into protobuf.</p> <p>The plugin generated code follows this same pattern. It generates a transformation class for every mapping that has a <code>transformerClassName</code> value. The transformers contain essentially the same sequence of method calls that a developer would currently write by hand.</p> <p>Each transformation class has one public method that accepts an RDA API message object and a <code>DataTransformer</code> and returns a corresponding entity object. The caller provides the <code>DataTransformer</code> since it collects any validation errors and the caller can query it afterwards to determine if there were any errors.</p> <pre><code>public class FissClaimTransformer2 {\n  private final Function&lt;String, String&gt; idHasher;\n\n  // fields generated by transformer objects inserted here\n\n  public FissClaimTransformer2(Function&lt;String, String&gt; idHasher,\n      EnumStringExtractor.Factory enumExtractorFactory) {\n    this.idHasher = idHasher;\n    // initializers for fields generated by transformer objects inserted here\n    }\n\n  // convenience method that handles creating a DataTransformer and throwing an exception if transformation fails.\n  public PreAdjFissClaim transformMessage(FissClaim from) {\n    final DataTransformer transformer = new DataTransformer();;\n    final PreAdjFissClaim to = transformMessage(from, transformer, Instant.now());\n    if (transformer.getErrors().size() &gt; 0) {\n      throw new DataTransformer.TransformationException(\"data transformation failed\", transformer.getErrors());\n    }\n    return to;\n  }\n\n  public PreAdjFissClaim transformMessage(\n      FissClaim from, DataTransformer transformer, Instant now) {\n    final PreAdjFissClaim to = transformMessageToFissClaim(from,transformer,now,\"\");\n    transformMessageArrays(from,to,transformer,now,\"\");\n    return to;\n  }\n\n  // all the generated private methods to handle individual mappings\n</code></pre> <p>The <code>transformMessageToFissClaim()</code> and <code>transformMessageArrays()</code> methods are private methods created for each mapping. Generally there is one <code>transformMessageTo...</code> method for the parent mapping plus one for each array mapping. There will currently be only one <code>transformMessageArrays</code> method since the RDA API only uses arrays at the top level. But the plugin would be capable of handling arrays in the child objects as well in case that changes in the future.</p>","tags":["RFC's"]},{"location":"rfcs/proposed/0014-dsl-driven-rda-code-generation.html#field-transformations","title":"Field Transformations","text":"<p>A transformation takes data from one field in the input message, validates it, and copies it to a destination field in the entity.</p> <p>Each transformation is implemented as a Java class that implements an interface. The interface contains three methods: - One to get a list of field definitions for any class level fields needed by the transformer. - One to get initialization code for each such field for use in the generated transformer's constructor. - One to generate any java statements needed to perform the transformation.</p> <p>The transformations needed to fully implement the current RDA API and RIF data models include:</p> <ul> <li>Amount: parse and copy a dollar amount string</li> <li>Char: copy a single character into a char field</li> <li>Date: parse and copy a date string</li> <li>EnumValueIfPresent: set an enum column if a specific field is present in the RDA message</li> <li>IdHash: hash and copy a string (MBI hash)</li> <li>Int: copy an integer value</li> <li>IntString: parse and copy an integer string</li> <li>LongString: parse and copy a long int string</li> <li>MessageEnum: extract string value from an enum and copy it</li> <li>RifTimestamp: parse and copy a time stamp in RIF formats</li> <li>String: copy a string</li> <li>Timestamp: copy the current timestamp</li> </ul> <p>The simplest case for a transformer just inserts a single method call:</p> <pre><code>public class TimestampFieldTransformer extends AbstractFieldTransformer {\n  @Override\n  public CodeBlock generateCodeBlock(\n      MappingBean mapping, ColumnBean column, TransformationBean transformation) {\n    return destSetter(column, NOW_VALUE);\n  }\n}\n</code></pre> <p>In this example <code>destSetter</code> is a helper method in the abstract base class to create a code block that sets the value of the destination (entity) field.</p> <p>The most complex transformation would add a field with an <code>EnumStringExtractor</code> object, create code to initialize it appropriately, and code to invoke it to copy the enum's value into an entity field:</p> <pre><code>public class MessageEnumFieldTransformer extends AbstractFieldTransformer {\n  @Override\n  public CodeBlock generateCodeBlock(\n      MappingBean mapping, ColumnBean column, TransformationBean transformation) {\n    final ClassName enumClass =\n        PoetUtil.toClassName(transformation.transformerOption(ENUM_CLASS_OPT).get());\n    CodeBlock.Builder builder = CodeBlock.builder();\n    if (column.isChar()) {\n      builder.addStatement(\n          \"$L.copyEnumAsCharacter($L, $L.getEnumString($L), $L)\",\n          TRANSFORMER_VAR,\n          fieldNameReference(mapping, column),\n          extractorName(mapping, transformation),\n          SOURCE_VAR,\n          destSetRef(column));\n    } else {\n      builder.addStatement(\n          \"$L.copyEnumAsString($L,$L,1,$L,$L.getEnumString($L),$L)\",\n          TRANSFORMER_VAR,\n          fieldNameReference(mapping, column),\n          column.isNullable(),\n          column.computeLength(),\n          extractorName(mapping, transformation),\n          SOURCE_VAR,\n          destSetRef(column));\n    }\n    return builder.build();\n  }\n\n  @Override\n  public List&lt;FieldSpec&gt; generateFieldSpecs(\n      MappingBean mapping, ColumnBean column, TransformationBean transformation) {\n    final ClassName messageClass = PoetUtil.toClassName(mapping.getMessageClassName());\n    final ClassName enumClass =\n        PoetUtil.toClassName(transformation.transformerOption(ENUM_CLASS_OPT).get());\n    FieldSpec.Builder builder =\n        FieldSpec.builder(\n            ParameterizedTypeName.get(\n                ClassName.get(EnumStringExtractor.class), messageClass, enumClass),\n            extractorName(mapping, transformation),\n            Modifier.PRIVATE,\n            Modifier.FINAL);\n    return ImmutableList.of(builder.build());\n  }\n\n  @Override\n  public List&lt;CodeBlock&gt; generateFieldInitializers(\n      MappingBean mapping, ColumnBean column, TransformationBean transformation) {\n    final ClassName messageClass = PoetUtil.toClassName(mapping.getMessageClassName());\n    final ClassName enumClass =\n        PoetUtil.toClassName(transformation.transformerOption(ENUM_CLASS_OPT).get());\n    final boolean hasUnrecognized =\n        transformation\n            .transformerOption(HAS_UNRECOGNIZED_OPT)\n            .map(Boolean::parseBoolean)\n            .orElse(true);\n    CodeBlock initializer =\n        CodeBlock.builder()\n            .addStatement(\n                \"$L = $L.createEnumStringExtractor($L,$L,$L,$L,$T.UNRECOGNIZED,$L,$L)\",\n                extractorName(mapping, transformation),\n                ENUM_FACTORY_VAR,\n                sourceEnumHasValueMethod(messageClass, transformation),\n                sourceEnumGetValueMethod(messageClass, transformation),\n                sourceHasUnrecognizedMethod(hasUnrecognized, messageClass, transformation),\n                sourceGetUnrecognizedMethod(hasUnrecognized, messageClass, transformation),\n                enumClass,\n                unsupportedEnumValues(enumClass, transformation),\n                extractorOptions(transformation))\n            .build();\n    return ImmutableList.of(initializer);\n  }\n</code></pre>","tags":["RFC's"]},{"location":"rfcs/proposed/0014-dsl-driven-rda-code-generation.html#array-transformation-detail-records","title":"Array Transformation (Detail records)","text":"<p>Arrays are recognized and code is generated to transform the array elements appropriately to produce the detail objects for the JPA entities. Detail tables require additional columns containing their parent record's primary key. Additionally, each detail object in the array is assigned a \"priority\" number equal to its array index. The <code>priority</code> is used when sorting the detail records so that their original order in the RDA API message is preserved.</p> <p>The DSL has two special <code>from</code> values (<code>PARENT</code> and <code>INDEX</code>) for this purpose.</p> <ul> <li><code>PARENT</code> tells the transformer to copy the value of the <code>to</code> field from the parent into the detail object.</li> <li><code>INDEX</code> tells the transformer to set the value of the <code>to</code> field to the object's array index.</li> </ul> <p>The generated code for transforming arrays first creates the detail objects and then applies any <code>PARENT</code> or <code>INDEX</code> transformations.</p> <p>Here is a subset of the YAML for a detail object containing a number of fields including <code>dcn</code> (copied from parent) and <code>priority</code> (set to array index):</p> <pre><code>  - id: FissProcCode\n    transformations:\n      - from: PARENT\n        to: dcn\n      - from: INDEX\n        to: priority\n      - from: procCd\n        to: procCode\n        optional: false\n      - from: procFlag\n      - from: procDt\n        to: procDate\n      - from: NOW\n        to: lastUpdated\n</code></pre> <p>Here is the code generated to create the array object, initialize the fields from the parent, and add the object to its parent.</p> <pre><code>  private void transformMessageArrays(FissClaim from, PreAdjFissClaim to,\n      DataTransformer transformer, Instant now, String namePrefix) {\n    for (short index = 0; index &lt; from.getFissProcCodesCount(); ++index) {\n      final String itemNamePrefix = namePrefix + \"procCode\" + \"-\" + index + \"-\";\n      final FissProcedureCode itemFrom = from.getFissProcCodes(index);\n      final PreAdjFissProcCode itemTo = transformMessageImpl(itemFrom,transformer,now,itemNamePrefix);\n      itemTo.setDcn(from.getDcn());\n      itemTo.setPriority(index);\n      to.getProcCodes().add(itemTo);\n    }\n  }\n</code></pre>","tags":["RFC's"]},{"location":"rfcs/proposed/0014-dsl-driven-rda-code-generation.html#proposed-solution-unresolved-questions","title":"Proposed Solution: Unresolved Questions","text":"<p>Collect a list of action items to be resolved or officially deferred before this RFC is submitted for final comment, including:</p> <p>None yet.</p>","tags":["RFC's"]},{"location":"rfcs/proposed/0014-dsl-driven-rda-code-generation.html#proposed-solution-drawbacks","title":"Proposed Solution: Drawbacks","text":"<p>Why should we not do this?</p> <p>Reason 1: Code generators can be complex</p> <p>A case can be made that lots of handwritten code can be more directly comprehensible than a code generator. This is particularly true if the design of the code generator hard codes too many things and embeds too much knowledge of the data it generates code for (e.g. if it adds or looks for fields with specific names that aren't defined in the meta data).</p> <p>Both of these concerns can be addressed by careful design and coding of the plugin. Embedding knowledge of conventions is perfectly OK. That is why the plugin exists: to centralize that knowledge in a reusable component. However embedding knowledge of fields themselves is harmful since it splits knowledge of the fields between the metadata and the plugin source code.</p> <p>Complexity of the plugin can be addressed through design. Use of a strategy pattern for transformations can provide a clear interface and convention for how those work. Adding comments with example output to each section that generates code can make the intent of that code clearer.</p> <p>Reason 2: RDA API Conventions may change</p> <p>Since RDA API is not yet in production won't their conventions change substantially in the near future? That would be a danger either with handwritten code or with a plugin. The plugin centralizes the implementation of those conventions so we can leverage that to simplify adapting to the change. Simply change the plugin and the new conventions apply to all classes and fields automatically.</p> <p>A similar approach has been taken with the handwritten code too. However, though embedding the conventions in library classes and methods is helpful it can still lead to widespread code changes if you need to add a parameter to a library method. Suddenly dozens of lines of code need to be changed by hand to add that new parameter. A code generator can do that sort of thing automatically.</p> <p>Reason 3: A single plugin used in multiple modules implies too much knowledge</p> <p>The code using the plugin is in separate modules for a reason. Won't using a plugin require adding many dependencies to the plugin module that don't make sense there?</p> <p>This can be addressed by defining interfaces that the plugin generated code calls to perform some of its work. Then the module that uses the generated code can implement the interface with whatever extra knowledge it needs. This was done in the proof of concept where the code that actually extracts string values from RDA API enums was called through an interface. The interface was defined in the plugin library and a factory to create a concrete implementation was passed to the constructor of the generated code. This allowed the plugin to generate all of its code without any access to the RDA API stubs themselves.</p>","tags":["RFC's"]},{"location":"rfcs/proposed/0014-dsl-driven-rda-code-generation.html#proposed-solution-notable-alternatives","title":"Proposed Solution: Notable Alternatives","text":"<p>A spreadsheet could be used for the DSL. However, we decided that a YAML file format has several advantages over a spreadsheet for this process: * Existing open source libraries such as jackson can directly convert YAML into java beans. * RDA API and RIF data are inherently hierarchical and YAML naturally supports hierarchical data. * YAML is pure text, so it can be edited from within an IDE and diffs of the file can be reviewed as part of the GitHub PR review process.</p> <p>We considered using java annotation processing but decided that a maven plugin has some advantages: * The maven plugin works directly within the maven build process rather than adding the complexity of java annotation processing. * The same plugin can be invoked from multiple modules to generate different portions of the code exactly where it is needed. * Maven plugins interact more seamlessly with IDEs.</p> <p>We considered defining a full-blown imperative DSL using groovy or something else but: * Writing transformations in java fits more naturally into the BFD code base and team expertise. * Declarative structure allows the plugin to guarantee adherence to the RDA API conventions and proper code review. (i.e. no cheats or workarounds can be inserted as code in the DSL file) * Transformations implemented in java within the plugin have a standard structure that makes them easier to develop and debug.</p> <p>We considered using a dynamic transformation system rather than a code generator. The same metadata could be used to configure a class at runtime to perform all of the same transformations on the fly. This idea would have the advantage of eliminating the need for a maven plugin since a single class in a java library could dynamically perform all of the same work as the generated code. There are downsides to this approach though: * It would not work for creating entity classes or SQL migration files. * A fully dynamic object would have a performance penalty compared to compiled code.  For example with generated code the java JIT could determine that specific code branches are unused in the transformation classes for one of the claim types but always used for a different one. Since we will be processing millions of messages per day this could become a bottleneck or increase CPU resource requirements. * Breakpoints can be set in specific places in the generated code to see what's happening if a bug is encountered.   Dynamic code is more general and isolating a problem to a specific field can be more difficult.  (e.g. skipping past the first 30 fields in your debugger to see what happens in the field you actually care about can be painful.)</p> <p>Continuing with the existing hard-written code would have a number of disadvantages: * It would make it more difficult to react to changes in the RDA API going forward. * It would complicate PR reviews for RDA API changes since multiple files would have to be reviewed for each change. * Changes in conventions by the RDA API team would require changing logic in multiple places rather than just in the plugin.  An example of this would be if they changed how they map enums to strings in their responses.</p>","tags":["RFC's"]},{"location":"rfcs/proposed/0014-dsl-driven-rda-code-generation.html#prior-art","title":"Prior Art","text":"<p>The RIF entities are currently generated using java annotation processing and reading metadata from a spreadsheet. The proposed plugin would replace this.</p>","tags":["RFC's"]},{"location":"rfcs/proposed/0014-dsl-driven-rda-code-generation.html#future-possibilities","title":"Future Possibilities","text":"<p>The plugin can be adapted as new ways of using the RDA API data appear over time. A similar approach could be used in the future to consume different types of APIs or data. For example data from a REST API or a different file format could be handled along similar lines.</p>","tags":["RFC's"]},{"location":"rfcs/proposed/0014-dsl-driven-rda-code-generation.html#addendums","title":"Addendums","text":"<p>The following addenda are required reading before voting on this proposal:</p> <ul> <li>(none at this time)</li> </ul> <p>Please note that some of these addenda may be encrypted. If you are unable to decrypt the files, you are not authorized to vote on this proposal.</p>","tags":["RFC's"]},{"location":"rfcs/rejected/0006-or-condition.html","title":"RFC Proposal","text":"<ul> <li>RFC Proposal ID: <code>0006-or-condition</code></li> <li>Start Date: 2020-08-21</li> <li>RFC PR: https://github.com/CMSgov/beneficiary-fhir-data/pull/344</li> <li>JIRA Ticket(s):<ul> <li>https://jira.cms.gov/browse/AB2D-1862</li> </ul> </li> </ul> <p>This proposal will suggest adding an OR clause to queries that gather the contract to patient mapping in order to improve efficiency on both the client and server.</p>","tags":["RFC's"]},{"location":"rfcs/rejected/0006-or-condition.html#status","title":"Status","text":"<ul> <li>Status: Rejected (This RFC was previously approved but after further discussion within the BFD team it has been retroactively rejected because the parameter passing is inappropriate. If this functionality is still required a new approach should be discussed and the RFC updated and re-reviewed.)</li> <li>Implementation JIRA Ticket(s):<ul> <li>N/A</li> </ul> </li> </ul>","tags":["RFC's"]},{"location":"rfcs/rejected/0006-or-condition.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>RFC Proposal</li> <li>Status</li> <li>Table of Contents</li> <li>Motivation</li> <li>Proposed Solution<ul> <li>Proposed Solution: Detailed Design</li> <li>Proposed Solution: Unresolved Questions</li> <li>Proposed Solution: Drawbacks</li> <li>Proposed Solution: Notable Alternatives</li> </ul> </li> <li>Future Possibilities</li> </ul>","tags":["RFC's"]},{"location":"rfcs/rejected/0006-or-condition.html#motivation","title":"Motivation","text":"<p>AB2D has long running export jobs where the amount of time the job takes is critical for customer happiness. In order to lessen the amount of time a job takes to run, we will need to be able to improve performance in every section of our code. One part that takes a long time is the call to the Patient URL that maps contracts to beneficiaries on a monthly basis. Being able to query multiple months in the same call will reduce the load on the BFD database since it will reduce the number of calls needed to it, and be more efficient since we will reduce the number of API calls we need to make. It is more efficient on the server side to make 1 call to get the data, than it is to make 6 different calls that gather the same amount of data.</p>","tags":["RFC's"]},{"location":"rfcs/rejected/0006-or-condition.html#proposed-solution","title":"Proposed Solution","text":"<p>The solution will allow a client to specify any number of months that they want to be passed to the BFD server. The BFD server will be able to parse what months the client is asking for, and then return all of the months at once, instead of having the client make individual requests.</p>","tags":["RFC's"]},{"location":"rfcs/rejected/0006-or-condition.html#proposed-solution-detailed-design","title":"Proposed Solution: Detailed Design","text":"<p>PatientResourceProvider.queryBeneficiaryIds currently takes in just one field, and one value. This code should be changed so that it takes in a list of objects. The new object would just be a simple value class that can hold a field and value</p> <p>PatientResourceProvider.queryBeneficiaryIds can take in an object like so: // Assume getters and setters class FieldValue {     private String field;     private String value; }</p> <p>https://bluebutton.cms.gov/resources/variables/ptdcntrct01 is the current variable for how data is sent over from AB2D for the month of Jan. to the BFD server</p> <p>When it is used with an identifier, it is combined with a contract, e.g. https://bluebutton.cms.gov/resources/variables/ptdcntrct01|Z0001</p> <p>In order to send over multiple months using the FHIR client libraries, the systemAndIdentifier method takes 2 strings, the first being the one we're interested in (the system). Months can be separated by a delimiter, so that multiple months can be included, e.g. https://bluebutton.cms.gov/resources/variables/ptdcntrct01-02-03</p> <p>This value can be parsed by BFD to determine if there should be an or parameter in the event there's a list of items sent over. One month would mean that there's no need for a month parameter. The SQL query in queryBeneficiaryIds can provide an OR clause when necessary.</p>","tags":["RFC's"]},{"location":"rfcs/rejected/0006-or-condition.html#proposed-solution-unresolved-questions","title":"Proposed Solution: Unresolved Questions","text":"<p>Before a solution goes into place, how can we validate that this actually does help performance? Performance testing this with real world examples, e.g. comparing AB2D performance querying 6 months separately vs. 1 query with 6 months.</p> <p>Will other groups use this solution?</p>","tags":["RFC's"]},{"location":"rfcs/rejected/0006-or-condition.html#proposed-solution-drawbacks","title":"Proposed Solution: Drawbacks","text":"<p>This may not work if it becomes a pain to parse apart multiple months, but it shouldn't be much of an issue, since there will be a large gain from being able to query for multiple months at once, instead of doing them one at a time. If clients abuse this and query more months than what they need, that would be an issue, but any client could abuse an API.</p>","tags":["RFC's"]},{"location":"rfcs/rejected/0006-or-condition.html#proposed-solution-notable-alternatives","title":"Proposed Solution: Notable Alternatives","text":"<p>We have to make a design fit within the FHIR spec, so unfortunately there's not too much room for alternatives. This solution tries to make as few changes as possible to the code on BFD's side so that it's a simple one that can utilize the existing system for using a month parameter, and simply expand it to optionally use additional months.</p>","tags":["RFC's"]},{"location":"rfcs/rejected/0006-or-condition.html#future-possibilities","title":"Future Possibilities","text":"<p>There could also be an \"or\" condition for parameters beyond months, but those would come on a case by case basis.</p>","tags":["RFC's"]},{"location":"runbooks/index.html","title":"BFD Runbooks","text":""},{"location":"runbooks/bfd-db-migrator/recover-from-failures.html","title":"How to Recover from <code>bfd-db-migrator</code> Failures","text":"<p>Follow this runbook if the the deployment of <code>migrator</code> fails pre-deployment checks or results in a non-zero (0) exit status.</p> <p>The <code>migrator</code> only runs during the course of a deployment, and this runbook covers troubleshooting steps for application and non-application failures that we may encounter during a deployment.</p>"},{"location":"runbooks/bfd-db-migrator/recover-from-failures.html#failures","title":"Failures","text":""},{"location":"runbooks/bfd-db-migrator/recover-from-failures.html#undeployable-state","title":"Undeployable State","text":"<p>The failure modes impacting the deployability of the migrator are numerous and may include problems with AWS IAM permissions, network access, other outages in the AWS environment, availability of the Cloudbees Jenkins deployment services, and lingering issues in the logic serving the various Jenkinsfile resources, and supporting global libraries.</p>  <p>Note</p> <p>The troubleshooting steps below only deals with the most common error where errant messages persist in the SQS queue used for signaling between the BFD environments and Cloudbees Jenkins. For other areas of investigation, please see the References section for a list of links to resources that might assist in investigating errors not covered here.</p>  <p>The CI process failed to even attempt deployment of the migrator, resulting in message like one or more of the following:</p>  <p>Queue bfd-${env}-migrator has messages. Is there an old bfd-db-migrator instance running? Migrator deployment cannot proceed.</p> <p>Halting Migrator Deployment. Check the SQS Queue bfd-${env}-migrator.</p>  <p>Causes might include: - resources from a previously failed deployment that wasn't cleaned up - untidy development practices involving a migrator deployment - other out-of-band deployment activity</p> <p>NOTE: Operators must verify that there is no ongoing migration under way before troubleshooting or attempting re-deployment. At the time of this writing, the EC2 instance should only exist for as long as it takes to execute the necessary migration(s) unless there is a failure. If a bfd-migrator EC2 instance exists at the time of deployment or persists after the deployment, this is anomalous.</p>"},{"location":"runbooks/bfd-db-migrator/recover-from-failures.html#performance-steps","title":"Performance Steps","text":"More...  1. Identify the problematic AWS SQS Queue from the relevant Jenkins log message, e.g. `bfd-${env}-migrator` 2. Fetch messages if they exist     - through the console          1. navigate to the [SQS panel](https://us-east-1.console.aws.amazon.com/sqs/v2/home?region=us-east-1#/queues)         2. note the available messages count from this interface         3. if there are more than 0 messages             1. select the appropriate SQS queue `bfd-${env}-migrator`, e.g. `bfd-test-migrator`             2. from the specific queue's interface, select `Send and receive messages`             3. select `Poll for Messages`                - through the cli          If the following returns nothing, there are no messages available:          <pre><code>queue_name=bfd-test-migrator # CHANGE AS NECESSARY\nurl=\"$(aws sqs get-queue-url --queue-name \"$queue_name\" --region us-east-1 --output text)\"\naws sqs receive-message --region us-east-1 --queue-url \"$url\"\n</code></pre>   3. Read messages     - if there is a message that contains a normal exit with a zero (0) return code, continue with step 4     - if there are messages that contains a non-zero return code, continue to troubleshoot with the instructions for the corresponding code:         - [for return code 1](#invalid-app-configuration-1)         - [for return code 2](#migration-failed-2)         - [for return code 3](#validation-failed-3)     - if there are no messages containing return codes **AND** a migrator instance exists, complete the following steps and continue to step 4:         - Ensure the migrator is not running              1. Identify the bfd-migrator instance for the environment             2. Connect to the EC2 instance via SSH             3. Inspect output of `sudo systemctl status bfd-db-migrator`             4. Ensure there is no active migration `sudo systemctl stop bfd-db-migrator`             5. _Optionally_ inspect logs: `view /bluebutton-data-pipeline/bluebutton-data-pipeline.log`                4. Purge the AWS SQS Queue of **all** messages      - through the console          1. navigate to the [SQS panel](https://us-east-1.console.aws.amazon.com/sqs/v2/home?region=us-east-1#/queues)         2. select the appropriate SQS queue `bfd--migrator`, e.g. `bfd-test-migrator`         3. next, select the `Purge` action         4. in the dialogue, enter `purge` and confirm with by selecting `Purge`                - through the cli <pre><code>queue_name=bfd-test-migrator # CHANGE AS NECESSARY\nurl=\"$(aws sqs get-queue-url --queue-name \"$queue_name\" --region us-east-1 --output text)\"\naws sqs purge-queue --region us-east-1 --queue-url \"$url\"\n</code></pre>   5. Attempt to re-deploy from Jenkins 6. If failures persist    - further scrutinize the Jenkins logs for errors leading up to the migrator deployment    - verify AWS IAM permissions for the `cloudbees-jenkins` role and inspect the [AWS CloudTrails Errors Dashboard][splunk-cloudtrail-errors] in splunk    - ensure AWS isn't reporting any open issues in the [health dashboard](https://health.aws.amazon.com/health/home#/account/dashboard/open-issues)"},{"location":"runbooks/bfd-db-migrator/recover-from-failures.html#invalid-app-configuration-1","title":"Invalid App Configuration (1)","text":"<p>The application failed to resolve full, error-free configuration for the given environment and provides the message:</p>  <p>Migrator completed with exit status 1</p>  <p>Common causes might include: - permissions, connectivity problems in resolution of AWS SSM Parameter Store configuration values - permissions, connectivity problems in accessing the AWS Key Management Service (KMS) Customer Managed Key (CMK) - erroneous or missing configuration from the <code>base</code> module - erroneous [<code>ansible</code>][migrator-ansible-role] or [<code>terraform</code>][migrator-terraform] templates</p>"},{"location":"runbooks/bfd-db-migrator/recover-from-failures.html#performance-steps_1","title":"Performance Steps","text":"More...  1. Identify the bfd-migrator instance for the environment 2. Connect to the EC2 instance via SSH 3. Inspect the contents of the cloud-init `/var/lib/cloud/instance/user-data.txt` script 4. Attempt to re-apply: `sudo bash /var/lib/cloud/instance/user-data.txt`; Troubleshoot.     - Search for errors in `/var/log/cloud-init.log` and `/var/log/cloud-init-output.log`     - Inspect the resultant `/opt/bfd-db-migrator/bfd-db-migrator-service.sh` for format errors     - Verify AWS SSM Parameter Store Access         - Evaluate permission: `aws sts get-caller-identity`         - Verify: `aws ssm get-parameters-by-path --path /bfd/${env}/pipeline`         - Verify: `aws ssm get-parameters-by-path --path /bfd/${env}/common/nonsensitive`     - Inspect resultant cloud-init derived extra variables files:         - /beneficiary-fhir-data/ops/ansible/playbooks-ccs/common_vars.json         - /beneficiary-fhir-data/ops/ansible/playbooks-ccs/extra_vars.json         - /beneficiary-fhir-data/ops/ansible/playbooks-ccs/pipeline_vars.json     - Inspect output of `sudo systemctl status bfd-db-migrator` and `sudo systemctl migrator-monitor` for errors 5. Make adjustments as necessary:     - AWS IAM Policies attached to `bfd--migrator`     - Likely Ansible Configuration Files:         - /beneficiary-fhir-data/ops/ansible/playbooks-ccs/launch_bfd-db-migrator.yml         - /beneficiary-fhir-data/ops/ansible/roles/bfd-db-migrator/templates/bfd-db-migrator.service.j2         - /beneficiary-fhir-data/ops/ansible/roles/bfd-db-migrator/templates/bfd-db-migrator-service.sh.j2         - /beneficiary-fhir-data/ops/ansible/roles/bfd-db-migrator/templates/migrator-monitor.service.j2         - /beneficiary-fhir-data/ops/ansible/roles/bfd-db-migrator/templates/migrator-monitor.sh.j2 6. Record all adjustments in a new branch and seek PR feedback from BFD engineers as necessary. 7. Re-deploy with latest changes from Jenkins"},{"location":"runbooks/bfd-db-migrator/recover-from-failures.html#migration-failed-2","title":"Migration Failed (2)","text":"<p>The application failed to apply the required flyway migration script or scripts to the appropriate database and provides the following message:</p>  <p>Migrator completed with exit status 2</p>  <p>Causes might include: - development version of a migration was inappropriately applied to an environment's database - other out-of-band changes were applied to the RDS cluster resulting in mismatching schema version or schema version hashes - tests failed to identify an error only impacting AWS Aurora PostgreSQL, but   - succeed against HSQLDB (uncommon)   - success against non-Aurora PostgreSQL (rare)</p>"},{"location":"runbooks/bfd-db-migrator/recover-from-failures.html#performance-steps_2","title":"Performance Steps","text":"More...  1. Identify the bfd-migrator instance for the environment 2. Identify the error message(s) from the logs     - through the instance          1. navigate to the appropriate CloudWatch Panel              - [test][migrator-cw-logs-test]             - [prod-sbx][migrator-cw-logs-prod-sbx]             - [prod][migrator-cw-logs-prod]             - [all migrator log groups][migrator-cw-logs-all]          2. select the appropriate instance (typically the top instance)                - through the console          1. SSH to the instance         2. `view /bluebutton-data-pipeline/bluebutton-data-pipeline.log`            3. Support migration author in resolving the migration error with PR feedback, deployment support, etc 4. Purge SQS Message Queue as necessary before the next deployment (purge instructions [found here](#undeployable-state))    ### Validation Failed (3)  **NOTE**: _This should be exceedingly rare. If you happen to experience this, please expand this section with any helpful details that assisted in your remediation efforts._  The application failed to validate the Hibernate models against the currently applied schema version and provides the following message: &gt; Migrator completed with exit status 3  #### Performance Steps  More...  1. Identify the bfd-migrator instance for the environment 2. Identify the error message(s) from the logs     - through the instance          1. navigate to the appropriate CloudWatch Panel              - [test][migrator-cw-logs-test]             - [prod-sbx][migrator-cw-logs-prod-sbx]             - [prod][migrator-cw-logs-prod]             - [all migrator log groups][migrator-cw-logs-all]          2. select the appropriate instance (typically the top instance)                - through the console          1. Connect to the EC2 instance via SSH         2. `view /bluebutton-data-pipeline/bluebutton-data-pipeline.log`            3. Support migration author in resolving the validation error with PR feedback, deployment support, etc 4. Purge SQS Message Queue as necessary before the next deployment (purge instructions [found here](#undeployable-state))    ## References - [Original Migrator RFC Document][migrator-rfc] - [Migrator Java Artifact Source Code][migrator-java-source] - [Monolithic Packer Manifest][migrator-packer-manifest] - [Migrator Terraform][migrator-terraform]     - [Migrator cloud-init user-data Template][migrator-cloud-init]     - [Migrator-Specific Jenkins Deployment Logic][migrator-jenkins] - [Migrator Ansible Role][migrator-ansible-role] - [Migrator Ansible Playbook][migrator-ansible-playbook] - [Jenkins Global Pipeline Libraries][jenkins-global-pipeline-libraries] - [Migrator Cloud Watch Logs][migrator-cw-logs-all]     - [test][migrator-cw-logs-test]     - [prod-sbx][migrator-cw-logs-prod-sbx]     - [prod][migrator-cw-logs-prod] - [Splunk CloudTrail Errors][splunk-cloudtrail-errors] _(CMS VPN Required)_   [migrator-rfc]: https://github.com/CMSgov/beneficiary-fhir-data/blob/master/rfcs/0011-separate-flyway-from-pipeline.md \"Original Migrator RFC Document\" [migrator-java-source]: https://github.com/CMSgov/beneficiary-fhir-data/tree/master/apps/bfd-db-migrator \"Migrator Java Artifact Source Code\" [migrator-packer-manifest]: https://github.com/CMSgov/beneficiary-fhir-data/blob/master/ops/packer/build_bfd-all.json \"Monolithic Packer Manifest\" [migrator-terraform]: https://github.com/CMSgov/beneficiary-fhir-data/tree/master/ops/terraform/services/migrator \"Migrator Terraform\" [migrator-cloud-init]: https://github.com/CMSgov/beneficiary-fhir-data/blob/master/ops/terraform/services/migrator/user-data.tftpl \"Migrator cloud-init user-data Template\" [migrator-jenkins]: https://github.com/CMSgov/beneficiary-fhir-data/blob/master/ops/terraform/services/migrator/deploy.groovy \"Migrator-Specific Jenkins Deployment Logic\" [migrator-ansible-role]: https://github.com/CMSgov/beneficiary-fhir-data/tree/master/ops/ansible/roles/bfd-db-migrator \"Migrator Ansible Role\" [migrator-ansible-playbook]: https://github.com/CMSgov/beneficiary-fhir-data/blob/master/ops/ansible/playbooks-ccs/launch_bfd-db-migrator.yml \"Migrator Ansible Playbook\" [jenkins-global-pipeline-libraries]: https://github.com/CMSgov/beneficiary-fhir-data/tree/master/ops/jenkins/global-pipeline-libraries \"Jenkins Global Pipeline Libraries\"  [migrator-cw-logs-test]: https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logsV2:log-groups/log-group/$252Fbfd$252Ftest$252Fbfd-db-migrator$252Fmigrator-log.json \"TEST Migrator CloudWatch Logs\" [migrator-cw-logs-prod-sbx]: https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logsV2:log-groups/log-group/$252Fbfd$252Ftest$252Fbfd-db-migrator$252Fmigrator-log.json \"PROD-SBX Migrator CloudWatch Logs\" [migrator-cw-logs-prod]: https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logsV2:log-groups/log-group/$252Fbfd$252Ftest$252Fbfd-db-migrator$252Fmigrator-log.json \"PROD Migrator CloudWatch Logs\" [migrator-cw-logs-all]: https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logsV2:log-groups$3FlogGroupNameFilter$3Dmigrator \"ALL Migrator CloudWatch Logs\"  [splunk-cloudtrail-errors]: https://splunk.cloud.cms.gov/en-US/app/cms_oeda_bfd_landing_page/cloudtrailerrors0 \"Splunk CloudTrail Errors\""},{"location":"runbooks/bfd-insights/firehose-failures.html","title":"How to Investigate Firehose Ingestion Processing Failures","text":"<p>Follow this runbook to investigate processing failures from Amazon Kinesis Data Firehose Delivery Stream ingestion. As of writing, this refers specifically to the <code>bfd-insights-bfd-ENV-cw-to-flattened-json</code> Lambda(s), but may include others that submit data to a Firehose Delivery Stream in the future as well.</p> <ul> <li>How to Investigate Firehose Ingestion Processing Failures</li> <li>Glossary</li> <li>FAQ<ul> <li>How would \"processing failures\" occur?</li> <li>Has there ever been processing failures?</li> </ul> </li> <li>Prerequisites</li> <li>Instructions</li> </ul>"},{"location":"runbooks/bfd-insights/firehose-failures.html#glossary","title":"Glossary","text":"Term Definition     Amazon Kinesis Data Firehose An ETL (extract, transform, load) service for capturing data and delivering data to various stores   Firehose Delivery Stream The \"underlying entity\" of Kinesis. This is the component of Firehose that data is sent to"},{"location":"runbooks/bfd-insights/firehose-failures.html#faq","title":"FAQ","text":""},{"location":"runbooks/bfd-insights/firehose-failures.html#how-would-processing-failures-occur","title":"How would \"processing failures\" occur?","text":"<p>In this process, the Lambda that transforms the incoming data before submitting it to the Delivery Stream is responsible for determining whether the incoming data is able to be processed or not. As of writing there is only a single Lambda used by BFD Insights that transforms, or processes, data prior to Delivery Stream submission, and this Lambda's source code is available (relative to repository root) at <code>insights/terraform/projects/bfd/api-requests/lambda_src/bfd-cw-to-flattened-json.py</code>. This Lambda is based upon the AWS-provided <code>kinesis-firehose-cloudwatch-logs-processor-python</code> Lambda Blueprint and it is likely that future BFD Insights Lambdas will be based upon the very same Blueprint.</p> <p>So, for Lambdas based upon the <code>kinesis-firehose-cloudwatch-logs-processor-python</code> Lambda Blueprint, the only time when a processing failure could occur is when a single CloudWatch log record exceeds 6 MB in size. Note that collections of log records greater than 6 MB will be automatically split and chunked before submission to the Delivery Stream.</p> <p>For custom Lambdas, or for those we have modified heavily from the source Blueprint, their source code may need to be investigated to understand when a processing failure could occur.</p>"},{"location":"runbooks/bfd-insights/firehose-failures.html#has-there-ever-been-processing-failures","title":"Has there ever been processing failures?","text":"<p>Yes, as of writing we have had processing failures occur twice since the creation of the <code>bfd-insights-bfd-ENV-cw-to-flattened-json</code> Lambda and corresponding Delivery Stream. The first such batch of processing failures occurred on (MM-DD-YYYY) 09-29-2022, and the second batch occurred on 10-13-2022. Both were the result of using an outdated <code>kinesis-firehose-cloudwatch-logs-processor-python</code> Lambda Blueprint as the base of the <code>bfd-insights-bfd-ENV-cw-to-flattened-json</code> Lambda(s) that did not have any logic for automatically splitting and chunking log record collections greater than 6 MBs. Instead, collections greater than 6 MBs in size would automatically get marked as <code>processing-failed</code> and would not be ingested by the Delivery Stream.</p> <p>Subsequently, the <code>bfd-insights-bfd-ENV-cw-to-flattened-json</code> Lambda(s) have been updated to use the latest <code>kinesis-firehose-cloudwatch-logs-processor-python</code> AWS Blueprint which does support automatic chunking of large data, and so it is not expected that we will encounter further processing failures in the same vein.</p>"},{"location":"runbooks/bfd-insights/firehose-failures.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to the BFD/CMS AWS account</li> <li>Permissions to access and download files from the Delivery Stream's S3 Bucket destination</li> <li>Familiarity with using command-line interface (CLI) tools</li> <li>Installation of the <code>base64</code> CLI tool, or equivalent</li> <li>By default, MacOS provides a similar utility with the same name and usage</li> <li>Installation of the <code>gzip</code> CLI tool, or equivalent</li> </ul>"},{"location":"runbooks/bfd-insights/firehose-failures.html#instructions","title":"Instructions","text":"<ol> <li>Finding the S3 bucket and error output path:</li> <li>In any browser, navigate to https://aws.amazon.com and sign-in. The AWS starting page should       load, and a searchbar should be visible at the top of the page</li> <li>In the searchbar, search for \"kinesis\" and select the Kinesis service when it appears. A new       page should with three cards listing \"Data Streams\", \"Data Firehose\", and \"Data Analytics\"</li> <li>Under \"Data Firehose\", click the number under \"Total delivery streams\". This will take you to       a new page listing all of the Delivery Streams under our AWS account</li> <li>In the list of Delivery Streams, click on the matching Delivery Stream that needs to be       investigated. A new page should load showing information about the Delivery Stream<ol> <li>For example, the Delivery Stream that corresponds with the      <code>bfd-insights-bfd-prod-cw-to-flattened-json</code> Lambda is the      <code>bfd-insights-bfd-prod-firehose-ingester</code> Delivery Stream</li> </ol> </li> <li>Click on \"Configuration\"</li> <li>Under \"Destination settings\", note down the S3 bucket and the S3 bucket error output prefix.       This will be the destination where processing failures will be written to<ol> <li>Note that, for this case, <code>!{firehose:error-output-type}</code> will become <code>processing-failed</code>      in the resulting error prefix</li> </ol> </li> <li>Now that you have the S3 bucket and path where the processing failures are stored, you can    download the failed files from said bucket and path. This runbook will not go into detail how to    do this, so consult AWS's documentation if your are unsure of how to download files from an S3    bucket. Subsequent steps assume you have downloaded these files to your local machine</li> <li> <p>Open the downloaded file(s) in any text editor</p> </li> <li> <p>Note that these files are large, so editors like <code>vim</code> or (Windows-only) Notepad++ would be       best suited to viewing them.</p> </li> <li> <p>Each line of each file is a JSON object with 7 properties, and each object should have the       following structure (note that the following JSON has been formatted):</p> <pre><code>{\n  \"attemptsMade\": 4,\n  \"arrivalTimestamp\": 1665653487107,\n  \"errorCode\": \"Lambda.FunctionError\",\n  \"errorMessage\": \"The Lambda function was successfully invoked but it returned an error result.\",\n  \"attemptEndingTimestamp\": 1665678887550,\n  \"rawData\": \"H4sIA...\",\n  \"lambdaArn\": \"...\"\n}\n</code></pre> </li> <li> <p>Copy all of the <code>rawData</code> string (excluding quotes) and paste it into a new file. Save this    file as something meaningful</p> </li> <li>This runbook assumes this file is named <code>data.base64</code></li> <li><code>rawData</code> is the <code>gzip</code>'d <code>base64</code>-encoded raw incoming data that the Lambda attempted to       process, but failed to do so. Subsequent steps will decode and decompress this \"raw data\" into       a JSON file</li> <li>Open a terminal to the directory where the file from Step 4 is located</li> <li>In the terminal, run the following command (replacing <code>data.base64</code> and <code>data.json</code> with the name    you gave the file from Step 4):</li> </ol> <pre><code>cat data.base64 | base64 -d | gzip -d &gt; data.json\n</code></pre> <ol> <li><code>data.json</code> (replace \"data\" with the equivalent name from Step 4) should now contain a JSON       object following the following structure (assuming the Lambda is processing CloudWatch Logs):</li> </ol> <pre><code> {\n \"messageType\": \"DATA_MESSAGE\",\n \"owner\": \"...\",\n \"logGroup\": \"...\",\n \"logStream\": \"...\",\n \"subscriptionFilters\": [\"...\"],\n \"logEvents\": [\n   {\n     \"id\": \"...\",\n     \"timestamp\": 1664471296685,\n     \"message\": \"...\"\n   },\n   ...\n</code></pre> <ol> <li>Repeat Steps 3 - 6 for each file and each JSON object in the file that you want to investigate</li> </ol> <p>With the <code>rawData</code> now decompressed and decoded, you can investigate the data that the Lambda attempted to process and determine if it was malformed, too large, etc. since the <code>rawData</code> JSON is the data that caused the failure. For example, <code>rawData</code> JSON for CloudWatch logs will contain records, in the <code>logEvents</code> array, for each CloudWatch log event including the raw message.</p>"},{"location":"runbooks/bfd-insights/load-cw-logs.html","title":"Load cw logs","text":"<p>Follow this runbook to load historical production access log data into BFD insights.</p> <p>There are three Glue tables that are referenced in this runbook: - <code>export</code> table - a table that is built on top of the raw export data from Cloudwatch - <code>staging</code> table - a non-partitioned table that has the desired column structure and can be loaded from the <code>export</code> table efficiently - <code>target</code> table - the partitioned table that is the final destination for the data (should already exist via terraform)</p> <p>This runbook should be executed after the Kinesis Firehose has started to populate data into the <code>target</code> table.</p> <ol> <li>Export the data from Cloudwatch.</li> <li>Review the exports that are available already in the export location: s3://bfd-insights-bfd-app-logs/export/prod/       to see if the needed data has been exported previously. New exports should be contiguous and non-overlapping with       existing exports. Start and end timestamps should be selected to run from the first day of a month at 00:00:00 UTC       until the first day of a subsequent month at 00:00:00 UTC (exporting more than one month at a time is advisable).       Removing an existing export in favor of exporting that       data again with a later end date is an acceptable way to keep the number of exports manageable. The last export       chronologically should have a small overlap of 12 hours with the data that is populated with Firehose. This       overlap will be accounted for when populating the <code>staging</code> table.</li> <li>Navigate to Cloudwatch in the AWS console.</li> <li>Select <code>Log Groups</code></li> <li>Select <code>/bfd/prod/bfd-server/access.json</code></li> <li>Select <code>Actions -&gt; Export Data to Amazon S3</code></li> <li>Choose the time period </li> <li>Select Account: This Account</li> <li>S3 Bucket Name: <code>bfd-insights-bfd-app-logs</code></li> <li>S3 Bucket Prefix: <code>export/prod/YYYY_(MM-MM)</code> (For example, export/prod/2022_(01-06) for the data that spans from        2022-01-01 00:00:00 through 2022-07-01 00:00:00)</li> <li>Run the job. Production exports can take up to an hour per month depending on the activity level that month.</li> <li> <p>When the job completes, remove the <code>aws-logs-write-test</code> subfolder that AWS creates for internal testing.</p> </li> <li> <p>Create/Update the Glue <code>export</code> table.</p> </li> <li>From the AWS Glue console, select Crawlers</li> <li>Select the <code>bfd_cw_export</code> crawler (this crawler is in terraform and should already exist)</li> <li>Select <code>Run</code></li> <li>Verify that the <code>bfd_cw_export.prod</code> table has the expected data by inspecting the results of this query that       retrieves the number of records per month and ensuring that the newly exported data is represented:</li> </ol> <pre><code>select date_format(from_iso8601_timestamp(\"timestamp\"), '%Y-%m'), count(*)\nfrom bfd_cw_export.prod\ngroup by 1\norder by 1\n</code></pre> <ol> <li>Determine the column list for the <code>staging</code> table.</li> <li> <p>The column list for the <code>staging</code> table should be identical in names, data types, and ordering to the <code>target</code>       table after removing the <code>year</code> and <code>month</code> partition columns. The column definition for the <code>target</code> table can be       retrieved by navigating to the table in the AWS Glue console, selecting <code>Actions</code> and then <code>View properties</code> which       makes a JSON schema for the table available which includes the ordered list of columns. The column names can be       extracted from this file and should resemble the list below. Only include the column names from the <code>Columns</code>       array -- do not include <code>PartitionKeys</code>.</p> <p>Sample output:   <pre><code>cw_timestamp\ncw_id\ntimestamp\nlevel\nthread\nlogger\nmessage\ncontext\nmdc_bene_id\nmdc_database_query_bene_by_coverage_batch\n... &lt;over 200 additional mdc columns&gt;\n</code></pre>    2. The column list for the <code>staging</code> table is constructed from the output from step 2 adding the column type (all   columns are of string type).    <pre><code>   cw_timestamp string,\n   cw_id string,\n   timestamp string,\n   level string,\n   thread string,\n   logger string,\n   message string,\n   context string,\n   mdc_bene_id string,\n   mdc_database_query_bene_by_coverage_batch string,\n   ... &lt;all other mdc columns in order&gt;\n</code></pre></p> </li> <li> <p>Create a non-partitioned Parquet Glue table to serve as the <code>staging</code> table.    <pre><code>CREATE EXTERNAL TABLE prod_staging (\n   cw_timestamp string, -- Cloudwatch timestamp\n   cw_id string,        -- Cloudwatch ID, set to null for historical loads\n   timestamp string,    -- BFD timestamp\n   level string,        -- Fields from BFD JSON from here on down\n   thread string,\n   logger string,\n   message string,\n   context string,\n   mdc_bene_id string,\n   mdc_database_query_bene_by_coverage_batch string,\n   ... &lt;all other mdc columns in order&gt;\n) STORED AS parquet\nLOCATION 's3://bfd-insights-bfd-&lt;account-id&gt;/databases/bfd_cw_export/prod_staging/'\n</code></pre></p> </li> <li> <p>Load the <code>staging</code> table from the <code>export</code> table.    The <code>staging</code> table must be loaded in batches of no more than ~300 million records to avoid hitting the 30 minute    Athena timeout. This is accomplished by running the following statement with different where clauses in the with    clause that load a portion of the data each time. Note that in order to avoid duplication with the running    firehose, the most recent export should have a small overlap with the firehose data and include a where clause    that performs de-duplication against the <code>target</code> table. The where clauses for the initial load are included below,    commented out, with the elapsed time to load as a comment.    <pre><code>insert into prod_staging\nwith dataset as (\nselect\n   timestamp as \"cw_timestamp\",\n   null as \"cw_id\",\n   json_extract_scalar(message,'$.timestamp') as \"timestamp\",\n   json_extract_scalar(message,'$.level') as \"level\",\n   json_extract_scalar(message,'$.thread') as \"thread\",\n   json_extract_scalar(message,'$.logger') as \"logger\",\n   json_extract_scalar(message,'$.message') as \"message\",\n   json_extract_scalar(message,'$.context') as \"context\",\n   transform_keys(cast(json_extract(message, '$.mdc') as MAP(VARCHAR, VARCHAR)), (k, v) -&gt; replace(lower(k), '.', '_')) as stuff\nfrom prod\n --where partition_0 = '2019_(01-12)'                      --  3m30s, 5.84GB\n --where partition_0 = '2020_(01-12)'                      --  9m13s, 32.58GB\n --where partition_0 = '2021_(01-06)'                      -- 14m14s, 54.34GB\n --where partition_0 = '2021_(07-12)'                      --  7m40s, 28.03GB\n --where partition_0 = '2022_(01-05)'                      -- 13m21s, 52.49GB\n --where partition_0 = '2022_(06-0903)'\n --and month(from_iso8601_timestamp(\"timestamp\")) &lt;= 6   --  9m20s, 92.13GB\n --where partition_0 = '2022_(06-0903)'\n --and month(from_iso8601_timestamp(\"timestamp\")) in (7,8) -- 16m05s, 92.13GB\n where partition_0 = '2022_(06-0903)'                      --  4m08s, 92.46GB\n   and month(from_iso8601_timestamp(\"timestamp\")) &gt;= 9\n   and json_extract_scalar(message,'$.mdc[\"http_access_response_header_X-Request-ID\"]') not in (\n         select \"mdc_http_access_response_header_x-request-id\"\n         from \"bfd-insights-bfd-prod\".bfd_insights_bfd_prod_api_requests\n   )\n) select\n   cw_timestamp,\n   cw_id,\n   timestamp,\n   level,\n   thread,\n   logger,\n   message,\n   context,\n   stuff['bene_id'] as \"bene_id\",\n   stuff['database_query_bene_by_coverage_batch'] as \"database_query_bene_by_coverage_batch\",\n   ... &lt;repeat for all mdc columns in order since this is positional&gt;\n   from dataset\n</code></pre></p> </li> <li> <p>Load the <code>target</code> table from the <code>staging</code> table.</p> </li> <li>The <code>target</code> table definition resides in terraform and should match the <code>staging</code> table columns and ordering with the one       difference being that the <code>staging</code> table does not include the <code>year</code> and <code>month</code> partition columns. Verify that       the table structure is the same if not done already. </li> <li> <p>Similar to the loading of the <code>staging</code> table, this must be done in batches to avoid the Athena 30 minute timeout.       The batches here can be larger than for the <code>staging</code> table but ~500 million is the limit.       <pre><code>insert into \"bfd_insights_bfd_prod_api_requests\" \nselect\n   cw_timestamp,\n   cw_id,\n   timestamp,\n   level,\n   thread,\n   logger,\n   message,\n   context,\n   \"mdc_bene_id\",\n   \"mdc_database_query_bene_by_coverage_batch\",\n   \"mdc_jpa_query_eobs_by_bene_id_snf_record_count\",\n   ... &lt;all mdc columns here&gt;\n   date_format(from_iso8601_timestamp(\"timestamp\"), '%Y') as \"year\",\n   date_format(from_iso8601_timestamp(\"timestamp\"), '%m') as \"month\"\nfrom bfd_cw_export.api_requests_no_partitions\n--where year(from_iso8601_timestamp(\"timestamp\")) = 2019\n--where year(from_iso8601_timestamp(\"timestamp\")) = 2020\n--where year(from_iso8601_timestamp(\"timestamp\")) = 2021\n--where year(from_iso8601_timestamp(\"timestamp\")) = 2022\n  --and month(from_iso8601_timestamp(\"timestamp\")) &lt; 8\nwhere year(from_iso8601_timestamp(\"timestamp\")) = 2022\n   and month(from_iso8601_timestamp(\"timestamp\")) = 8\n</code></pre></p> </li> <li> <p>Verify the load.</p> </li> <li> <p>Select a sample of the data and inspect the most important columns: timestamp, mdc_bene_id, mdc_http* to ensure       that the columns are populated sensibly. Note that many of the other columns are only sparsely populated.       <pre><code>select *\nfrom bfd_insights_bfd_prod_api_requests\nlimit 100;\n</code></pre></p> </li> <li> <p>Compare the count of records by month between the <code>export</code> table and the <code>target</code> table. The counts for each month       should match (accounting for any data in the <code>target</code> table that was loaded independently by firehose).       <pre><code>-- Retrieve count of records by month for the target table\nselect date_format(from_iso8601_timestamp(timestamp), '%Y-%m'), count(*)\nfrom bfd_insights_bfd_prod_api_requests\ngroup by 1\norder by 1\n\n-- Retrieve count of records by month for the export table\nselect date_format(from_iso8601_timestamp(timestamp), '%Y-%m'), count(*)\nfrom bfd_cw_export.prod\ngroup by 1\norder by 1\n</code></pre></p> </li> </ol> <p>Reference:</p> <p>The following query was used to extract the canonical ordered list of JSON MDC keys from the Cloudwatch exports to define the initial table schema for the <code>export</code> table.</p> <pre><code>with dataset AS (\n   select map_keys(cast(json_extract(message, '$.mdc') as MAP(VARCHAR, VARCHAR))) as things\n   from bfd_cw_export.prod\n)\nselect distinct concat('mdc_', replace(lower(name), '.', '_')) as column_name from dataset\ncross join unnest(things) as t(name)\norder by column_name;\n</code></pre>"},{"location":"runbooks/bfd-insights/new-quicksight-dashboards.html","title":"How to Create BFD Insights QuickSight Dashboards","text":"<p>Note: The <code>prod</code> version of each analysis is listed here because it is the only environment for which we do this analysis. To set up for another environment, replace <code>prod</code> with the name of your environment, such as <code>prod</code> or <code>prod-sbx</code>. Note that for Athena table names, replace any <code>-</code> with <code>_</code>, such as <code>prod_sbx</code> instead of <code>prod-sbx</code>. (Athena doesn't like hyphens in table names).</p> <ol> <li>Go to QuickSight. It's an AWS service but for some reason it uses a different email-based permissions. You may need to request access.</li> <li>Create the dataset by going to Datasets on the left menu and choosing the New Dataset button on the right.<ul> <li>Choose Athena to open the \"New Athena data source\" modal.<ul> <li>Name your data source. Example: <code>bfd-prod</code></li> <li>Athena Workgroup: <code>bfd</code></li> <li>Create Data Source.</li> </ul> </li> <li>\"Choose Your Table\" modal<ul> <li>Catalog: <code>AwsDataCatalog</code></li> <li>Database: <code>bfd-insights-bfd-prod</code></li> <li>Table: <code>new_benes_by_day</code></li> <li>Select</li> </ul> </li> <li>\"Finish dataset creation\" modal<ul> <li>Select \"Import to SPICE for quicker analysis\"</li> <li>Check \"Email owners when a refresh fails\"</li> <li>Select \"Edit/Preview data\"</li> </ul> </li> <li>\"Data Prep\"<ul> <li>Select \"Add Calculated Field\"</li> <li>Name: <code>yearMonth</code></li> <li>Calculation: <code>formatDate({day}, 'yyyy-MM')</code></li> <li>Save</li> <li>Select \"Add Calculated Field\"</li> <li>Name: <code>daily_running_count_total</code></li> <li>Calculation: <code>runningSum(sum({bfd_new_benes}), [{day} ASC])</code></li> <li>Save</li> <li>Select \"Add Calculated Field\"</li> <li>Name: <code>daily_running_count_ab2d</code></li> <li>Calculation: <code>runningSum(sum({ab2d_new_benes}), [{day} ASC])</code></li> <li>Save</li> <li>Select \"Add Calculated Field\"</li> <li>Name: <code>daily_running_count_bb2</code></li> <li>Calculation: <code>runningSum(sum({bb2_new_benes}), [{day} ASC])</code></li> <li>Save</li> <li>Select \"Add Calculated Field\"</li> <li>Name: <code>daily_running_count_bcda</code></li> <li>Calculation: <code>runningSum(sum({bcda_new_benes}), [{day} ASC])</code></li> <li>Save</li> <li>Select \"Add Calculated Field\"</li> <li>Name: <code>daily_running_count_dpc</code></li> <li>Calculation: <code>runningSum(sum({dpc_new_benes}), [{day} ASC])</code></li> <li>Save</li> <li>Select \"Add Calculated Field\"</li> <li>Name: <code>daily_running_count_bulk</code></li> <li>Calculation: <code>runningSum(sum({bulk_new_benes}), [{day} ASC])</code></li> <li>Save</li> <li>Select \"Add Calculated Field\"</li> <li>Name: <code>monthly_running_count_total</code></li> <li>Calculation: <code>runningSum(sum({bfd_new_benes}), [{yearMonth} ASC])</code></li> <li>Save</li> <li>Select \"Add Calculated Field\"</li> <li>Name: <code>monthly_running_count_ab2d</code></li> <li>Calculation: <code>runningSum(sum({ab2d_new_benes}), [{yearMonth} ASC])</code></li> <li>Save</li> <li>Select \"Add Calculated Field\"</li> <li>Name: <code>monthly_running_count_bb2</code></li> <li>Calculation: <code>runningSum(sum({bb2_new_benes}), [{yearMonth} ASC])</code></li> <li>Save</li> <li>Select \"Add Calculated Field\"</li> <li>Name: <code>monthly_running_count_bcda</code></li> <li>Calculation: <code>runningSum(sum({bcda_new_benes}), [{yearMonth} ASC])</code></li> <li>Save</li> <li>Select \"Add Calculated Field\"</li> <li>Name: <code>monthly_running_count_dpc</code></li> <li>Calculation: <code>runningSum(sum({dpc_new_benes}), [{yearMonth} ASC])</code></li> <li>Save</li> <li>Select \"Add Calculated Field\"</li> <li>Name: <code>monthly_running_count_bulk</code></li> <li>Calculation: <code>runningSum(sum({bulk_new_benes}), [{yearMonth} ASC])</code></li> <li>Save</li> <li>Select \"Publish &amp; Visualize\"</li> </ul> </li> </ul> </li> <li>Analysis screen.<ul> <li>Create Calculated Fields that Require Aggregation Functions (must be added to the Analysis,   not the Dataset)</li> <li>Select \"Add\" then \"Add Calculated Field\"<ul> <li>Name: <code>monthly_unique_count_total</code></li> <li>Calculation: <code>sum({bfd_new_benes}, [yearMonth])</code></li> <li>Save</li> </ul> </li> <li>Select \"Add\" then \"Add Calculated Field\"<ul> <li>Name: <code>monthly_unique_count_ab2d</code></li> <li>Calculation: <code>sum({ab2d_new_benes}, [yearMonth])</code></li> <li>Save</li> </ul> </li> <li>Select \"Add\" then \"Add Calculated Field\"<ul> <li>Name: <code>monthly_unique_count_bb2</code></li> <li>Calculation: <code>sum({bb2_new_benes}, [yearMonth])</code></li> <li>Save</li> </ul> </li> <li>Select \"Add\" then \"Add Calculated Field\"<ul> <li>Name: <code>monthly_unique_count_bcda</code></li> <li>Calculation: <code>sum({bcda_new_benes}, [yearMonth])</code></li> <li>Save</li> </ul> </li> <li>Select \"Add\" then \"Add Calculated Field\"<ul> <li>Name: <code>monthly_unique_count_dpc</code></li> <li>Calculation: <code>sum({dpc_new_benes}, [yearMonth])</code></li> <li>Save</li> </ul> </li> <li>Select \"Add\" then \"Add Calculated Field\"<ul> <li>Name: <code>monthly_unique_count_bulk</code></li> <li>Calculation: <code>sum({bulk_new_benes}, [yearMonth])</code></li> <li>Save</li> </ul> </li> <li>Create the \"All APIs\" sheet. (Use the + symbol to add a new sheet)</li> <li>Add a new visualization of type \"Stacked area line chart\" and configure:<ul> <li>X axis: <code>day</code></li> <li>Value: <code>daily_running_count_total</code></li> <li>Title: <code>Unique Medicare Enrollees (All APIs)</code></li> <li>X-axis title: <code>Date</code></li> <li>Y-axis title: <code>Cumulative Unique Medicare Enrollees</code></li> </ul> </li> <li>Add a new visualization of type \"KPI\" and configure:<ul> <li>Value: bfd_new_benes (Sum)</li> <li>Title: <code>Unique Medicare Enrollees (All APIs)</code></li> </ul> </li> <li>Create the \"Bulk APIs\" sheet. (Use the + symbol to add a new sheet)</li> <li>Add a new visualization of type \"Stacked area line chart\" and configure:<ul> <li>X axis: <code>day</code></li> <li>Value: <code>daily_running_count_bulk</code></li> <li>Title: <code>Unique Medicare Enrollees (Bulk APIs)</code></li> <li>X-axis title: <code>Date</code></li> <li>Y-axis title: <code>Cumulative Unique Medicare Enrollees</code></li> </ul> </li> <li>Add a new visualization of type \"KPI\" and configure:<ul> <li>Value: bulk_new_benes (Sum)</li> <li>Title: <code>Unique Medicare Enrollees (Bulk APIs)</code></li> </ul> </li> <li>Create the \"AB2D\" sheet. (Use the + symbol to add a new sheet)</li> <li>Add a new visualization of type \"Stacked area line chart\" and configure:<ul> <li>X axis: <code>day</code></li> <li>Value: <code>daily_running_count_ab2d</code></li> <li>Title: <code>Unique Medicare Enrollees (AB2D)</code></li> <li>X-axis title: <code>Date</code></li> <li>Y-axis title: <code>Cumulative Unique Medicare Enrollees</code></li> </ul> </li> <li>Add a new visualization of type \"KPI\" and configure:<ul> <li>Value: ab2d_new_benes (Sum)</li> <li>Title: <code>Unique Medicare Enrollees (AB2D)</code></li> </ul> </li> <li>Create the \"BB2\" sheet. (Use the + symbol to add a new sheet)</li> <li>Add a new visualization of type \"Stacked area line chart\" and configure:<ul> <li>X axis: <code>day</code></li> <li>Value: <code>daily_running_count_bb2</code></li> <li>Title: <code>Unique Medicare Enrollees (BB2)</code></li> <li>X-axis title: <code>Date</code></li> <li>Y-axis title: <code>Cumulative Unique Medicare Enrollees</code></li> </ul> </li> <li>Add a new visualization of type \"KPI\" and configure:<ul> <li>Value: bb2_new_benes (Sum)</li> <li>Title: <code>Unique Medicare Enrollees (BB2)</code></li> </ul> </li> <li>Create the \"BCDA\" sheet. (Use the + symbol to add a new sheet)</li> <li>Add a new visualization of type \"Stacked area line chart\" and configure:<ul> <li>X axis: <code>day</code></li> <li>Value: <code>daily_running_count_bcda</code></li> <li>Title: <code>Unique Medicare Enrollees (BCDA)</code></li> <li>X-axis title: <code>Date</code></li> <li>Y-axis title: <code>Cumulative Unique Medicare Enrollees</code></li> </ul> </li> <li>Add a new visualization of type \"KPI\" and configure:<ul> <li>Value: bcda_new_benes (Sum)</li> <li>Title: <code>Unique Medicare Enrollees (BCDA)</code></li> </ul> </li> <li>Create the \"DPC\" sheet. (Use the + symbol to add a new sheet)</li> <li>Add a new visualization of type \"Stacked area line chart\" and configure:<ul> <li>X axis: <code>day</code></li> <li>Value: <code>daily_running_count_dpc</code></li> <li>Title: <code>Unique Medicare Enrollees (DPC)</code></li> <li>X-axis title: <code>Date</code></li> <li>Y-axis title: <code>Cumulative Unique Medicare Enrollees</code></li> </ul> </li> <li>Add a new visualization of type \"KPI\" and configure:<ul> <li>Value: dpc_new_benes (Sum)</li> <li>Title: <code>Unique Medicare Enrollees (DPC)</code></li> </ul> </li> <li>Create the \"Monthly\" sheet. (Use the + symbol to add a new sheet)</li> <li>Add a new visualization of type \"KPI\" and configure:<ul> <li>Value: bfd_new_benes (Sum)</li> <li>Title: <code>Total Unique Medicare Enrollees</code></li> </ul> </li> <li>Add a new visualization of type \"KPI\" and configure:<ul> <li>Value: ab2d_new_benes (Sum)</li> <li>Title: <code>AB2D Total Unique Medicare Enrollees</code></li> </ul> </li> <li>Add a new visualization of type \"KPI\" and configure:<ul> <li>Value: bb2_new_benes (Sum)</li> <li>Title: <code>BB2 Total Unique Medicare Enrollees</code></li> </ul> </li> <li>Add a new visualization of type \"KPI\" and configure:<ul> <li>Value: bcda_new_benes (Sum)</li> <li>Title: <code>BCDA Total Unique Medicare Enrollees</code></li> </ul> </li> <li>Add a new visualization of type \"KPI\" and configure:<ul> <li>Value: dpc_new_benes (Sum)</li> <li>Title: <code>DPC Total Unique Medicare Enrollees</code></li> </ul> </li> <li>Add a new visualization of type \"Table\" and configure:<ul> <li>Group by: <code>yearMonth</code></li> <li>Value: <code>monthly_running_count_*</code></li> <li>Title: <code>Cumulative Unique Medicare Enrollees</code></li> <li>Group-by column names:</li> <li>yearMonth -&gt; <code>Month</code></li> <li>Value column names:</li> <li>monthly_running_count_ab2d -&gt; <code>AB2D</code></li> <li>monthly_running_count_bb2 -&gt; <code>BB2</code></li> <li>monthly_running_count_bcda -&gt; <code>BCDA</code></li> <li>monthly_running_count_dpc -&gt; <code>DPC</code></li> <li>monthly_running_count_bulk -&gt; <code>Bulk APIs</code></li> <li>monthly_running_count_total -&gt; <code>All APIs</code></li> </ul> </li> <li>Add a new visualization of type \"Table\" and configure:<ul> <li>Group by: <code>yearMonth</code></li> <li>Value: <code>monthly_unique_count_*</code></li> <li>Title: <code>New Unique Medicare Enrollees</code></li> <li>Group-by column names:</li> <li>yearMonth -&gt; <code>Month</code></li> <li>Value column names:</li> <li>monthly_unique_count_ab2d -&gt; <code>AB2D</code></li> <li>monthly_unique_count_bb2 -&gt; <code>BB2</code></li> <li>monthly_unique_count_bcda -&gt; <code>BCDA</code></li> <li>monthly_unique_count_dpc -&gt; <code>DPC</code></li> <li>monthly_unique_count_bulk -&gt; <code>Bulk APIs</code></li> <li>monthly_unique_count_total -&gt; <code>All APIs</code></li> </ul> </li> <li>Create the \"Daily\" sheet. (Use the + symbol to add a new sheet)</li> <li>Add a new visualization of type \"KPI\" and configure:<ul> <li>Value: bfd_new_benes (Sum)</li> <li>Title: <code>Total Unique Medicare Enrollees</code></li> </ul> </li> <li>Add a new visualization of type \"KPI\" and configure:<ul> <li>Value: ab2d_new_benes (Sum)</li> <li>Title: <code>AB2D Total Unique Medicare Enrollees</code></li> </ul> </li> <li>Add a new visualization of type \"KPI\" and configure:<ul> <li>Value: bb2_new_benes (Sum)</li> <li>Title: <code>BB2 Total Unique Medicare Enrollees</code></li> </ul> </li> <li>Add a new visualization of type \"KPI\" and configure:<ul> <li>Value: bcda_new_benes (Sum)</li> <li>Title: <code>BCDA Total Unique Medicare Enrollees</code></li> </ul> </li> <li>Add a new visualization of type \"KPI\" and configure:<ul> <li>Value: dpc_new_benes (Sum)</li> <li>Title: <code>DPC Total Unique Medicare Enrollees</code></li> </ul> </li> <li>Add a new visualization of type \"Table\" and configure:<ul> <li>Group by: <code>day</code></li> <li>Value: <code>daily_running_count_*</code></li> <li>Title: <code>Cumulative Unique Medicare Enrollees</code></li> <li>Group-by column names:</li> <li>day -&gt; <code>Date</code></li> <li>Value column names:</li> <li>daily_running_count_ab2d -&gt; <code>AB2D</code></li> <li>daily_running_count_bb2 -&gt; <code>BB2</code></li> <li>daily_running_count_bcda -&gt; <code>BCDA</code></li> <li>daily_running_count_dpc -&gt; <code>DPC</code></li> <li>daily_running_count_bulk -&gt; <code>Bulk APIs</code></li> <li>daily_running_count_total -&gt; <code>All APIs</code></li> </ul> </li> <li>Add a new visualization of type \"Table\" and configure:<ul> <li>Group by: <code>day</code></li> <li>Value: <code>*_new_benes(Sum)</code></li> <li>Title: <code>New Unique Medicare Enrollees</code></li> <li>Group-by column names:</li> <li>day -&gt; <code>Date</code></li> <li>Value column names:</li> <li>ab2d_new_benes -&gt; <code>AB2D</code></li> <li>bb2_new_benes -&gt; <code>BB2</code></li> <li>bcda_new_benes -&gt; <code>BCDA</code></li> <li>dpc_new_benes -&gt; <code>DPC</code></li> <li>bulk_new_benes -&gt; <code>Bulk APIs</code></li> <li>bfd_new_benes -&gt; <code>All APIs</code></li> </ul> </li> </ul> </li> <li>Create the dashboard.<ul> <li>While still on the analysis screen, in the upper-right, click Share &gt; Publish Dashboard. Title it \"BFD Unique Medicare Enrollees\".</li> <li>The default options should otherwise be fine, so click Publish Dashboard.</li> </ul> </li> <li>Make the dashboard public.<ul> <li>While still on the dashboard screen, in the upper right, click Share &gt; Share dashboard.</li> <li>On the left, there is a toggle under \"Enable access for\" labeled \"Everyone in this account\". Turn it on.</li> <li>On the left, there is also a toggle labeled \"Discoverable in QuickSight\". Turn that one on also.</li> </ul> </li> <li>Set the SPICE refresh.<ul> <li>Return to the main QuickSight index.</li> <li>Select \"Datasets\" on the left.</li> <li>Click on the dataset you created in step 2 above.</li> <li>Click on the Refresh tab.</li> <li>In the upper right, click on \"Add New Schedule\".<ul> <li>The defaults should be what you want.<ul> <li>\"Full Refresh\"</li> <li>Timezone should be \"America/New_York\"</li> <li>Start time should be \" 11:59 PM\" <li>Frequency \"Daily\"</li>   <li>Save.</li>"},{"location":"runbooks/bfd-pipeline/rerun-failed-pipeline-load.html","title":"How to Re-run a Failed BFD Pipeline Load","text":"<p>Follow this runbook to successfully reload data via BFD pipeline after a failed run.</p> <p>Note: If there are pending deployments or db migrations, make sure those finish before running these steps.</p> <ol> <li> <p>SSH into the AWS ETL EC2 instance for a given environment <code>bfd-&lt;test/prod/prod-sbx&gt;-etl</code> with <code>ssh -i &lt;local ssh key&gt; &lt;your ssh username&gt;@&lt;EC2 IP Address&gt;</code>.</p> </li> <li> <p>Confirm the pipeline has failed to load data. </p> <ul> <li>In AWS S3, the RIF folder (i.e. <code>&lt;yyyy&gt;-&lt;MM&gt;-&lt;dd&gt;T&lt;HH&gt;:&lt;mm&gt;:&lt;ss&gt;Z</code>) containing the data for reloading will still be in 'Incoming' with the file S3 file structure as:     <pre><code>&lt;S3 Bucket Name&gt;-&lt;aws-account-id&gt;\n\u2502\n\u2514\u2500\u2500\u2500Incoming/\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u25002022-09-23T13:44:55Z/\n\u2502   \u2502    \u2502   *_manifest.xml\n\u2502   \u2502    \u2502   *.rif\n\u2502   \u2502    \u2502   ...\n\u2502   \u2502 \n\u2502   \u2514\u2500\u2500\u2500...\n\u2502   \n\u2514\u2500\u2500\u2500Done/\n\u2502    \u2502   \n\u2502    \u2514\u2500\u2500\u2500...\n</code></pre>     The AWS S3 bucket name in the file structure above can be found within the ETL EC2 instance by running <code>grep S3_BUCKET_NAME /bluebutton-data-pipeline/bfd-pipeline-service.sh | cut -f2 -d=</code>.</li> </ul> </li> <li> <p>Check if the pipeline is running with <code>sudo systemctl status bfd-pipeline</code>, and if so, stop it with <code>sudo systemctl stop bfd-pipeline</code>.</p> </li> <li> <p>In the EC2 instance enable idempotent mode for the pipeline:</p> <ul> <li>Open the file <code>/bluebutton-data-pipeline/bfd-pipeline-service.sh</code>.</li> <li>Change the line <code>export IDEMPOTENCY_REQUIRED='false'</code> to <code>export IDEMPOTENCY_REQUIRED='true'</code>.</li> <li>Save and close the file.</li> </ul> </li> <li> <p>Restart the pipeline with <code>sudo systemctl start bfd-pipeline</code>.</p> </li> <li> <p>Confirm restarting the pipleine and loading data in idempotent mode is succesful: </p> <ul> <li>The output of running <code>sudo systemctl status bfd-pipeline</code> should say \"active(running) since \u2026\".</li> <li>As data is loading check the logs by running <code>tail /bluebutton-data-pipeline/bluebutton-data-pipeline.log -f</code>. </li> <li>When data is loaded properly, in AWS S3, the RIF folder containing the data for reloading will have automatically moved from 'Incoming' to 'Done' with the file S3 file structure as:     <pre><code>&lt;S3 Bucket Name&gt;-&lt;aws-account-id&gt;\n\u2502\n\u2514\u2500\u2500\u2500Incoming/\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500...\n\u2502   \n\u2514\u2500\u2500\u2500Done/\n\u2502   \u2502   \n\u2502   \u2514\u2500\u2500\u25002022-09-23T13:44:55Z/\n\u2502   \u2502   \u2502   *_manifest.xml\n\u2502   \u2502   \u2502   *.rif\n\u2502   \u2502   \u2502  ...\n\u2502   \u2502 \n\u2502   \u2514\u2500\u2500\u2500...\n</code></pre></li> </ul> </li> <li> <p>With the data successfully loaded, in the EC2 instance, make sure to disable idempotent mode for the pipeline again:</p> <ul> <li>Open the file <code>/bluebutton-data-pipeline/bfd-pipeline-service.sh</code>.</li> <li>Change the line <code>export IDEMPOTENCY_REQUIRED='true'</code> to <code>export IDEMPOTENCY_REQUIRED='false'</code>.</li> <li>Save and close the file.</li> </ul> </li> <li> <p>Restart the pipeline <code>sudo systemctl restart bfd-pipeline</code>.</p> </li> </ol>"},{"location":"runbooks/bfd-server/recreate-regression-dashboards.html","title":"Recreating <code>server-regression</code> QuickSight Dashboards","text":"<p>Follow this runbook in the event that the QuickSight dashboards for the <code>server-regression</code>'s test suite's performance metrics are ever destroyed and need to be recreated.</p> <p>As of writing, the following dashboards are available to view for members of the BFD team:</p> <ul> <li><code>bfd-test-server-regression</code></li> <li><code>bfd-prod-sbx-server-regression</code></li> <li><code>bfd-prod-server-regression</code></li> </ul>  <p>Note: The above dashboards can also be found in QuickSight's \"Dashboards\" view</p>"},{"location":"runbooks/bfd-server/recreate-regression-dashboards.html#glossary","title":"Glossary","text":"Term Definition     <code>server-regression</code> Technical identifier for the Locust-based performance regression test suite that runs against BFD Server during deployment in all environments   QuickSight An AWS Service that allows for the creation of visual dashboards from various datasets (i.e. S3, RDS, Athena, etc.)   Dataset QuickSight's term for a data store (such as S3, Athena, etc) along with the fields to display from the store   Analysis A work-in-progress Dashboard that uses a Dataset to display fields   Dashboard A collection of graphs and visuals that represents a Dataset   Athena An AWS Service that is used to analyze data stored in AWS S3 using standard SQL"},{"location":"runbooks/bfd-server/recreate-regression-dashboards.html#faq","title":"FAQ","text":""},{"location":"runbooks/bfd-server/recreate-regression-dashboards.html#will-the-hash-in-each-of-the-dataset-sql-queries-below-ever-change-what-is-it-used-for","title":"Will the <code>hash</code> in each of the Dataset SQL queries below ever change? What is it used for?","text":"<p>Firstly, the <code>hash</code> (or, more specifically, <code>metadata.hash</code>) is a SHA256 hash of various test parameters and metadata values from a given test suite run. Performance metrics with the same <code>metadata.hash</code> value can be confidently compared as they were collected from test suite runs that were started with the same parameters (i.e. number of simulated users, desired test runtime, types of tests run, etc.).</p> <p>There are really only two scenarios where the hash could change:</p> <ol> <li>The parameters (runtime, user count, spawn rate) that control how the <code>server-regression</code> test suite runs during BFD Server deployments change from their current defaults</li> <li>The code to generate the hash changes because we add an additional field or want to change the hashing algorithm</li> </ol> <p>These scenarios are unlikely for the following reasons:</p> <ol> <li>With the current parameters, consistent and useful results are being captured with minimal increase in deployment time. That is to say, at time of writing there is no benefit in changing these parameters</li> <li>There is no additional metadata that Locust exposes that we would want to include in the hash to further delineate performance metrics that can or can't be compared</li> </ol> <p>If, however, the hash does change in the future, the queries listed below will need to be updated in both this runbook and in each of the Datasets in QuickSight. Note that this scenario will be easy to identify, as the visuals in each Dashboard will stop displaying data.</p>"},{"location":"runbooks/bfd-server/recreate-regression-dashboards.html#instructions","title":"Instructions","text":"<ol> <li> <p>You will need to navigate to the QuickSight AWS Service in order to start creating QuickSight Dashboards</p> </li> <li> <p>Navigate to <code>aws.amazon.com</code> in any browser</p> </li> <li>Click the \"Sign In\" link in the navigation bar on the top right of the screen</li> <li>Enter your AWS account credentials and sign-in</li> <li>You should now have navigated to the \"Console Home\", click the search input in the navigation bar on the top left of the screen</li> <li> <p>Type \"QuickSight\" and click the QuickSight result that appears</p> </li> <li> <p>Every QuickSight dashboard requires a Dataset, which for the <code>server-regression</code> dashboards will be a <code>Custom SQL</code> query against Athena. As there will be three dashboards, there will be three datasets (one dataset for each dashboard)</p> </li> <li> <p>Creating the <code>bfd-test-server-regression</code> dataset:</p> <ol> <li>Navigate to \"Datasets\" in QuickSight (click on \"Datasets\" in the navigation tree on the left side of the page)</li> <li>Click the \"New dataset\" button on the top-left of the page</li> <li>The page that appears should list a large number of possible data sources. Find \"Athena\" in this list, and click on it</li> <li>Enter \"bfd-test-server-regression\" for the \"Data source name\"</li> <li>Choose <code>bfd</code> for the \"Athena workgroup\"</li> <li>Click \"Create data source\"</li> <li>Choose <code>AwsDataCatalog</code> as the \"Catalog\"</li> <li>Choose <code>bfd-insights-bfd-test</code> as the \"Database\"</li> <li>Choose <code>bfd_insights_bfd_test_server_regression</code> as the \"Table\"</li> <li>Click \"Use custom SQL\"</li> <li>In the first input box, enter \"bfd-test-server-regression\" (replacing \"New custom SQL\")</li> <li>In the large text area, enter the following SQL query:</li> </ol> <pre><code>select\n  totals.total_reqs_per_second,\n  totals.num_requests as total_requests,\n  totals.num_failures as total_failures,\n  totals.min_response_time as total_min_response_time,\n  totals.average_response_time as total_avg_response_time,\n  totals.median_response_time as total_median_response_time,\n  totals.response_time_percentiles.\"0.95\" as total_95_percentile,\n  totals.response_time_percentiles.\"0.99\" as total_99_percentile,\n  totals.response_time_percentiles.\"1.0\" as total_100_percentile,\n  metadata.compare_result,\n  date_format(from_unixtime(metadata.timestamp, 'US/Eastern'), '%b-%e-%y %T EST') as date_time,\n  metadata.timestamp as raw_timestamp\nfrom \"bfd-insights-bfd-test\".\"bfd_insights_bfd_test_server_regression\"\nwhere\n  metadata.hash = '85d93b342368c8f57f8e78ea6c56979ad5e32901776aaeaffdd8467e4cee1df9'\n  and contains(metadata.tags, 'master')\n  and (\n     metadata.validation_result = 'PASSED'\n     or metadata.validation_result = 'NOT_APPLICABLE'\n  )\norder by metadata.timestamp asc\n</code></pre> <ol> <li>Click \"Confirm query\"</li> <li>In \"Finish dataset creation\", select \"Directly query your data\"</li> <li>Click \"Edit/preview data\"</li> <li>An editor should appear with the SQL you entered above, click \"Apply\"</li> <li>Ensure that data is returned in the data preview</li> <li>Click \"SAVE &amp; PUBLISH\" in the navigation bar on the top right of the screen</li> <li>Ensure that an in-page notification appears indicating the dataset was saved successfully</li> <li>Click \"CANCEL\" in the navigation bar on the top right of the screen to exit the Dataset creation page and return to QuickSight Home</li> </ol> </li> <li> <p>Creating the <code>bfd-prod-sbx-server-regression</code> dataset:</p> <ol> <li>Navigate to \"Datasets\" in QuickSight (click on \"Datasets\" in the navigation tree on the left side of the page)</li> <li>Click the \"New dataset\" button on the top-left of the page</li> <li>The page that appears should list a large number of possible data sources. Find \"Athena\" in this list, and click on it</li> <li>Enter \"bfd-prod-sbx-server-regression\" for the \"Data source name\"</li> <li>Choose <code>bfd</code> for the \"Athena workgroup\"</li> <li>Click \"Create data source\"</li> <li>Choose <code>AwsDataCatalog</code> as the \"Catalog\"</li> <li>Choose <code>bfd-insights-bfd-prod-sbx</code> as the \"Database\"</li> <li>Choose <code>bfd_insights_bfd_prod_sbx_server_regression</code> as the \"Table\"</li> <li>Click \"Use custom SQL\"</li> <li>In the first input box, enter \"bfd-prod-sbx-server-regression\" (replacing \"New custom SQL\")</li> <li>In the large text area, enter the following SQL query:</li> </ol> <pre><code>select\n  totals.total_reqs_per_second,\n  totals.num_requests as total_requests,\n  totals.num_failures as total_failures,\n  totals.min_response_time as total_min_response_time,\n  totals.average_response_time as total_avg_response_time,\n  totals.median_response_time as total_median_response_time,\n  totals.response_time_percentiles.\"0.95\" as total_95_percentile,\n  totals.response_time_percentiles.\"0.99\" as total_99_percentile,\n  totals.response_time_percentiles.\"1.0\" as total_100_percentile,\n  metadata.compare_result,\n  date_format(from_unixtime(metadata.timestamp, 'US/Eastern'), '%b-%e-%y %T EST') as date_time,\n  metadata.timestamp as raw_timestamp\nfrom \"bfd-insights-bfd-prod-sbx\".\"bfd_insights_bfd_prod_sbx_server_regression\"\nwhere\n  metadata.hash = '71a5310d52e0ffb8c03dc74cdee7bf635bc7fbd75dfe1d8f647fcfb11e5b65ce'\n  and contains(metadata.tags, 'master')\n  and (\n     metadata.validation_result = 'PASSED'\n     or metadata.validation_result = 'NOT_APPLICABLE'\n  )\norder by metadata.timestamp asc\n</code></pre> <ol> <li>Click \"Confirm query\"</li> <li>In \"Finish dataset creation\", select \"Directly query your data\"</li> <li>Click \"Edit/preview data\"</li> <li>An editor should appear with the SQL you entered above, click \"Apply\"</li> <li>Ensure that data is returned in the data preview</li> <li>Click \"SAVE &amp; PUBLISH\" in the navigation bar on the top right of the screen</li> <li>Ensure that an in-page notification appears indicating the dataset was saved successfully</li> <li>Click \"CANCEL\" in the navigation bar on the top right of the screen to exit the Dataset creation page and return to QuickSight Home</li> </ol> </li> <li> <p>Creating the <code>bfd-prod-server-regression</code> dataset:</p> <ol> <li>Navigate to \"Datasets\" in QuickSight (click on \"Datasets\" in the navigation tree on the left side of the page)</li> <li>Click the \"New dataset\" button on the top-left of the page</li> <li>The page that appears should list a large number of possible data sources. Find \"Athena\" in this list, and click on it</li> <li>Enter \"bfd-prod-server-regression\" for the \"Data source name\"</li> <li>Choose <code>bfd</code> for the \"Athena workgroup\"</li> <li>Click \"Create data source\"</li> <li>Choose <code>AwsDataCatalog</code> as the \"Catalog\"</li> <li>Choose <code>bfd-insights-bfd-prod</code> as the \"Database\"</li> <li>Choose <code>bfd_insights_bfd_prod_server_regression</code> as the \"Table\"</li> <li>Click \"Use custom SQL\"</li> <li>In the first input box, enter \"bfd-prod-server-regression\" (replacing \"New custom SQL\")</li> <li>In the large text area, enter the following SQL query:</li> </ol> <pre><code>select\n  totals.total_reqs_per_second,\n  totals.num_requests as total_requests,\n  totals.num_failures as total_failures,\n  totals.min_response_time as total_min_response_time,\n  totals.average_response_time as total_avg_response_time,\n  totals.median_response_time as total_median_response_time,\n  totals.response_time_percentiles.\"0.95\" as total_95_percentile,\n  totals.response_time_percentiles.\"0.99\" as total_99_percentile,\n  totals.response_time_percentiles.\"1.0\" as total_100_percentile,\n  metadata.compare_result,\n  date_format(from_unixtime(metadata.timestamp, 'US/Eastern'), '%b-%e-%y %T EST') as date_time,\n  metadata.timestamp as raw_timestamp\nfrom \"bfd-insights-bfd-prod\".\"bfd_insights_bfd_prod_server_regression\"\nwhere\n  metadata.hash = '727a44e865e717728ac409e8e23a7e13ce73a155874f3832a0caef1fde17e95c'\n  and contains(metadata.tags, 'master')\n  and (\n     metadata.validation_result = 'PASSED'\n     or metadata.validation_result = 'NOT_APPLICABLE'\n  )\norder by metadata.timestamp asc\n</code></pre> <ol> <li>Click \"Confirm query\"</li> <li>In \"Finish dataset creation\", select \"Directly query your data\"</li> <li>Click \"Edit/preview data\"</li> <li>An editor should appear with the SQL you entered above, click \"Apply\"</li> <li>Ensure that data is returned in the data preview</li> <li>Click \"SAVE &amp; PUBLISH\" in the navigation bar on the top right of the screen</li> <li>Ensure that an in-page notification appears indicating the dataset was saved successfully</li> <li>Click \"CANCEL\" in the navigation bar on the top right of the screen to exit the Dataset creation page and return to QuickSight Home</li> </ol> </li> <li> <p>Navigate to \"Analyses\" in QuickSight (click on \"Analyses\" in the navigation tree on the left side of the page from the QuickSight \"home\")</p> </li> <li>Click \"New analysis\" on the top right of the page</li> <li>Choose <code>bfd-test-server-regression</code></li> <li>A new page should appear with details about the dataset. Click on \"USE IN ANALYSIS\"</li> <li>A new page should appear with an empty \"AutoGraph\" and a variety of controls on the left-side:</li>  <li> <p>Start by re-creating all of the line graphs for the <code>server-regression</code> dashboard:</p> </li> <li> <p>Click anywhere within the empty \"AutoGraph\". An outline should appear indicating it has been selected, along with controls in the top-right corner</p> </li> <li>In \"Visual types\", choose a line graph (red outlined button in figure):<ol>  </ol> </li> <li>The graph's title/description should change to \"You need to add or remove fields\" and the collapsed \"Field wells\" section should expand showing \"X axis\", \"Value\", and \"Color\" as shown in the following figure:<ol>  </ol> </li> <li>In the \"Fields list\", click and drag \"date_time\" to the \"X axis\" control in \"Field wells\". You should see the graph update after doing so</li> <li>In the \"Fields list\", click and drag \"total_reqs_per_second\" to the \"Value\" control in \"Field wells\". You should see the graph update, and the page should look like the following:<ol>  </ol> </li> <li>The graph's \"Value\" is currently representing the unique count of the \"total_reqs_per_second\" which is not correct. Click on \"total_reqs_per_second (Count)\" in the \"Value\" control in the \"Field wells\" section. A dropdown menu should appear</li> <li>Click on \"Aggregate: Count\". An additional dropdown menu should appear to the right of the menu</li> <li>Click on \"Sum\" in the list of aggregations</li> <li>The graph should update and the values being graphed should be correct, as shown below (note the values will be different, but the graph should now be graphing the actual value of <code>total_reqs_per_second</code>):<ol>  </ol> </li> <li>The graph's x-axis (<code>date_time</code>) is currently sorted alphabetically and not by time. Similar to above, click on \"date_time\" in the \"X axis\" control in \"Field wells\". A dropdown menu should appear. The first item in the menu should be \"Sort by\" and should be \"total_reqs_per_second\" as shown in the figure:<ol>  </ol> </li> <li>Click on the \"Sort by\" option (highlighted by the red outline in the figure above). A second dropdown should appear to the right of the original menu</li> <li>Click on \"Sort options\" in the second, right-most dropdown</li> <li>The left pane (where \"Fields list\" was) should be replaced by \"Sort options\". In the select/dropdown menu labeled \"Sort by\", select \"raw_timestamp\" as shown in the figure below:<ol>  </ol> </li> <li>Click \"Apply\" at the bottom of the \"Sort options\" pane. The graph should update and now be ordered correctly by time</li> <li>Click \"Close\". The left-most pane should switch back to \"Fields list\"</li> <li>Change the title of the graph to \"Total RPS Over Time\" by double-clicking on the current title (\"Sum of Total_reqs_per_second by Date_time\") and entering the new title in the text area</li> <li>Double-click the y-axis label (\"total_reqs_per_second (Sum)\", shown vertically). A dropdown menu should appear</li> <li>Select \"Rename\" from the dropdown menu</li> <li>Rename to \"Total Requests-Per-Second\"</li> <li>Follow the same steps as above to rename the x-axis, but this time the name should be \"Date and Time (EST)\"</li> <li>Hover over the graph and click the small pencil icon in the top right corner. Hovering over the icon should show a tooltip with the text \"Format visual\". The \"Fields list\" pane should be replaced by a pane named \"Format visual\" on the leftside of the page, as shown below:<ol>  </ol> </li> <li>There should be multiple collapsible sections, click on \"Data labels\"</li> <li>Click on the \"Show data labels\" checkbox. Each point on the line graph should now be labeled with its value</li> <li>Hover over the graph again and this time click the three-dots in the top right corner. A dropdown menu should appear</li> <li>Click \"Duplicate visual\"</li> <li>A duplicate of the original \"Total RPS Over Time\" line graph should appear to the right</li> <li>Click on this new, duplicate graph. It should become outlined</li> <li>Uncollapse the \"Field wells\" section above the graphs by clicking anywhere within it</li> <li>Click on the green \"total_reqs_per_second (Sum)\" button in the \"Value\" section. A dropdown menu should appear</li> <li>In the very bottom of the dropdown menu, beneath the \"Search fields\" input, find \"total_requests\". Click on it</li> <li>The graph's y-axis should change to \"total_requests (Sum)\" and the \"Value\" of the graph should now be \"total_requests (Sum)\"</li> <li>Change the title of the graph to \"Total Requests over Time\" following instructions above</li> <li>Change the title of the y-axis to \"Total Requests\" following instructions above</li> <li>Ensure the title of the x-axis remains \"Date and Time (EST)\". If not, change it following instructions above</li> <li>Ensure that data labels appear above each point in the graph. If not, change it following instructions above</li> <li>Duplicate any of the two graphs following the instructions above. A third graph should appear on bottom-left of the screen</li> <li>Following instructions outlined above, change the \"Value\"/y-axis of the graph to \"total_failures\"</li> <li>Change the title to \"Number of Total Request Failures over Time\"</li> <li>Change the y-axis title to \"Total Failures\"</li> <li>Ensure the x-axis title is \"Date and Time (EST)\"</li> <li>Ensure data labels appear above each point in the line graph</li> <li>Navigate to \"Format visual\" (ensure that the new failures graph is selected)</li> <li>In the \"Title\" collapsible section, click the \"Edit subtitle\" button</li> <li> <p>Change the subtitle to \"Successful runs should always have 0 failures\"</p> </li> <li> <p>Next, recreate the totals bar charts</p> </li> <li> <p>On the top-left of the page, click the \"ADD\" button. A dropdown menu should appear</p> </li> <li>Select \"Add visual\". A new \"AutoGraph\" should appear on the page</li> <li>From \"Visual types\" (bottom collapsible section of the left-pane) select \"Vertical bar chart\" (top-most row, right-most icon). Vertical bar charts require both an x-axis and values, but since these charts will be used to display values without any meaningful x-axis to plot them against we will need to create a fake x-axis value to plot against</li> <li>Click the \"ADD\" button again. Choose \"Add calculated field\". A new page should open with three main sections: a name field with the value \"Add name\", a section on the right with inner collapsible sections showing \"Fields\", \"Parameters\" and \"Functions\", and a large text area that takes up most of the screen</li> <li>In the large text area, type \"1\"</li> <li>For the name of the field, enter \"fake_x\". See the figure below:<ol>  </ol> </li> <li>Click \"Save\". You should now return to the main Analysis screen with the graphs</li> <li>Select the empty vertical bar chart graph</li> <li>In \"Field wells\", set the x-axis to \"fake_x\"</li> <li>Set the \"Value\" to \"total_avg_response_time\". When set, the aggregate function should default to \"Count\"</li> <li>Following instructions outlined above to change the aggregate for a value, change the aggregate function from \"Count\" to \"Min\"</li> <li>Add another copy/duplicate of \"totalavg_response_time\" to the graph's \"Value\" (by dragging \"total_avg_response_time\" to \"Value\" in \"Field wells\" again) _beneath the previous \"total_avg_response_time (Min)\" value, as shown in the figure below:</li>  <li>This time, change the aggregate function from \"Count\" to \"Median\"</li> <li>Repeat the above steps two more times: add two more \"total_avg_reponse_time\" values, set the first's aggregate to \"Average\", and set the second's to \"Max\". When finished, \"Values\" should have 4 \"total_avg_response_time\" values each with a different aggregate function. See figure below:</li>  <li>Go to \"Format visual\" for the bar chart</li> <li>Un-collapse \"X-axis\" in \"Format visual\" by clicking on it</li> <li>Un-select every checkbox in the section: \"Show title\", \"Show sort\", \"Show data zoom\", \"Show axis line\", and \"Show labels\". The bar chart should now only display the y-axis</li> <li>Un-collapse \"Legend\"</li> <li>Un-select every checkbox in the section: \"Show legend\" and \"Show legend title\". The legend to the right of the bars in the vertical bar chart should no longer be visible</li> <li>Un-collapse \"Data labels\"</li> <li>Select \"Show data labels\". Each bar's value should now appear above their respective bar</li> <li>Un-collapse \"Tooltip\"</li> <li>Under \"Display options\", un-select \"Use primary value as title\" and select \"Show aggregations\"</li> <li>Under \"Fields\", find \"fake_x\" and click on the three-dots to the right of its label. A dropdown menu should appear</li> <li>Click/select \"Hide\". The \"fake_x\" field should darken/grey-out. Now when hovering over each distinct bar in the bar chart you will see a tooltip showing the value and the corresponding aggregation function, as shown in the figure below:</li>  <li>Close the \"Format visual\" section by clicking the \"x\" button next to the name at the top of the section. The \"Fields list\" should re-appear</li> <li>In \"Fields list\", hover over \"total_avg_response_time\" until a vertical three-dots button appears to the right of the label. Click on it. A dropdown menu should appear</li> <li>Hover over \"Format: 1,234.57\". A dropdown menu should appear to the right</li> <li>Click on \"More formatting options...\". The \"Fields list\" section should be replaced with \"Format data\"</li> <li>Un-collapse the \"Units\" section</li> <li>In the input field labeled \"Suffix\", enter \" ms\". Do not forget the leading space. You should see the values in the vertical bar chart for \"total_avg_response_time\" update to include the new \"ms\" suffix</li> <li>Change the bar chart's title to \"Total Average Response Time\"</li> <li>Change the bar chart's subtitle to \"Minimum, Median, Average, Maximum\"</li> <li>Duplicate the bar chart following the instructions for duplicating charts above</li> <li>For this new chart, change all of its aggregated values from \"total_avg_response_time\" to \"total_reqs_per_second\". You may need to change the aggregation function from \"Count\" to \"Min\"/\"Median\"/etc. after changing the value's field</li> <li>Change this new chart's title to \"Total Requests Per Second\"</li> <li>Ensure this new chart's subtitle is \"Minimum, Median, Average, Maximum\"</li> <li>Duplicate this new chart</li> <li>Change all aggregated values to \"total_requests\"</li> <li>Change the \"total_requests\" chart's title to \"Total Requests\"</li> <li>Ensure this new \"total_requests\" chart's subtitle is \"Minimum, Median, Average, Maximum\"</li> <li>Resize and move the 3 bar charts such that they are arranged at the top of the dashboard in a similar way to the following figure:</li> <li>  </li> <li> <p>Next, recreate the comparison results pie chart</p> <ol> <li>Create a new visual following the steps outlined above</li> <li>In \"Visual types\" choose \"Pie chart\"</li> <li>Set the \"Group/Color\" in \"Field wells\" to \"compare_result\". The pie chart should update and show the ratios of each type of comparison result</li> <li>Rename the bottom-most label from \"Group By: compare_result\" to \"Comparison Result\"</li> <li>Change the title to \"Comparison Results\"</li> </ol> </li> <li> <p>Finally, recreate the percentiles and average response time bar charts</p> <ol> <li>Add a new visual and set its type to \"Vertical bar chart\"</li> <li>Set the x-axis to \"date_time\"</li> <li>Set the values to \"total_95_percentile\", \"total_99_percentile\", \"total_100_percentile\". Ensure each value's aggregation function is \"Sum\". The bar chart should update and show each response time percentile for each date</li> <li>Following the instructions outlined above, set \"date_time\"'s sort to \"raw_timestamp\" so that the graph is sorted by time</li> <li>Following the instructions outlined above, enable data labels such that the value of each bar in the bar chart is displayed above the bar</li> <li>Following the instructions outlined above, for each field in \"Values\" add a \" ms\" suffix to the field. The values displayed in the bar chart should now include a \"ms\" suffix to indicate that the value is a measure of time</li> <li>Change the chart's title to \"95%, 99%, and 100% Total Response Time Percentiles over Time\"</li> <li>Change the x-axis's title to \"Date and Time (EST)\"</li> <li>Duplicate the chart</li> <li>Change the chart's values to be \"total_min_response_time\", \"total_median_response_time\", and \"total_max_response_time\". Ensure each value's aggregation function is \"Sum\", not \"Count\". The bar chart should update and show each response time for each date</li> <li>Following the instructions outlined above, change the suffix for \"total_min_response_time\" and \"total_median_response_time\" to \" ms\"</li> <li>Change the chart title to \"Minimum, Median, and Average Response Times (ms) over Time\"</li> <li>Ensure the x-axis title is \"Date and Time (EST)\"</li> </ol> </li> <li> <p>Click on \"ADD\" in the top-left</p> </li> <li>Select \"Add title\"</li> <li>Set the sheet's title to \"server-regression Statistics from TEST Environment 'master' Deployment Runs\"</li> <li>On the top-right of the screen, click the share icon (next to the save and download icons). A dropdown menu should appear</li> <li>Click \"Publish dashboard\"</li> <li>Name the dashboard \"bfd-test-server-regression\"</li> <li>Click \"Publish dashboard\". A new page should load showing the completed Dashboard</li> <li>Click the share icon on the top-right of the screen. A dropdown menu should appear</li> <li>Click \"Share dashboard\". A new page should load with sharing options</li> <li>In the bottom right, under \"Enable access for\", select \"Everyone in this account\"</li> <li>Select \"Discoverable in QuickSight\" as well</li> <li>Close the Dashboard and return to the QuickSight \"home\" by clicking on \"QuickSight\" on the top-left of the screen</li> <li>Return to the \"Analyses\" view</li> <li>Open the \"bfd-test-server-regression\" analysis</li> <li>When loaded, click on the save icon on the top-right of the screen. A dialog pop-up should appear titled \"Save a copy\"</li> <li>In the input field, enter \"bfd-prod-sbx-server-regression\"</li> <li>Click \"SAVE\". The copied Analysis should load</li> <li>In the left pane, click the pencil icon next to the \"Dataset\" label. A dialog pop-up should appear with the title \"Datasets in this analysis\"</li> <li>Click on the vertical three-dots. A dropdown menu should appear</li> <li>Click on \"Replace\". A new dialog should appear named \"Select replacement dataset\"</li> <li>Select the \"bfd-prod-sbx-server-regression\" Dataset</li> <li>Click \"Select\"</li> <li>Click \"Replace\". All graphs in the Analysis should update appropriately with the new Dataset's field values</li> <li>Change the sheet's title to \"server-regression Statistics from PROD-SBX Environment 'master' Deployment Runs\"</li> <li>Repeat the steps outlined above to publish and share the Analsyis as a Dashboard named \"bfd-prod-sbx-server-regression\"</li> <li>Repeat the previous steps to duplicate the \"bfd-prod-sbx-server-regression\" Analysis. Name the duplicated analysis \"bfd-prod-server-regression\"</li> <li>Repeat the previous steps to replace the \"bfd-prod-sbx-server-regression\" Dataset with \"bfd-prod-server-regression\"</li> <li>Change the sheet's title to \"server-regression Statistics from PROD Environment 'master' Deployment Runs\" following instructions outlined above</li> <li>Repeat the steps outlined above to publish and share the Analsyis as a Dashboard named \"bfd-prod-server-regression\"</li> </ol> <p>Once the steps above are complete, there should be three new Dashboards available to view in QuickSight displaying performance metrics for each of the three environments (<code>TEST</code>, <code>PROD-SBX</code>, and <code>PROD</code>). These Dashboards should appear similar to the following:</p>"},{"location":"runbooks/bfd-server/run-locust-tests.html","title":"How to Run the Regression and Load Test Suites","text":"<p>Follow this runbook to run the regression and load test suites either locally or against a particular BFD Server host.</p> <ul> <li>How to Run the Regression and Load Test Suites</li> <li>Glossary</li> <li>FAQ<ul> <li>I specified <code>--host</code> with a valid URL like so <code>example.com</code>, but my tests aren't running. What am I doing wrong?</li> </ul> </li> <li>Prerequisites</li> <li>Instructions<ul> <li>How to Run the Regression Suite Locally Against a Local BFD Server</li> <li>How to Run the Regression Suite Locally Against any BFD Server SDLC Environment</li> <li>How to Run the Regression Suite On a Detached Instance Against any BFD Server SDLC Environment</li> <li>How to Run a Scaling Load Test Using the <code>bfd-run-server-load</code> Jenkins Job</li> <li>How to Run a Static Load Test Using the <code>bfd-run-server-load</code> Jenkins Job</li> </ul> </li> </ul>"},{"location":"runbooks/bfd-server/run-locust-tests.html#glossary","title":"Glossary","text":"Term Definition     Locust A load testing library that allows for performance tests to be written in Python"},{"location":"runbooks/bfd-server/run-locust-tests.html#faq","title":"FAQ","text":""},{"location":"runbooks/bfd-server/run-locust-tests.html#i-specified-host-with-a-valid-url-like-so-examplecom-but-my-tests-arent-running-what-am-i-doing-wrong","title":"I specified <code>--host</code> with a valid URL like so <code>example.com</code>, but my tests aren't running. What am I doing wrong?","text":"<p>Firstly, <code>--host</code> is a default Locust argument that is a bit of a misnomer; valid <code>--host</code> values must include the protocol (i.e. <code>https</code>), hostname (i.e. <code>example.com</code>) and, optionally, the port in the following format: <code>PROTOCOL://HOSTNAME:PORT</code>. Be aware that Locust does not trim trailing slashes after the <code>PORT</code>; however, we have implemented a check for trailing slashes in <code>--host</code> and remove them ourselves. So, it is recommended that <code>--host</code> does not include any trailing characters after <code>PORT</code> as well.</p>"},{"location":"runbooks/bfd-server/run-locust-tests.html#prerequisites","title":"Prerequisites","text":"<ul> <li>A global installation of Python 3</li> <li>An installation of the AWS CLI that is configured properly for access to the BFD/CMS AWS account</li> <li>An installation of <code>jq</code></li> <li>A tool for creating virtual environments (<code>virtualenv</code>s) such as   <code>virtualenv</code> or <code>pew</code></li> <li>This runbook will assume you are using <code>pew</code> as it is fairly simple to work with and has a     relatively intuitive UX</li> <li>Access to AWS</li> <li>Access to the CMS VPN</li> </ul>"},{"location":"runbooks/bfd-server/run-locust-tests.html#instructions","title":"Instructions","text":"<p>Note: This runbook assumes you have cloned the <code>beneficiary-fhir-data</code> repository locally and are relatively comfortable with the command-line (CLI).</p>"},{"location":"runbooks/bfd-server/run-locust-tests.html#how-to-run-the-regression-suite-locally-against-a-local-bfd-server","title":"How to Run the Regression Suite Locally Against a Local BFD Server","text":"<p>Note: These steps assume you have followed the top-level README.md's steps for running BFD locally (including running the database using PostgreSQL in Docker).</p> <p>Additionally, it is highly recommended to read the entirety of the <code>locust_test</code> <code>README.md</code> before continuing.</p>  <ol> <li>Navigate to the root of the <code>beneficiary-fhir-data</code> repository in any terminal application</li> <li>From the root of the <code>beneficiary-fhir-data</code> repository, set <code>BFD_ROOT</code> to the current working    directory:</li> </ol> <pre><code>BFD_ROOT=$(pwd)\n</code></pre> <ol> <li>Navigate to <code>apps/utils/locust_tests</code>:</li> </ol> <pre><code>cd $BFD_ROOT/apps/utils/locust_tests\n</code></pre> <ol> <li>If not already created, create a new virtual environment for <code>locust_tests</code>:</li> </ol> <pre><code>pew new -p 3.8 -a . -r requirements.txt py3.8__locust_tests\n</code></pre> <ol> <li> <p>This will create a new virtual environment using Python 3.8 named <code>py3.8__locust_tests</code> and       will automatically install the necessary Python dependencies to run the various Locust test       suites in the <code>locust_tests</code> directory. It will also associate this new virtual environment       to the <code>$BFD_ROOT/apps/utils/locust_tests</code> directory</p> </li> <li> <p>Open a new subshell that uses the <code>py3.8__locust_tests</code> virtual environment:</p> </li> </ol> <pre><code>pew workon py3.8__locust_tests\n</code></pre> <ol> <li>Set <code>CLIENT_CERT_PATH</code> to the unsecured certificate available in the repository:</li> </ol> <pre><code>CLIENT_CERT_PATH=$BFD_ROOT/apps/bfd-server/dev/ssl-stores/client-unsecured.pem\n</code></pre> <ol> <li>Set <code>DATABASE_CONSTR</code> to the database connection string pointing to your locally running    PostgreSQL instance:</li> </ol> <pre><code>DATABASE_CONSTR=\"postgres://bfd:InsecureLocalDev@localhost:5432/fhirdb\"\n</code></pre> <ol> <li> <p>Running the above command assumes you have followed the README.md's \"Native Setup\" section       and are running your local PostgreSQL instance in a Docker container with the defaults       provided in that section. If you are not, you will need to change the connection string above       to point to your local BFD database instance</p> </li> <li> <p>Run the Locust tests using <code>locust</code>. Replace <code>&lt;NUM_USERS&gt;</code> with the number of simulated users    (amount of load) to run with, <code>&lt;SPAWN_RATE&gt;</code> with the rate at which you would like the simulated    users to spawn per-second, and <code>&lt;RUNTIME&gt;</code> with the amount of time you would like to run the    performance tests for once all users have spawned (you can specify runtime like \"10m30s\" or    \"30s\"):</p> </li> </ol> <pre><code>locust -f v2/regression_suite.py \\\n  --users=&lt;NUM_USERS&gt; \\\n  --host=\"localhost:$BFD_PORT\" \\\n  --spawn-rate=&lt;SPAWN_RATE&gt; \\\n  --spawned-runtime=\"&lt;RUNTIME&gt;\" \\\n  --client-cert-path=\"$CLIENT_CERT_PATH\" \\\n  --database-connection-string=\"$DATABASE_CONSTR\"\n  --headless\n</code></pre> <ol> <li>Once the regression tests have ended, Locust will print a summary table with the performance    statistics of the previous run for each endpoint as well as an aggregated total of all endpoint    performance</li> </ol>"},{"location":"runbooks/bfd-server/run-locust-tests.html#how-to-run-the-regression-suite-locally-against-any-bfd-server-sdlc-environment","title":"How to Run the Regression Suite Locally Against any BFD Server SDLC Environment","text":"<p>Note: These steps assume you will be testing against the <code>TEST</code> environment, but you are able to test against all SDLC environments following these instructions.</p> <p>Additionally, it is highly recommended to read the entirety of the <code>locust_test</code> <code>README.md</code> before continuing.</p>  <ol> <li>Set <code>BFD_ENV</code> to the environment you want to test:</li> </ol> <pre><code>BFD_ENV=\"test\"\n</code></pre> <ol> <li> <p>Other valid values are <code>\"prod-sbx\"</code> and <code>\"prod\"</code>, however it is unlikely you will need to run       the regression suite manually against environments other than <code>TEST</code></p> </li> <li> <p>Navigate to the root of the <code>beneficiary-fhir-data</code> repository in any terminal application</p> </li> <li>From the root of the <code>beneficiary-fhir-data</code> repository, set <code>BFD_ROOT</code> to the current working    directory:</li> </ol> <pre><code>BFD_ROOT=$(pwd)\n</code></pre> <ol> <li>Navigate to <code>apps/utils/locust_tests</code>:</li> </ol> <pre><code>cd $BFD_ROOT/apps/utils/locust_tests\n</code></pre> <ol> <li>If not already created, create a new virtual environment for <code>locust_tests</code>:</li> </ol> <pre><code>pew new -p 3.8 -a . -r requirements.txt py3.8__locust_tests\n</code></pre> <ol> <li> <p>This will create a new virtual environment using Python 3.8 named <code>py3.8__locust_tests</code> and       will automatically install the necessary Python dependencies to run the various Locust test       suites in the <code>locust_tests</code> directory. It will also associate this new virtual environment       to the <code>$BFD_ROOT/apps/utils/locust_tests</code> directory</p> </li> <li> <p>Open a new subshell that uses the <code>py3.8__locust_tests</code> virtual environment:</p> </li> </ol> <pre><code>pew workon py3.8__locust_tests\n</code></pre> <ol> <li>Ensure your AWS credentials are valid</li> <li>Ensure you are connected to the CMS VPN</li> <li>Download and decrypt the testing certificate from SSM and store it to a local file:</li> </ol> <pre><code>aws ssm get-parameter --name \"/bfd/$BFD_ENV/server/sensitive/test_client_key\" \\\n   --region \"us-east-1\" \\\n   --with-decryption | jq -r '.Parameter.Value' &gt; $HOME/bfd-test-cert.pem\naws ssm get-parameter --name \"/bfd/$BFD_ENV/server/sensitive/test_client_cert\" \\\n   --region \"us-east-1\" \\\n   --with-decryption | jq -r '.Parameter.Value' &gt;&gt; $HOME/bfd-test-cert.pem\n</code></pre> <ol> <li> <p>Running the above commands assume you have appropriate permissions to read and decrypt       sensitive SSM parameters in the environment under test</p> </li> <li> <p>Set <code>CLIENT_CERT_PATH</code> to the downloaded testing certificate from the previous step:</p> <pre><code>CLIENT_CERT_PATH=$HOME/bfd-test-cert.pem\n</code></pre> </li> <li> <p>Set <code>DATABASE_CONSTR</code> to the database connection string for the reader endpoint of the     environment under test:</p> <pre><code>DB_CLUSTER_ID=$(aws ssm get-parameter --name \"/bfd/$BFD_ENV/common/nonsensitive/rds_cluster_identifier\" \\\n                 --region \"us-east-1\" | jq -r '.Parameter.Value')\nDB_USERNAME=$(aws ssm get-parameter --name \"/bfd/$BFD_ENV/server/sensitive/vault_data_server_db_username\" \\\n                 --with-decryption \\\n                 --region \"us-east-1\" | jq -r '.Parameter.Value')\nDB_RAW_PASSWORD=$(aws ssm get-parameter --name \"/bfd/$BFD_ENV/server/sensitive/vault_data_server_db_password\" \\\n                 --with-decryption \\\n                 --region \"us-east-1\" | jq -r '.Parameter.Value')\nDB_PASSWORD=$(printf %s \"$DB_RAW_PASSWORD\" | jq -sRr @uri)\nDB_READER_URI=$(aws rds describe-db-clusters --db-cluster-identifier \"$DB_CLUSTER_ID\" \\\n                 --region \"us-east-1\" | jq -r '.DBClusters[0].ReaderEndpoint')\nDATABASE_CONSTR=\"postgres://$DB_USERNAME:$DB_PASSWORD@$DB_READER_URI:5432/fhirdb\"\n</code></pre> <ol> <li>Running the above commands assume you have appropriate permissions to read and decrypt    sensitive SSM parameters in the environment under test</li> </ol> </li> <li> <p>Run the Locust tests using <code>locust</code>. Replace <code>&lt;NUM_USERS&gt;</code> with the number of simulated users     (amount of load) to run with, <code>&lt;SPAWN_RATE&gt;</code> with the rate at which you would like the simulated     users to spawn per-second, and <code>&lt;RUNTIME&gt;</code> with the amount of time you would like to run the     performance tests for once all users have spawned (you can specify runtime like \"10m30s\" or     \"30s\"):</p> <pre><code>locust -f v2/regression_suite.py \\\n  --users=&lt;NUM_USERS&gt; \\\n  --host=\"https://$BFD_ENV.bfd.cms.gov\" \\\n  --spawn-rate=&lt;SPAWN_RATE&gt; \\\n  --spawned-runtime=\"&lt;RUNTIME&gt;\" \\\n  --client-cert-path=\"$CLIENT_CERT_PATH\" \\\n  --database-connection-string=\"$DATABASE_CONSTR\"\n  --headless\n</code></pre> <ol> <li>Note that <code>--host</code> can be anything (including the IP address of another instance that you    would like to target specifically). However, if you are targeting a particular environment,    you should stick with only testing instances under that environment (or the default provided    here)</li> </ol> </li> <li> <p>Once the regression tests have ended, Locust will print a summary table with the performance     statistics of the previous run for each endpoint as well as an aggregated total of all endpoint     performance</p> </li> <li> <p>Delete the <code>bfd-test-cert.pem</code>:</p> <pre><code>rm -f $CLIENT_CERT_PATH\n</code></pre> </li> </ol>"},{"location":"runbooks/bfd-server/run-locust-tests.html#how-to-run-the-regression-suite-on-a-detached-instance-against-any-bfd-server-sdlc-environment","title":"How to Run the Regression Suite On a Detached Instance Against any BFD Server SDLC Environment","text":"<p>Note: These steps assume you will be testing against the <code>TEST</code> environment, but you are able to test against all SDLC environments following these instructions.</p> <p>Additionally, it is highly recommended to read the entirety of the <code>locust_test</code> <code>README.md</code> before continuing.</p>  <ol> <li>First, detach an instance from the desired environment under test's auto-scaling group:</li> <li>Go to the AWS website and sign-in</li> <li>Click Services &gt; EC2</li> <li>Click Auto scaling groups</li> <li>Click an active group corresponding to the desired environment under test within this list<ol> <li>I.e. if <code>TEST</code> is the desired environment, click on <code>bfd-test-fhir...</code></li> </ol> </li> <li>In the Details area below the node you clicked, click the Instance Management tab</li> <li>Pick one of the instances here using the checkbox on the left</li> <li>In the actions dropdown, click Detach</li> <li>In the popup, check the Add a new instance to the Auto Scaling group to balance the load       checkbox to add a new instance in its place</li> <li>Confirm the detachment by clicking Detach instance</li> <li>In the list, the detached instance will still exist in the group; open the detached instance        in a new tab to keep track of it</li> <li>Go to Services &gt; EC2 and click Instances</li> <li>Find the detached instance by comparing its ID in the tab you opened against the ID in the        list</li> <li>Click the Edit button near the ID of the detached instance and rename it something that        indicates your name so the instance is marked as yours for others information</li> <li>Click on the newly-created Instance ID to open a details page</li> <li>Copy the private IP address from \"Private IPv4 addresses\"</li> <li>Ensure you are connected to the CMS VPN</li> <li>SSH into the detached instance using the private IP address copied from the previous step:</li> </ol> <pre><code>ssh -i &lt;YOUR PRIVATE KEY HERE&gt; &lt;YOUR USER HERE&gt;@&lt;DETACHED IP ADDRESS HERE&gt;\n</code></pre> <ol> <li>Become the <code>root</code> user on the detached instance:</li> </ol> <pre><code>sudo su\n</code></pre> <ol> <li> <p>As the <code>root</code> user you are able to do many dangerous things, especially if you are connected       to a detached instance from <code>PROD-SBX</code> or <code>PROD</code>. Be very careful while logged-in as the       <code>root</code> user!</p> </li> <li> <p>Set <code>BFD_ENV</code> to the environment under test:</p> </li> </ol> <pre><code>BFD_ENV=\"test\"\n</code></pre> <ol> <li>Navigate to the root of the <code>beneficiary-fhir-data</code> repository in any terminal application</li> <li>For detached instances, this will be at <code>/beneficiary-fhir-data</code></li> <li>From the root of the <code>beneficiary-fhir-data</code> repository, set <code>BFD_ROOT</code> to the current working    directory:</li> </ol> <pre><code>BFD_ROOT=$(pwd)\n</code></pre> <ol> <li>Navigate to <code>apps/utils/locust_tests</code>:</li> </ol> <pre><code>cd $BFD_ROOT/apps/utils/locust_tests\n</code></pre> <ol> <li>Install the Python dependencies necessary to run the regression suite:</li> </ol> <pre><code>pip3 install -r requirements.txt\n</code></pre> <ol> <li> <p>The instance comes with an installation of Python 3, and since we will be destroying the       instance after running the regression suite we do not need to use virtual environments</p> </li> <li> <p>Download and decrypt the testing certificate from SSM and store it to a local file:</p> <pre><code>aws ssm get-parameter --name \"/bfd/$BFD_ENV/server/sensitive/test_client_key\" \\\n   --region \"us-east-1\" \\\n   --with-decryption | jq -r '.Parameter.Value' &gt; $BFD_ROOT/bfd-test-cert.pem\naws ssm get-parameter --name \"/bfd/$BFD_ENV/server/sensitive/test_client_cert\" \\\n   --region \"us-east-1\" \\\n   --with-decryption | jq -r '.Parameter.Value' &gt;&gt; $BFD_ROOT/bfd-test-cert.pem\n</code></pre> </li> <li> <p>Running the above commands assume you have appropriate permissions to read and decrypt     sensitive SSM parameters in the environment under test</p> </li> <li> <p>Set <code>CLIENT_CERT_PATH</code> to the testing certificate from the previous step:</p> <pre><code>CLIENT_CERT_PATH=$BFD_ROOT/bfd-test-cert.pem\n</code></pre> </li> <li> <p>Set <code>DATABASE_CONSTR</code> to the database connection string for the reader endpoint of the     environment under test:</p> <pre><code>DB_CLUSTER_ID=$(aws ssm get-parameter --name \"/bfd/$BFD_ENV/common/nonsensitive/rds_cluster_identifier\" \\\n                 --region \"us-east-1\" | jq -r '.Parameter.Value')\nDB_USERNAME=$(aws ssm get-parameter --name \"/bfd/$BFD_ENV/server/sensitive/vault_data_server_db_username\" \\\n                 --with-decryption \\\n                 --region \"us-east-1\" | jq -r '.Parameter.Value')\nDB_RAW_PASSWORD=$(aws ssm get-parameter --name \"/bfd/$BFD_ENV/server/sensitive/vault_data_server_db_password\" \\\n                 --with-decryption \\\n                 --region \"us-east-1\" | jq -r '.Parameter.Value')\nDB_PASSWORD=$(printf %s \"$DB_RAW_PASSWORD\" | jq -sRr @uri)\nDB_READER_URI=$(aws rds describe-db-clusters --db-cluster-identifier \"$DB_CLUSTER_ID\" \\\n                 --region \"us-east-1\" | jq -r '.DBClusters[0].ReaderEndpoint')\nDATABASE_CONSTR=\"postgres://$DB_USERNAME:$DB_PASSWORD@$DB_READER_URI:5432/fhirdb\"\n</code></pre> <ol> <li>Running the above commands assume you have appropriate permissions to read and decrypt    sensitive SSM parameters in the environment under test</li> </ol> </li> <li> <p>Run the Locust tests using <code>locust</code>. Replace <code>&lt;NUM_USERS&gt;</code> with the number of simulated users     (amount of load) to run with, <code>&lt;SPAWN_RATE&gt;</code> with the rate at which you would like the simulated     users to spawn per-second, and <code>&lt;RUNTIME&gt;</code> with the amount of time you would like to run the     performance tests for once all users have spawned (you can specify runtime like \"10m30s\" or     \"30s\"):</p> <pre><code>locust -f v2/regression_suite.py \\\n  --users=&lt;NUM_USERS&gt; \\\n  --host=\"https://$BFD_ENV.bfd.cms.gov\" \\\n  --spawn-rate=&lt;SPAWN_RATE&gt; \\\n  --spawned-runtime=\"&lt;RUNTIME&gt;\" \\\n  --client-cert-path=\"$CLIENT_CERT_PATH\" \\\n  --database-connection-string=\"$DATABASE_CONSTR\"\n  --headless\n</code></pre> <ol> <li>Note that <code>--host</code> can be anything (including the IP address of another instance that you    would like to target specifically). However, if you are targeting a particular environment and    have detached an instance from that environment's ASG, you should stick with only testing    instances under that environment (or the default provided here)</li> </ol> </li> <li> <p>Once the regression tests have ended, Locust will print a summary table with the performance     statistics of the previous run for each endpoint as well as an aggregated total of all endpoint     performance</p> </li> <li> <p>Delete the <code>bfd-test-cert.pem</code>:</p> <pre><code>rm -f $CLIENT_CERT_PATH\n</code></pre> </li> <li> <p>In your web browser, navigate back to AWS and sign-in (if necessary)</p> </li> <li>Navigate to Services &gt; EC2</li> <li>Navigate to Instances in the navigation bar on the left side</li> <li>Find the detached instance by searching for its name or Instance ID</li> <li>Select the checkbox to the left of the name of the detached instance</li> <li>Click on the \"Instance state\" dropdown in the top right of the screen</li> <li>Select \"Terminate instance\" and accept any dialogs that appear</li> </ol>"},{"location":"runbooks/bfd-server/run-locust-tests.html#how-to-run-a-scaling-load-test-using-the-bfd-run-server-load-jenkins-job","title":"How to Run a Scaling Load Test Using the <code>bfd-run-server-load</code> Jenkins Job","text":"<ol> <li>Ensure you are connected to the CMS VPN</li> <li>Navigate to the Jenkins CloudBees instance in your web browser and sign-in</li> <li>From the main page, select \"bfd\". A list of jobs should load</li> <li>From this list of jobs, click on \"bfd-run-server-load\". A new page should load showing the \"Stage    View\" and a list of actions on the left side of the screen</li> <li>Click on \"Build with Parameters\" on the left side of the screen. A new page should load showing a    variety of input fields</li> <li>Choose the desired SDLC environment to load test from the \"ENVIRONMENT\" dropdown list</li> <li>Adjust the default parameters according to the desired load test. For this particular case, it is    assumed the desired load test is to continuously ramp-up load until a scaling event occurs and so    the defaults can be used</li> <li>Click \"Build\" at the bottom of the page. The page from Step #4 should load again, however an    in-progress build should appear in the \"Build History\" list on the left side of the screen</li> <li>Click on the build number of the in-progress build. A new page should load showing an overview    of the current build</li> <li>Click on \"Console Output\" on the left side of the screen. A new page should load showing     realtime log output from the job</li> <li> <p>Monitor the log output until the following prompt appears in the output:</p> <pre><code>Once the run is finished, click either Abort or Proceed to cleanup the test\nProceed or Abort\n</code></pre> </li> <li> <p>Scroll up in the log output and find the line starting with:</p> <pre><code>aws_instance.this[0]: Creation complete after...\n</code></pre> </li> <li> <p>Note the instance ID within square brackets -- use this later to follow the log output from the     controller in CloudWatch</p> </li> <li>In your web browser, navigate to AWS and sign-in (if necessary)</li> <li>Navigate to Services &gt; CloudWatch</li> <li>Navigate to \"Log groups\" by clicking on the link in the navigation tree</li> <li>Search for \"server-load-controller.log\" and select the corresponding log group in the SDLC     environment currently under test</li> <li>Refresh the log group until a log stream with the name of the instance ID noted down in Step 13     appears</li> <li>Open the log stream corresponding to the instance ID noted down in Step 13</li> <li>Monitor the log continuously by clicking \"Resume\" at the bottom of the log output. The log     should automatically update in realtime as the load test runs. You may need to continuously     scroll to view the log</li> <li>Wait until the load tests finish running. If at anytime something goes wrong, return to the     running Jenkins job and click either the \"Proceed\" or \"Abort\" prompt in the log output to     immediately end the test and start cleaning up</li> <li>Once Locust prints the summary table and has finished, indicated by the \"Locust master process     has stopped\" message, return to the Jenkins job and click \"Proceed\". This will cleanup the test,     destroying the controller instance and stopping any orphaned Lambda nodes</li> <li>View the stats of the run under the following log groups (the log stream corresponding to the     current run will be named according to the instance ID noted down in Step 13). Note     \"{ENVIRONMENT}\" should be replaced with the environment under test (i.e. \"test\"):<ul> <li>/bfd/{ENVIRONMENT}/bfd-server-load/load_exceptions.csv</li> <li>/bfd/{ENVIRONMENT}/bfd-server-load/load_failures.csv</li> <li>/bfd/{ENVIRONMENT}/bfd-server-load/load_stats.csv</li> <li>/bfd/{ENVIRONMENT}/bfd-server-load/load_stats_history.csv</li> </ul> </li> </ol>"},{"location":"runbooks/bfd-server/run-locust-tests.html#how-to-run-a-static-load-test-using-the-bfd-run-server-load-jenkins-job","title":"How to Run a Static Load Test Using the <code>bfd-run-server-load</code> Jenkins Job","text":"<ol> <li>Ensure you are connected to the CMS VPN</li> <li>Navigate to the Jenkins CloudBees instance in your web browser and sign-in</li> <li>From the main page, select \"bfd\". A list of jobs should load</li> <li>From this list of jobs, click on \"bfd-run-server-load\". A new page should load showing the \"Stage    View\" and a list of actions on the left side of the screen</li> <li>Click on \"Build with Parameters\" on the left side of the screen. A new page should load showing a    variety of input fields</li> <li>Choose the desired SDLC environment to load test from the \"ENVIRONMENT\" dropdown list</li> <li>Adjust the default parameters according to the desired load test. For this particular case, the    default values will need to be changed:</li> <li>Set <code>INITIAL_WORKER_NODES</code> to the number of worker nodes/Lambdas desired in total</li> <li>Set <code>MAX_SPAWNED_NODES</code> equal to <code>INITIAL_WORKER_NODES</code></li> <li>Set <code>MAX_SPAWNED_USERS</code> to the desired number of simulated users in total<ol> <li>Note that a ratio of 10 simulated users to 1 worker node should be followed for best      performance</li> </ol> </li> <li>Set <code>USER_SPAWN_RATE</code> equal to <code>MAX_SPAWNED_USERS</code> if no ramp-up is desired</li> <li>Unselect <code>STOP_ON_SCALING</code> if the load test should not stop when a scaling event is       encountered -- for a static test, this should probably be false</li> <li>Deselect <code>STOP_ON_NODE_LIMIT</code> to ensure that the load test does not end immediately due to the       node limit being hit</li> <li>Click \"Build\" at the bottom of the page. The page from Step #4 should load again, however an    in-progress build should appear in the \"Build History\" list on the left side of the screen</li> <li>Click on the build number of the in-progress build. A new page should load showing an overview    of the current build</li> <li>Click on \"Console Output\" on the left side of the screen. A new page should load showing     realtime log output from the job</li> <li> <p>Monitor the log output until the following prompt appears in the output:</p> <pre><code>Once the run is finished, click either Abort or Proceed to cleanup the test\nProceed or Abort\n</code></pre> </li> <li> <p>Scroll up in the log output and find the line starting with:</p> <pre><code>aws_instance.this[0]: Creation complete after...\n</code></pre> </li> <li> <p>Note the instance ID within square brackets -- use this later to follow the log output from the     controller in CloudWatch</p> </li> <li>In your web browser, navigate to AWS and sign-in (if necessary)</li> <li>Navigate to Services &gt; CloudWatch</li> <li>Navigate to \"Log groups\" by clicking on the link in the navigation tree</li> <li>Search for \"server-load-controller.log\" and select the corresponding log group in the SDLC     environment currently under test</li> <li>Refresh the log group until a log stream with the name of the instance ID noted down in Step 13     appears</li> <li>Open the log stream corresponding to the instance ID noted down in Step 13</li> <li>Monitor the log continuously by clicking \"Resume\" at the bottom of the log output. The log     should automatically update in realtime as the load test runs. You may need to continuously     scroll to view the log</li> <li>Wait until the load tests finish running. If at anytime something goes wrong, return to the     running Jenkins job and click either the \"Proceed\" or \"Abort\" prompt in the log output to     immediately end the test and start cleaning up</li> <li>Once Locust prints the summary table and has finished, indicated by the \"Locust master process     has stopped\" message, return to the Jenkins job and click \"Proceed\". This will cleanup the test,     destroying the controller instance and stopping any orphaned Lambda nodes</li> <li>View the stats of the run under the following log groups (the log stream corresponding to the     current run will be named according to the instance ID noted down in Step 13). Note     \"{ENVIRONMENT}\" should be replaced with the environment under test (i.e. \"test\"):<ul> <li>/bfd/{ENVIRONMENT}/bfd-server-load/load_exceptions.csv</li> <li>/bfd/{ENVIRONMENT}/bfd-server-load/load_failures.csv</li> <li>/bfd/{ENVIRONMENT}/bfd-server-load/load_stats.csv</li> <li>/bfd/{ENVIRONMENT}/bfd-server-load/load_stats_history.csv</li> </ul> </li> </ol>"},{"location":"runbooks/snyk/ignore-findings.html","title":"Ignore findings","text":"<p>Follow this runbook to mark a SNYK finding that has been determined not to be a vulnerability or that is considered an acceptable risk as 'Ignored' in the SNYK dashboard.</p> <ol> <li> <p>SNYK findings can be ignored when two lead engineers from either the BFD team or CMS agree on the justification.    Have a discussion about why the finding does not apply to BFD or why the risk of the finding is acceptable.</p> </li> <li> <p>Post a message in the bfd slack channel summarizing the findings    that will be marked 'Ignored' and the justification. Make adjustments as appropriate based on any feedback    provided.</p> </li> <li> <p>Draft a concise description of the justification for marking the finding as 'Ignored'.</p> </li> <li> <p>From the SNYK dashboard click the 'Ignore' button for the finding and    provide the justification as well as the usernames of the engineers that consulted on the justification. Select    the appropriate categorization:</p> </li> <li>'Ignore Permanently' should be used for findings that are not planned to ever be addressed.</li> <li>'Ignore Temporarily' should be used for findings that are planned to be addressed in the next PI at the latest.       When selecting this option the JIRA ticket number for the remediation must be provided in the description.       The JIRA ticket should include a link to the finding in the SNYK dashboard and any additional information that is       available as an aid to the resolution. For Maven dependency findings, reference the 'Additional Information'       section of the Resolving SNK-bot PR for populating the JIRA ticket.</li> </ol>"},{"location":"runbooks/snyk/resolve-snyk-bot-prs.html","title":"Resolve snyk bot prs","text":"<p>Follow this runbook to resolve a Java Maven dependency upgrade PR that is created by SNYK-bot.</p> <ol> <li>Determine whether the PR may be merged as-is by answering these questions (all must be YES to merge as-is)</li> <li>Github actions are all passing (enforced by github)</li> <li>Code reviews have been completed (enforced by github)</li> <li>All other github preconditions have been met (enforced by github)</li> <li> <p>The PR does not involve an upgrade to any of these artifacts which require additional scrutiny:</p> <ul> <li>ca.uhn.hapi.fhir:* (HAPI)</li> <li>org.springframework.*:* (Spring)</li> <li>org.hibernate.*:* (Hibernate)</li> <li>org.eclipse.jetty.*:* (Jetty)</li> <li>javax.*:* (Java EE)</li> <li>jakarta.*:* (Jakarta EE)</li> <li>io.grpc.*:* (GRPC)</li> <li>JDK/JRE</li> </ul> </li> <li> <p>If the conditions above are all met, the PR may be merged and no further action is required.</p> </li> <li> <p>If any of the conditions above are NOT met these steps should be followed:</p> </li> <li>Create a JIRA ticket or modify the AC of an existing ticket to capture the task of performing the upgrade manually.<ul> <li>For upgrades that resolve critical severity vulnerabilities, the ticket is considered a sprint buster and should    be scheduled in the current sprint.</li> <li>For non-critical upgrades, the ticket should be scheduled no later than the next PI as part of the regularly    occurring dependency upgrade sweep.</li> <li>The ticket should reference the SNYK PR and explain why it could not be merged.</li> <li>The ticket AC should include performance regression testing.</li> </ul> </li> <li>Close the PR with a comment that references the JIRA ticket and a brief explanation of why it was not merged.</li> </ol> <p>Additional information (not exhaustive) for creating JIRA upgrade tickets:  - BFD relies on the Java/Jakarta EE ecosystem directly and indirectly via several dependencies: Jetty, HAPI,    Hibernate(JPA), JAXB, Jakarta Annotations, and potentially others now or in the future. Care must be taken when    upgrading these components to ensure cross compatibility even when newer versions are available. The versions of    these components must be harmonized around a single EE version.  - Keeping HAPI on the latest version is highly desirable and this determines the Java Servlet API version that must    be used by BFD and consequently the EE version and the Jetty version.  - The BFD Spring version should match the Spring version that HAPI depends on, so the HAPI version effectively    determines the Spring version that BFD will use.  - The Jetty version should be the highest version available that supports the EE version that is compatible with HAPI.  - Java EE and Jakarta EE libraries should be the latest available for the EE version that is compatible with HAPI/Jetty.  - Hibernate and JPA libraries must be mutually compatible with each other and with the EE version.  - Hibernate artifacts must be mutually compatible and use the same version numbers whenever possible.  - Spring artifacts must be mutually compatible.  - As of this writing (Aug 2022), the latest available version of HAPI is compatible with Servlet API 4.0 which is    supported by Jetty 10 (even though a later version of Jetty that is compatible with Servlet API 5.0 is available)    which is licensed as Jakarta EE 8 but still uses the javax namespace as part of the transition from Java EE to    Jakarta EE.</p> <p>References:  - Jetty versions and corresponding EE and Servlet API versions  - HAPI versions  - Spring version compatibility   - Background on Java EE to Jakarta EE transition</p>"},{"location":"runbooks/snyk/resolve-snyk-report-findings.html","title":"Resolve snyk report findings","text":"<p>Follow this runbook for each SNYK finding that appears in the SNYK dashboard that does not have an associated SNYK bot PR automatically created in github, for those findings that do have a SNYK-bot PR follow the Resolving SNKY-bot PRs.</p> <ol> <li> <p>Is this a licensing finding? If yes, the finding can be marked as 'Ignored' by following the    Ignore SNYK Findings Runbook.</p> </li> <li> <p>For all other findings, including infrastructure findings and Maven dependency updates that do not have a SNYK bot PR:</p> <ul> <li>Create a JIRA ticket or modify the AC of an existing ticket to capture the task of remediating the finding.</li> <li>For critical severity findings, the ticket is considered a sprint buster and should be scheduled in the   current sprint.</li> <li>For non-critical findings, the ticket should be scheduled no later than the next PI.</li> </ul> </li> </ol> <p>NOTE: As of this writing (Aug 2022), the BFD SNYK dashboard is not configured to report SNYK Code findings so those are not considered in this runbook. If BFD adopts SNYK Code, this runbook should be updated to reflect the process for resolving SNYK Code findings.</p>"},{"location":"runbooks/synthea/run-automation.html","title":"How to Run Synthea Automation","text":"<ol> <li>Go to the Synthea Automation Jenkins Job - FUTURE: Add specific jenkins job link for automation</li> <li>Click \"Build with Parameters\" on the left side</li> <li>The parameters here control how the Synthea automation will generate and load data:<ul> <li>Number of Beneficiaries: the number of beneficiaries to generate data for<ul> <li>Jenkins has an upper limit that it can handle creating in one batch, so if you need more than the specified number, please split the load into multiple batches or manually generate the load on a detached instance</li> </ul> </li> <li>Target environment: the environment(s) that this run's data will be loaded into<ul> <li>There are two options, a load into only test, or a full load into test, prod-sbx, and then prod sequentially</li> </ul> </li> <li>Number of Months to generate into the future: If greater than 0, Synthea will generate some beneficiaries which have claim creation dates up to that many months into the future, and those future claims will be automatically portioned into weekly loads and placed in the environments' Synthea/Incoming ETL bucket. These future claims will be loaded and updated when their load date comes to term every week for the number of months specified. If this is 0, no claims will have dates beyond the current date.</li> </ul> </li> <li>Once the parameters are as you'd like then, click Build to begin the process</li> <li>The Jenkins pipeline will begin going through the steps to generate and then load the Synthea data into the target databases</li> <li>Once the data is successfully loaded into Test, if the goal is a full load it will be loaded into prod-sbx and prod as well. Assuming success in Test, the other envionments should have no issues.</li> <li>Once all data is loaded, if the data is only for Test, you are done. For prod-sbx/prod loads, continue the steps below.</li> </ol>"},{"location":"runbooks/synthea/run-automation.html#prod-load-additional-steps","title":"Prod Load Additional Steps","text":"<p>For prod/prod-sbx loads, we need to publish the results to our consumers, so some additional steps are required.</p> <ol> <li>A characteristics file needs to be generated and made available in order to let our partners know what beneficiary ids and claims will be available to use. A script exists for generating the characteristics file at '''beneficiary-fhir-data/ops/ccs-ops-misc/synthetic-data/scripts/synthea-automation/generate-characteristics-file.py'''</li> <li>Ensure you have Python3 installed, and also psycopg2 and boto3 (Python libraries) installed, as the script will need them</li> <li>Ensure you're connected to the VPN, as you'll need access to the database to run the script</li> <li>Run the script locally. It takes three parameters:         - bene_id_start: this is the bene id the generation started at, which will be printed in the Jenkins log when you run the job         - bene_id_end: this is the bene id the generation ended at, which will be printed in the Jenkins log when you run the job         - output location: the local directory the characteristics file should be written to</li> <li>Once the script runs, a file should be output called characteristics.csv at the location you specified in parameter 3</li> <li>Upload this file to the AWS public folder bfd-public-test-data/characteristics</li> <li>Next we need to update the github wiki page: Synthetic Data Guide</li> <li>On this page there are two spots we need to update: <ul> <li>The Available Synthetic Beneficiaries table, which you should add a row for the date, bene ranges, and link to the characteristics file in AWS above. Additionally, if there are future updates with this batch, an additional column should specify for how many months the batch will update</li> <li>The Release History table, which should describe the purpose of the synthetic batch along with any other relevant information</li> </ul> </li> <li>Lastly, our partners should be made aware of the new data load; post a message in the bfd-users chat informing them of the newly available data and a link to the wiki page with additional information (update the parts in brackets as needed/if there is future data) Note the default update time is Wednesday at 7am, so just remove the brackets if the update was done with future data. <p>BFD Synthetic data in prod-sbx and prod has been updated with &lt;10,000&gt; new beneficiaries&lt;, which will update every Wednesday at 7am EST&gt;. Information about the beneficiary ranges added  can be found in the Synthetic Data Guide: https://github.com/CMSgov/beneficiary-fhir-data/wiki/Synthetic-Data-Guide"},{"location":"runbooks/synthea/run-automation.html#troubleshooting-high-level","title":"Troubleshooting (High Level)","text":"<ul> <li>Many steps of the automation process can fail during data generation or loading, which will fail the Jenkins job. If this occurs, you will need to investigate the failure to determine the next steps. If errors occur during generation, you may need to check the database or end-state.properties file parameters. If errors occur during the pipeline load, you may need to move the new load files generated in Synthetic/Incoming out and restart the pipeline, as well as investigate the failure.</li> <li>ETL Pipeline loading of Synthea data to a BFD database is subject to a pre-validation step that checks data ranges based on pre-validation elements in the manifest file. For example, the manifest includes two elements: bene_id_start and bene_id_end; those values represent a lo-hi bene_id range that can be checked vs. bene_id(s) currently stored in the target database. If pre-validation detects an overlap (i.e., a bene_id that falls within the lo-hi range that is already stored in the database), then pre-validation fails unless the ETL is running in idempotent mode. When Synthea pre-validation fails, the ETL does the following:<ul> <li>Logs an error message specific to the pre-validation failure and terminates further processing.</li> <li>moves the mainfest and associated RIF files out of the S3 bucket folder /Incoming to the S3 bucket folder /Failed. This is done to preclude the ETL process from attempting to process the same mainfest and files which would occur if they were left in the /Incoming folder.</li> </ul> </li> <li>During the pipeline data load, the automation will automatically pass the end-state.properties data used to create the load and pre-validate the target database(s) do not contain those values before loading any data. This is intended to ensure there are no unique column collisions with the data before loading. If this pre-validation fails, no data cleanup will be required in the database since the load will be stopped; however you will need to check the end-state.properties in s3 and determine which fields are problematic, and fix them in the file. Additionally, you may need to move/delete the files from Synthetic/Incoming in the target environments' ETL boxes in AWS (see bullet above).</li> <li>In the unlikely event of issues in the prod environments after successful test, keep in mind you may need investigation and manual re-run of the data to keep consistency between environments, or cleanup/rollback of the Test database.</li> </ul>"},{"location":"tags/index.html","title":"Index","text":""},{"location":"tags/index.html#contributing","title":"Contributing","text":"<ul> <li>Contributing to the BFD</li> </ul>"},{"location":"tags/index.html#rfcs","title":"RFC's","text":"<ul> <li>0008-partial-backfill-ccw</li> <li>0012-rda-claims-jsonb</li> <li>0016-cursor-paging.md</li> <li>0007-filtering-fields</li> <li>0000-bye-bye-jboss</li> <li>0001-beneficiary-fhir-server-rfc-process-v1</li> <li>0002-monorepo</li> <li>0004-since-parameter</li> <li>0005-mbi-search</li> <li>0007-service-date-filter</li> <li>0009-pipeline-orchestration-baby-steps</li> <li>0011-separate-flyway-from-pipeline</li> <li>0013-bfd-server-startup-check</li> <li>0015-centralized-configuration-management</li> <li>0010-custom-system-uris</li> <li>0013-Synthea-future-claims-and-automation</li> <li>0014-dsl-driven-rda-code-generation</li> <li>0006-or-condition</li> </ul>"},{"location":"tags/index.html#ab2d","title":"ab2d","text":"<ul> <li>AB2D Firehose Setup</li> </ul>"},{"location":"tags/index.html#api","title":"api","text":"<ul> <li>BFD Audit Headers</li> </ul>"},{"location":"tags/index.html#bb2","title":"bb2","text":"<ul> <li>BB2 Project information</li> </ul>"},{"location":"tags/index.html#bfd-insights","title":"bfd-insights","text":"<ul> <li>AB2D Firehose Setup</li> </ul>"},{"location":"tags/index.html#firehose","title":"firehose","text":"<ul> <li>AB2D Firehose Setup</li> </ul>"},{"location":"tags/index.html#insights","title":"insights","text":"<ul> <li>BB2 Project information</li> <li>Introduction</li> </ul>"},{"location":"tags/index.html#kinesis","title":"kinesis","text":"<ul> <li>AB2D Firehose Setup</li> </ul>"},{"location":"tags/index.html#legal","title":"legal","text":"<ul> <li>License</li> </ul>"},{"location":"tags/index.html#license","title":"license","text":"<ul> <li>License</li> </ul>"},{"location":"tags/index.html","title":"Index","text":""},{"location":"tags/index.html#contributing","title":"Contributing","text":"<ul> <li>Contributing to the BFD</li> </ul>"},{"location":"tags/index.html#rfcs","title":"RFC's","text":"<ul> <li>0008-partial-backfill-ccw</li> <li>0012-rda-claims-jsonb</li> <li>0016-cursor-paging.md</li> <li>0007-filtering-fields</li> <li>0000-bye-bye-jboss</li> <li>0001-beneficiary-fhir-server-rfc-process-v1</li> <li>0002-monorepo</li> <li>0004-since-parameter</li> <li>0005-mbi-search</li> <li>0007-service-date-filter</li> <li>0009-pipeline-orchestration-baby-steps</li> <li>0011-separate-flyway-from-pipeline</li> <li>0013-bfd-server-startup-check</li> <li>0015-centralized-configuration-management</li> <li>0010-custom-system-uris</li> <li>0013-Synthea-future-claims-and-automation</li> <li>0014-dsl-driven-rda-code-generation</li> <li>0006-or-condition</li> </ul>"},{"location":"tags/index.html#ab2d","title":"ab2d","text":"<ul> <li>AB2D Firehose Setup</li> </ul>"},{"location":"tags/index.html#api","title":"api","text":"<ul> <li>BFD Audit Headers</li> </ul>"},{"location":"tags/index.html#bb2","title":"bb2","text":"<ul> <li>BB2 Project information</li> </ul>"},{"location":"tags/index.html#bfd-insights","title":"bfd-insights","text":"<ul> <li>AB2D Firehose Setup</li> </ul>"},{"location":"tags/index.html#firehose","title":"firehose","text":"<ul> <li>AB2D Firehose Setup</li> </ul>"},{"location":"tags/index.html#insights","title":"insights","text":"<ul> <li>BB2 Project information</li> <li>Introduction</li> </ul>"},{"location":"tags/index.html#kinesis","title":"kinesis","text":"<ul> <li>AB2D Firehose Setup</li> </ul>"},{"location":"tags/index.html#legal","title":"legal","text":"<ul> <li>License</li> </ul>"},{"location":"tags/index.html#license","title":"license","text":"<ul> <li>License</li> </ul>"}]}